<!doctype html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <title>Vllm V1 Whisper Transcription // davidgao7 blog</title>
    <link rel="shortcut icon" href="/images/favicon.jpg" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.148.1">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="David Gao" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.5b1fcc8902588589c4767187402a3c29f8b8d7a6fdef6d9f8f77045bb0d14fee.css" />
    

    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Vllm V1 Whisper Transcription">
  <meta name="twitter:description" content="In this blog I’ll refine my work process for the whisper transcription api implementation , as a support material for transcription api endpoint pr, here are step by step how I did it
Create a GPU environment I use Runpod.io for renting a RTX4090. You will get ssh access when initialize a pod. Some trails and errors details can reference Deploying LLMs in a single Machine Clone the repo, Setup python environment with uv cd workspace/ # workspace has the largest size apt update &amp;&amp; apt upgrade curl -LsSf https://astral.sh/uv/install.sh | sh export PATH=&#34;$HOME/.local/bin:$PATH&#34; source ~/.bashrc Then you can use uv">

    <meta property="og:url" content="http://localhost:1313/posts/vllm-v1-whisper-transcription/">
  <meta property="og:site_name" content="davidgao7 blog">
  <meta property="og:title" content="Vllm V1 Whisper Transcription">
  <meta property="og:description" content="In this blog I’ll refine my work process for the whisper transcription api implementation , as a support material for transcription api endpoint pr, here are step by step how I did it
Create a GPU environment I use Runpod.io for renting a RTX4090. You will get ssh access when initialize a pod. Some trails and errors details can reference Deploying LLMs in a single Machine Clone the repo, Setup python environment with uv cd workspace/ # workspace has the largest size apt update &amp;&amp; apt upgrade curl -LsSf https://astral.sh/uv/install.sh | sh export PATH=&#34;$HOME/.local/bin:$PATH&#34; source ~/.bashrc Then you can use uv">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-30T13:49:26-04:00">
    <meta property="article:modified_time" content="2025-05-30T13:49:26-04:00">
    <meta property="article:tag" content="VLLM">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="Production">
    <meta property="article:tag" content="Deployment">
    <meta property="article:tag" content="EGPU">
    <meta property="article:tag" content="Lambda">


    
      <script>
window.MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]'], ['$$', '$$']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
</script>


    

  </head>
  <body>
    <header class="app-header">
      <a href="/"><img class="app-header-avatar" src="/images/avatar.jpg" alt="David Gao" /></a>
      <span class="app-header-title">davidgao7 blog</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
      </nav>
      <p>Note new findings every day to let the magic happen!</p>
      <div class="app-header-social">
        
          <a href="https://github.com/davidgao7" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-brand-github" viewBox="0 0 24 24" fill="currentColor"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
          </a>
        
          <a href="https://x.com/AiiGen71976j" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-brand-x" viewBox="0 0 24 24" fill="currentColor"><title>X</title><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"/></svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Vllm V1 Whisper Transcription</h1>
      <div class="post-meta">
        <div>
          <svg class="icon icon-calendar" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
          May 30, 2025
        </div>
        <div>
          <svg class="icon icon-clock" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>clock</title><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>
          8 min read
        </div>
        <div>
          <svg class="icon icon-tag" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>tag</title><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line></svg>
              <a class="tag" href="/tags/vllm/">VLLM</a>
              <a class="tag" href="/tags/llm/">LLM</a>
              <a class="tag" href="/tags/production/">Production</a>
              <a class="tag" href="/tags/deployment/">Deployment</a>
              <a class="tag" href="/tags/egpu/">EGPU</a>
              <a class="tag" href="/tags/lambda/">Lambda</a>
              <a class="tag" href="/tags/runpod/">Runpod</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <blockquote>
<p>In this blog I&rsquo;ll refine my work process for the whisper transcription api implementation , as a support material for <a href="https://github.com/vllm-project/production-stack/pull/469">transcription api endpoint pr</a>, here are step by step how I did it</p></blockquote>
<h1 id="create-a-gpu-environment">Create a GPU environment</h1>
<ul>
<li>I use <em>Runpod.io</em> for renting a RTX4090.</li>
<li>You will get ssh access when initialize a pod.</li>
<li>Some trails and errors details can reference <a href="https://davidgao7.github.io/posts/llm-production/">Deploying LLMs in a single Machine</a></li>
</ul>
<h1 id="clone-the-repo-setup-python-environment-with-uv">Clone the repo, Setup python environment with uv</h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>cd workspace/  <span style="color:#75715e"># workspace has the largest size</span>
</span></span><span style="display:flex;"><span>apt update <span style="color:#f92672">&amp;&amp;</span> apt upgrade
</span></span><span style="display:flex;"><span>curl -LsSf https://astral.sh/uv/install.sh | sh
</span></span><span style="display:flex;"><span>export PATH<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span>$HOME<span style="color:#e6db74">/.local/bin:</span>$PATH<span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>source ~/.bashrc
</span></span></code></pre></div><p>Then you can use <code>uv</code></p>
<p>[!NOTE] For production-stack repo we need <em>python 3.12</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/davidgao7/production-stack.git
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cd production-stack
</span></span></code></pre></div><p>[!NOTE] Pay attention to your runpod cuda and pytorch version</p>
<pre tabindex="0"><code>runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv venv --python 3.12 --seed
</span></span><span style="display:flex;"><span>source .venv/bin/activate
</span></span></code></pre></div><p>Install the dependencies</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>which pip  <span style="color:#75715e"># make sure the pip you are using is the venv one</span>
</span></span><span style="display:flex;"><span>pip install -e /workspace/production-stack<span style="color:#f92672">[</span>dev<span style="color:#f92672">]</span>
</span></span></code></pre></div><p>If you face this issue, I recommend you to build vllm from repo source too:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># --- Part A: Create the Python 3.12 Environment ---</span>
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;--- Creating a fresh Python 3.12 environment ---&#34;</span>
</span></span><span style="display:flex;"><span>cd /workspace/production-stack
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Remove any old virtual environments to be safe</span>
</span></span><span style="display:flex;"><span>rm -rf .venv*
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create a venv using the Python 3.12 we just installed</span>
</span></span><span style="display:flex;"><span>python3.12 -m venv .venv
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Activate it</span>
</span></span><span style="display:flex;"><span>source .venv/bin/activate
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --- Part B: Install Build Tools &amp; Correct PyTorch ---</span>
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;--- Installing build tools and PyTorch for CUDA 12.x ---&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Install cmake (if not already present) and upgrade pip</span>
</span></span><span style="display:flex;"><span>apt-get install -y cmake
</span></span><span style="display:flex;"><span>pip install --upgrade pip
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Install the PyTorch version that matches your system&#39;s CUDA driver</span>
</span></span><span style="display:flex;"><span>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --- Part C: Build vLLM from Source with Audio Support ---</span>
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;--- Building vLLM from source, this will take several minutes ---&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># MAX_JOBS=1 is a safeguard to prevent the compiler from crashing due to low RAM</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --no-cache-dir ensures a completely fresh build</span>
</span></span><span style="display:flex;"><span>MAX_JOBS<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> pip install --no-cache-dir <span style="color:#e6db74">&#34;vllm[audio] @ git+https://github.com/vllm-project/vllm.git&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --- Part D: Install Your Project ---</span>
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;--- Installing your production-stack project ---&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This will now work because your environment is Python 3.12</span>
</span></span><span style="display:flex;"><span>pip install -e .<span style="color:#f92672">[</span>dev<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --- FINAL STEP ---</span>
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;✅ Environment setup complete. You are now ready to start the vLLM server.&#34;</span>
</span></span></code></pre></div><p>Wait till it finishes, it will take a while to download the dependencies.</p>
<p>Install vLLM (with audio) without filling the small overlay cache</p>
<blockquote>
<p>Compiling large C++/CUDA codebases is a very memory-intensive process. By default, the build system tries to use all available CPU cores to compile files in parallel (e.g., you might see -j=32 in the logs, meaning 32 parallel jobs). On a machine with limited RAM, this can easily exhaust all available memory and cause the compiler to segfault.</p></blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>MAX_JOBS<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> pip install --no-cache-dir <span style="color:#e6db74">&#34;vllm[audio] @ git+https://github.com/vllm-project/vllm.git&#34;</span>
</span></span></code></pre></div><p>[!NOTE] If you have cuda and build failed,</p>
<p>Verify you have both <em>vllm</em> and <em>vllm-router</em> installed</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip list | grep -E <span style="color:#e6db74">&#34;vllm|vllm-router&#34;</span>
</span></span></code></pre></div><h1 id="code-implementation-snippet">Code implementation snippet</h1>
<p>In <em>production-stack</em> repo</p>
<ul>
<li>Main router implementation pit I fell before: <strong>src/vllm_router/routers/main_router.py/async def audio_transcriptions</strong></li>
<li>File upload for fastapi</li>
<li>Filter endpoint urls for transcription as the start url</li>
<li>Pick one of the endpoint url using <code>router.route_request</code></li>
<li>Proxy the request with <code>httpx.AsyncClient</code></li>
<li>Get the whisper model output</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@main_router.post</span>(<span style="color:#e6db74">&#34;/v1/audio/transcriptions&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">audio_transcriptions</span>(
</span></span><span style="display:flex;"><span>    file: UploadFile <span style="color:#f92672">=</span> File(<span style="color:#f92672">...</span>)
</span></span><span style="display:flex;"><span>    model: str <span style="color:#f92672">=</span> Form(<span style="color:#f92672">...</span>),
</span></span><span style="display:flex;"><span>    prompt: str <span style="color:#f92672">|</span> <span style="color:#66d9ef">None</span> <span style="color:#f92672">=</span> Form(<span style="color:#66d9ef">None</span>),
</span></span><span style="display:flex;"><span>    response_format: str <span style="color:#f92672">|</span> <span style="color:#66d9ef">None</span> <span style="color:#f92672">=</span> Form(<span style="color:#e6db74">&#34;json&#34;</span>),
</span></span><span style="display:flex;"><span>    temperature: float <span style="color:#f92672">|</span> <span style="color:#66d9ef">None</span> <span style="color:#f92672">=</span> Form(<span style="color:#66d9ef">None</span>),
</span></span><span style="display:flex;"><span>    language: str <span style="color:#f92672">=</span> Form(<span style="color:#e6db74">&#34;en&#34;</span>),
</span></span><span style="display:flex;"><span>):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># filter url for audio transcription endpoints</span>
</span></span><span style="display:flex;"><span>    transcription_endpoints <span style="color:#f92672">=</span> [ep <span style="color:#66d9ef">for</span> ep <span style="color:#f92672">in</span> endpoints <span style="color:#66d9ef">if</span> model <span style="color:#f92672">==</span> ep<span style="color:#f92672">.</span>model_name]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># pick one using the router&#39;s configured logic (roundrobin, least-loaded, etc.)</span>
</span></span><span style="display:flex;"><span>    chosen_url <span style="color:#f92672">=</span> router<span style="color:#f92672">.</span>route_request(
</span></span><span style="display:flex;"><span>        transcription_endpoints,
</span></span><span style="display:flex;"><span>        engine_stats,
</span></span><span style="display:flex;"><span>        request_stats,
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># we don’t need to pass the original FastAPI Request object here,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># but you can if your routing logic looks at headers or body</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># proxy the request</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># by default httpx will only wait for 5 seconds, large audio transcriptions generally</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># take longer than that</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">async</span> <span style="color:#66d9ef">with</span> httpx<span style="color:#f92672">.</span>AsyncClient(
</span></span><span style="display:flex;"><span>        base_url<span style="color:#f92672">=</span>chosen_url,
</span></span><span style="display:flex;"><span>        timeout<span style="color:#f92672">=</span>httpx<span style="color:#f92672">.</span>Timeout(
</span></span><span style="display:flex;"><span>            connect<span style="color:#f92672">=</span><span style="color:#ae81ff">60.0</span>,  <span style="color:#75715e"># connect timeout</span>
</span></span><span style="display:flex;"><span>            read<span style="color:#f92672">=</span><span style="color:#ae81ff">300.0</span>,  <span style="color:#75715e"># read timeout</span>
</span></span><span style="display:flex;"><span>            write<span style="color:#f92672">=</span><span style="color:#ae81ff">30.0</span>,  <span style="color:#75715e"># if you’re streaming uploads</span>
</span></span><span style="display:flex;"><span>            pool<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>,  <span style="color:#75715e"># no pool timeout</span>
</span></span><span style="display:flex;"><span>        ),
</span></span><span style="display:flex;"><span>    ) <span style="color:#66d9ef">as</span> client:
</span></span><span style="display:flex;"><span>        logger<span style="color:#f92672">.</span>debug(<span style="color:#e6db74">&#34;Sending multipart to </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">/v1/audio/transcriptions …&#34;</span>, chosen_url)
</span></span><span style="display:flex;"><span>        proxied <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> client<span style="color:#f92672">.</span>post(<span style="color:#e6db74">&#34;/v1/audio/transcriptions&#34;</span>, data<span style="color:#f92672">=</span>data, files<span style="color:#f92672">=</span>files)
</span></span></code></pre></div><ul>
<li>
<p>Make sure input is a wav audio file.</p>
</li>
<li>
<p>Adding model type: <strong>src/vllm_router/utils.py</strong></p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ModelType</span>(enum<span style="color:#f92672">.</span>Enum):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#...</span>
</span></span><span style="display:flex;"><span>    transcription <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/v1/audio/transcriptions&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@staticmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_test_payload</span>(model_type: str):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">match</span> ModelType[model_type]:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">case</span> ModelType<span style="color:#f92672">.</span>transcription:
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">return</span> {
</span></span><span style="display:flex;"><span>                    <span style="color:#e6db74">&#34;file&#34;</span>: <span style="color:#e6db74">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>                    <span style="color:#e6db74">&#34;model&#34;</span>: <span style="color:#e6db74">&#34;openai/whisper-small&#34;</span>
</span></span><span style="display:flex;"><span>                }
</span></span></code></pre></div><h1 id="testing">Testing</h1>
<ul>
<li>
<p>create a shell script to take <code>router port</code>, <code>backend_url</code>, spin up the vllm api endpoint</p>
</li>
<li>
<p>command to serve the model engine</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># vllm backend serve on port 8002:</span>
</span></span><span style="display:flex;"><span>uv run vllm serve --task transcription openai/whisper-small --host 0.0.0.0 --port <span style="color:#ae81ff">8002</span> --trust-remote-code
</span></span></code></pre></div><hr>
<p>[!NOTE] vllm process won&rsquo;t die when you stop it, you can <code>kill -9</code> the process</p>
<p>no wonder I always got no resoruces in pod&hellip; oof</p>
<p><img src="/images/vllm-wont-die-itself.png" alt="vllm processes"></p>
<p>kill all the vllm processes</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pkill -f <span style="color:#e6db74">&#34;vllm serve&#34;</span>
</span></span></code></pre></div><p>or kill by python path (more board)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pkill -f <span style="color:#e6db74">&#34;/workspace/production-stack/.venv/bin/python3&#34;</span>
</span></span></code></pre></div><hr>
<ul>
<li>command to run the router, connect to the backend</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e">#!/bin/bash
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">if</span> <span style="color:#f92672">[[</span> $# -ne <span style="color:#ae81ff">2</span> <span style="color:#f92672">]]</span>; <span style="color:#66d9ef">then</span>
</span></span><span style="display:flex;"><span>    echo <span style="color:#e6db74">&#34;Usage </span>$0<span style="color:#e6db74"> &lt;router port&gt; &lt;backend url&gt;&#34;</span>
</span></span><span style="display:flex;"><span>    exit <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">fi</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># router serve on port 8000, connect to the vllm backend on port 8002:</span>
</span></span><span style="display:flex;"><span>uv run python3 -m vllm_router.app <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --host 0.0.0.0 --port <span style="color:#ae81ff">8000</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --service-discovery static <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --static-backends <span style="color:#e6db74">&#34;http://0.0.0.0:8002&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --static-models <span style="color:#e6db74">&#34;openai/whisper-small&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --static-model-types <span style="color:#e6db74">&#34;transcription&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --routing-logic roundrobin <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --log-stats <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --engine-stats-interval <span style="color:#ae81ff">10</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --request-stats-window <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>    --static-backend-health-checks <span style="color:#75715e"># Enable this flag to make vllm-router check periodically if the models work by sending dummy requests to their endpoints.</span>
</span></span></code></pre></div><p><strong>Note</strong> that the port for <code>--static-backends</code> is the port you set for the vllm serve command, in this case <code>8002</code>.</p>
<p>Then wait till it&rsquo;s listen on the port, you can post an audio file to the endpoint, for example</p>
<ul>
<li>command to get the transcription result as json by using <code>curl</code></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># when calling the endpoint, make sure the file is a wav audio file</span>
</span></span><span style="display:flex;"><span>curl -v http://localhost:8002/v1/audio/transcriptions <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>     -F <span style="color:#e6db74">&#39;file=@/workspace/production-stack/src/vllm_router/audio_transcriptions_test.wav;type=audio/wav&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>     -F <span style="color:#e6db74">&#39;model=openai/whisper-small&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>     -F <span style="color:#e6db74">&#39;response_format=json&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>     -F <span style="color:#e6db74">&#39;language=en&#39;</span>
</span></span></code></pre></div><h3 id="handling-emptyno-audio-file-request">Handling empty(no) audio file request</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># one of the match case in python switch statement</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">case</span> ModelType<span style="color:#f92672">.</span>transcription:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Generate a 0.1 second silent audio file</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> io<span style="color:#f92672">.</span>BytesIO() <span style="color:#66d9ef">as</span> wav_buffer:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> wave<span style="color:#f92672">.</span>open(wav_buffer, <span style="color:#e6db74">&#34;wb&#34;</span>) <span style="color:#66d9ef">as</span> wf:
</span></span><span style="display:flex;"><span>            wf<span style="color:#f92672">.</span>setnchannels(<span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># mono audio channel, standard configuration</span>
</span></span><span style="display:flex;"><span>            wf<span style="color:#f92672">.</span>setsampwidth(<span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># 16 bit audio, common bit depth for wav file</span>
</span></span><span style="display:flex;"><span>            wf<span style="color:#f92672">.</span>setframerate(<span style="color:#ae81ff">16000</span>)  <span style="color:#75715e"># 16 kHz sample rate</span>
</span></span><span style="display:flex;"><span>            wf<span style="color:#f92672">.</span>writeframes(<span style="color:#e6db74">b</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\x00\x00</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">1600</span>)  <span style="color:#75715e"># 0.1 second of silence</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># retrieves the generated wav bytes, return</span>
</span></span><span style="display:flex;"><span>        wav_bytes <span style="color:#f92672">=</span> wav_buffer<span style="color:#f92672">.</span>getvalue()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;file&#34;</span>: (<span style="color:#e6db74">&#34;empty.wav&#34;</span>, wav_bytes, <span style="color:#e6db74">&#34;audio/wav&#34;</span>),
</span></span><span style="display:flex;"><span>    }
</span></span></code></pre></div><h3 id="log-for-running-vllm-backend">Log for running vllm backend</h3>
<p>In the test audio file, I said: &ldquo;Testing testing, testing the whisper small model; testing testing, testing the audio transcription function; testing testing, testing the whisper small model.&rdquo;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>INFO 06-01 10:51:27 <span style="color:#f92672">[</span>logger.py:39<span style="color:#f92672">]</span> Received request trsc-310b30730a4a433d9d9c84437206579c: prompt: <span style="color:#e6db74">&#39;&lt;|startoftranscript|&gt;&lt;|en|&gt;&lt;|transcribe|&gt;&lt;|notimestamps|&gt;&#39;</span>, params: SamplingParams<span style="color:#f92672">(</span>n<span style="color:#f92672">=</span>1, presence_penalty<span style="color:#f92672">=</span>0.0, frequency_penalty<span style="color:#f92672">=</span>0.0, repetition_penalty<span style="color:#f92672">=</span>1.0, temperature<span style="color:#f92672">=</span>0.0, top_p<span style="color:#f92672">=</span>1.0, top_k<span style="color:#f92672">=</span>-1, min_p<span style="color:#f92672">=</span>0.0, seed<span style="color:#f92672">=</span>None, stop<span style="color:#f92672">=[]</span>, stop_token_ids<span style="color:#f92672">=[]</span>, bad_words<span style="color:#f92672">=[]</span>, include_stop_str_in_output<span style="color:#f92672">=</span>False, ignore_eos<span style="color:#f92672">=</span>False, max_tokens<span style="color:#f92672">=</span>448, min_tokens<span style="color:#f92672">=</span>0, logprobs<span style="color:#f92672">=</span>None, prompt_logprobs<span style="color:#f92672">=</span>None, skip_special_tokens<span style="color:#f92672">=</span>True, spaces_between_special_tokens<span style="color:#f92672">=</span>True, truncate_prompt_tokens<span style="color:#f92672">=</span>None, guided_decoding<span style="color:#f92672">=</span>None, extra_args<span style="color:#f92672">=</span>None<span style="color:#f92672">)</span>, prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
</span></span><span style="display:flex;"><span>INFO 06-01 10:51:27 <span style="color:#f92672">[</span>engine.py:310<span style="color:#f92672">]</span> Added request trsc-310b30730a4a433d9d9c84437206579c.
</span></span><span style="display:flex;"><span>INFO 06-01 10:51:27 <span style="color:#f92672">[</span>metrics.py:481<span style="color:#f92672">]</span> Avg prompt throughput: 0.5 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: <span style="color:#ae81ff">1</span> reqs, Swapped: <span style="color:#ae81ff">0</span> reqs, Pending: <span style="color:#ae81ff">0</span> reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
</span></span><span style="display:flex;"><span>INFO:     127.0.0.1:41532 - <span style="color:#e6db74">&#34;POST /v1/audio/transcriptions HTTP/1.1&#34;</span> <span style="color:#ae81ff">200</span> OK
</span></span><span style="display:flex;"><span>INFO 06-01 10:51:37 <span style="color:#f92672">[</span>metrics.py:481<span style="color:#f92672">]</span> Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.8 tokens/s, Running: <span style="color:#ae81ff">0</span> reqs, Swapped: <span style="color:#ae81ff">0</span> reqs, Pending: <span style="color:#ae81ff">0</span> reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
</span></span><span style="display:flex;"><span>INFO 06-01 10:51:47 <span style="color:#f92672">[</span>metrics.py:481<span style="color:#f92672">]</span> Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: <span style="color:#ae81ff">0</span> reqs, Swapped: <span style="color:#ae81ff">0</span> reqs, Pending: <span style="color:#ae81ff">0</span> reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
</span></span></code></pre></div><p>And this is debug output, shows it hits my code:</p>
<h3 id="log-for-running-vllm-router">Log for running vllm router</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#f92672">==================================================</span>
</span></span><span style="display:flex;"><span>Server: http://localhost:8002
</span></span><span style="display:flex;"><span>Models:
</span></span><span style="display:flex;"><span>  - openai/whisper-small
</span></span><span style="display:flex;"><span> Engine Stats: Running Requests: 0.0, Queued Requests: 0.0, GPU Cache Hit Rate: 0.00
</span></span><span style="display:flex;"><span> Request Stats: No stats available
</span></span><span style="display:flex;"><span>--------------------------------------------------
</span></span><span style="display:flex;"><span><span style="color:#f92672">==================================================</span>
</span></span><span style="display:flex;"><span> <span style="color:#f92672">(</span>log_stats.py:104:vllm_router.stats.log_stats<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>2025-05-30 05:58:36,809<span style="color:#f92672">]</span> INFO: Scraping metrics from <span style="color:#ae81ff">1</span> serving engine<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span> <span style="color:#f92672">(</span>engine_stats.py:136:vllm_router.stats.engine_stats<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>2025-05-30 05:58:43,042<span style="color:#f92672">]</span> INFO: Received <span style="color:#ae81ff">200</span> from whisper backend <span style="color:#f92672">(</span>main_router.py:293:vllm_router.routers.main_router<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>2025-05-30 05:58:43,042<span style="color:#f92672">]</span> DEBUG: <span style="color:#f92672">====</span> Whisper response payload <span style="color:#f92672">====</span> <span style="color:#f92672">(</span>main_router.py:298:vllm_router.routers.main_router<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>2025-05-30 05:58:43,042<span style="color:#f92672">]</span> DEBUG: <span style="color:#f92672">{</span><span style="color:#e6db74">&#39;text&#39;</span>: <span style="color:#e6db74">&#39; Testing testing testing the whisper small model testing testing testing the audio transcription function testing testing testing the whisper small model&#39;</span><span style="color:#f92672">}</span> <span style="color:#f92672">(</span>main_router.py:299:vllm_router.routers.main_router<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>2025-05-30 05:58:43,042<span style="color:#f92672">]</span> DEBUG: <span style="color:#f92672">====</span> Whisper response payload <span style="color:#f92672">====</span> <span style="color:#f92672">(</span>main_router.py:300:vllm_router.routers.main_router<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>2025-05-30 05:58:43,042<span style="color:#f92672">]</span> DEBUG: Backend response headers: Headers<span style="color:#f92672">({</span><span style="color:#e6db74">&#39;date&#39;</span>: <span style="color:#e6db74">&#39;Fri, 30 May 2025 05:58:31 GMT&#39;</span>, <span style="color:#e6db74">&#39;server&#39;</span>: <span style="color:#e6db74">&#39;uvicorn&#39;</span>, <span style="color:#e6db74">&#39;content-length&#39;</span>: <span style="color:#e6db74">&#39;164&#39;</span>, <span style="color:#e6db74">&#39;content-type&#39;</span>: <span style="color:#e6db74">&#39;application/json&#39;</span><span style="color:#f92672">})</span> <span style="color:#f92672">(</span>main_router.py:302:vllm_router.routers.main_router<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>2025-05-30 05:58:43,042<span style="color:#f92672">]</span> DEBUG: Backend response body <span style="color:#f92672">(</span>truncated<span style="color:#f92672">)</span>: b<span style="color:#e6db74">&#39;{&#34;text&#34;:&#34; Testing testing testing the whisper small model testing testing testing the audio transcription function testing testing testing the whisper small model&#34;}&#39;</span> <span style="color:#f92672">(</span>main_router.py:303:vllm_router.routers.main_router<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>INFO:     127.0.0.1:49284 - <span style="color:#e6db74">&#34;POST /v1/audio/transcriptions HTTP/1.1&#34;</span> <span style="color:#ae81ff">200</span> OK
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>2025-05-30 05:58:46,769<span style="color:#f92672">]</span> INFO:
</span></span></code></pre></div><h3 id="logresult-of-the-posting-curl-command">Log/result of the posting (curl command)</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>*   Trying 127.0.0.1:8002...
</span></span><span style="display:flex;"><span>* Connected to localhost <span style="color:#f92672">(</span>127.0.0.1<span style="color:#f92672">)</span> port <span style="color:#ae81ff">8002</span> <span style="color:#f92672">(</span><span style="color:#75715e">#0)</span>
</span></span><span style="display:flex;"><span>&gt; POST /v1/audio/transcriptions HTTP/1.1
</span></span><span style="display:flex;"><span>&gt; Host: localhost:8002
</span></span><span style="display:flex;"><span>&gt; User-Agent: curl/7.81.0
</span></span><span style="display:flex;"><span>&gt; Accept: */*
</span></span><span style="display:flex;"><span>&gt; Content-Length: <span style="color:#ae81ff">1275490</span>
</span></span><span style="display:flex;"><span>&gt; Content-Type: multipart/form-data; boundary<span style="color:#f92672">=</span>------------------------058cd4e05f99b8bc
</span></span><span style="display:flex;"><span>&gt; Expect: 100-continue
</span></span><span style="display:flex;"><span>&gt;
</span></span><span style="display:flex;"><span>* Mark bundle as not supporting multiuse
</span></span><span style="display:flex;"><span>&lt; HTTP/1.1 <span style="color:#ae81ff">100</span> Continue
</span></span><span style="display:flex;"><span>* We are completely uploaded and fine
</span></span><span style="display:flex;"><span>* Mark bundle as not supporting multiuse
</span></span><span style="display:flex;"><span>&lt; HTTP/1.1 <span style="color:#ae81ff">200</span> OK
</span></span><span style="display:flex;"><span>&lt; date: Sun, <span style="color:#ae81ff">01</span> Jun <span style="color:#ae81ff">2025</span> 10:51:27 GMT
</span></span><span style="display:flex;"><span>&lt; server: uvicorn
</span></span><span style="display:flex;"><span>&lt; content-length: <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>&lt; content-type: application/json
</span></span><span style="display:flex;"><span>&lt;
</span></span><span style="display:flex;"><span>* Connection <span style="color:#75715e">#0 to host localhost left intact</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{</span><span style="color:#e6db74">&#34;text&#34;</span>:<span style="color:#e6db74">&#34; Testing testing testing the whisper small model testing testing testing the audio transcription function testing testing testing the whisper small model&#34;</span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>The &ldquo;clean&rdquo; output of the transcription is, in json format:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;text&#34;</span>:<span style="color:#e6db74">&#34; Testing testing testing the whisper small model testing testing testing the audio transcription function testing testing testing the whisper small model&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Which is what I said in the audio file.</p>
<p>This conclude that the whisper transcription api endpoint is working as expected.</p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
