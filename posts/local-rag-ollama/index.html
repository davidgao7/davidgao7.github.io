<!doctype html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <title>Local Rag Ollama // davidgao7 blog</title>
    <link rel="shortcut icon" href="/images/favicon.jpg" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.147.2">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="David Gao" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.5b1fcc8902588589c4767187402a3c29f8b8d7a6fdef6d9f8f77045bb0d14fee.css" />
    

    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Local Rag Ollama">
  <meta name="twitter:description" content="Local LightRAG: a GraphRAG Alternative but Fully Local with Ollama repo demo Install Install from source (Recommend) cd LightRAG pip install -e . Install from PyPI pip install lightrag-hku Quick Start Video demo of running LightRAG locally. All the code can be found in the examples. Set OpenAI API key in environment if using OpenAI models: export OPENAI_API_KEY=&#34;sk-...&#34;. Download the demo text “A Christmas Carol by Charles Dickens”: curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt &gt; ./book.txt Use the below Python snippet (in a script) to initialize LightRAG and perform queries:">

    <meta property="og:url" content="http://localhost:1313/posts/local-rag-ollama/">
  <meta property="og:site_name" content="davidgao7 blog">
  <meta property="og:title" content="Local Rag Ollama">
  <meta property="og:description" content="Local LightRAG: a GraphRAG Alternative but Fully Local with Ollama repo demo Install Install from source (Recommend) cd LightRAG pip install -e . Install from PyPI pip install lightrag-hku Quick Start Video demo of running LightRAG locally. All the code can be found in the examples. Set OpenAI API key in environment if using OpenAI models: export OPENAI_API_KEY=&#34;sk-...&#34;. Download the demo text “A Christmas Carol by Charles Dickens”: curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt &gt; ./book.txt Use the below Python snippet (in a script) to initialize LightRAG and perform queries:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-02-14T15:19:01-05:00">
    <meta property="article:modified_time" content="2025-02-14T15:19:01-05:00">
    <meta property="article:tag" content="Rag">
    <meta property="article:tag" content="LLM">


    
      <script>
window.MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]'], ['$$', '$$']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
</script>


    

  </head>
  <body>
    <header class="app-header">
      <a href="/"><img class="app-header-avatar" src="/images/avatar.jpg" alt="David Gao" /></a>
      <span class="app-header-title">davidgao7 blog</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
      </nav>
      <p>Note new findings every day to let the magic happen!</p>
      <div class="app-header-social">
        
          <a href="https://github.com/davidgao7" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-brand-github" viewBox="0 0 24 24" fill="currentColor"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
          </a>
        
          <a href="https://x.com/AiiGen71976j" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-brand-x" viewBox="0 0 24 24" fill="currentColor"><title>X</title><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"/></svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Local Rag Ollama</h1>
      <div class="post-meta">
        <div>
          <svg class="icon icon-calendar" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
          Feb 14, 2025
        </div>
        <div>
          <svg class="icon icon-clock" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>clock</title><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>
          19 min read
        </div>
        <div>
          <svg class="icon icon-tag" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>tag</title><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line></svg>
              <a class="tag" href="/tags/rag/">Rag</a>
              <a class="tag" href="/tags/llm/">LLM</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <h1 id="local-lightrag-a-graphrag-alternative-but-fully-local-with-ollama">Local LightRAG: a GraphRAG Alternative but Fully Local with Ollama</h1>
<ul>
<li><a href="https://github.com/HKUDS/LightRAG">repo</a></li>
<li><a href="https://www.youtube.com/watch?v=g21royNJ4fw">demo</a></li>
</ul>
<h2 id="install">Install</h2>
<ul>
<li>Install from source (Recommend)</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>cd LightRAG
</span></span><span style="display:flex;"><span>pip install -e .
</span></span></code></pre></div><ul>
<li>Install from PyPI</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install lightrag-hku
</span></span></code></pre></div><h2 id="quick-start">Quick Start</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=g21royNJ4fw">Video demo</a> of running LightRAG locally.</li>
<li>All the code can be found in the <code>examples</code>.</li>
<li>Set OpenAI API key in environment if using OpenAI models: <code>export OPENAI_API_KEY=&quot;sk-...&quot;.</code></li>
<li>Download the demo text &ldquo;A Christmas Carol by Charles Dickens&rdquo;:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt &gt; ./book.txt
</span></span></code></pre></div><p>Use the below Python snippet (in a script) to initialize LightRAG and perform queries:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> lightrag <span style="color:#f92672">import</span> LightRAG, QueryParam
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> lightrag.llm.openai <span style="color:#f92672">import</span> gpt_4o_mini_complete, gpt_4o_complete, openai_embed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#########</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Uncomment the below two lines if running in a jupyter notebook to handle the async nature of rag.insert()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># import nest_asyncio</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># nest_asyncio.apply()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#########</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>WORKING_DIR <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;./dickens&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>exists(WORKING_DIR):
</span></span><span style="display:flex;"><span>    os<span style="color:#f92672">.</span>mkdir(WORKING_DIR)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>rag <span style="color:#f92672">=</span> LightRAG(
</span></span><span style="display:flex;"><span>    working_dir<span style="color:#f92672">=</span>WORKING_DIR,
</span></span><span style="display:flex;"><span>    embedding_func<span style="color:#f92672">=</span>openai_embed,
</span></span><span style="display:flex;"><span>    llm_model_func<span style="color:#f92672">=</span>gpt_4o_mini_complete  <span style="color:#75715e"># Use gpt_4o_mini_complete LLM model</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># llm_model_func=gpt_4o_complete  # Optionally, use a stronger model</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;./book.txt&#34;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    rag<span style="color:#f92672">.</span>insert(f<span style="color:#f92672">.</span>read())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perform naive search</span>
</span></span><span style="display:flex;"><span>print(rag<span style="color:#f92672">.</span>query(<span style="color:#e6db74">&#34;What are the top themes in this story?&#34;</span>, param<span style="color:#f92672">=</span>QueryParam(mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;naive&#34;</span>)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perform local search</span>
</span></span><span style="display:flex;"><span>print(rag<span style="color:#f92672">.</span>query(<span style="color:#e6db74">&#34;What are the top themes in this story?&#34;</span>, param<span style="color:#f92672">=</span>QueryParam(mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;local&#34;</span>)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perform global search</span>
</span></span><span style="display:flex;"><span>print(rag<span style="color:#f92672">.</span>query(<span style="color:#e6db74">&#34;What are the top themes in this story?&#34;</span>, param<span style="color:#f92672">=</span>QueryParam(mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;global&#34;</span>)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perform hybrid search</span>
</span></span><span style="display:flex;"><span>print(rag<span style="color:#f92672">.</span>query(<span style="color:#e6db74">&#34;What are the top themes in this story?&#34;</span>, param<span style="color:#f92672">=</span>QueryParam(mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;hybrid&#34;</span>)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perform mix search (Knowledge Graph + Vector Retrieval)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Mix mode combines knowledge graph and vector search:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># - Uses both structured (KG) and unstructured (vector) information</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># - Provides comprehensive answers by analyzing relationships and context</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># - Supports image content through HTML img tags</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># - Allows control over retrieval depth via top_k parameter</span>
</span></span><span style="display:flex;"><span>print(rag<span style="color:#f92672">.</span>query(<span style="color:#e6db74">&#34;What are the top themes in this story?&#34;</span>, param<span style="color:#f92672">=</span>QueryParam(
</span></span><span style="display:flex;"><span>    mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;mix&#34;</span>)))
</span></span></code></pre></div><h3 id="conversation-history-support">Conversation History Support</h3>
<p>LightRAG now supports multi-turn dialogue through the conversation history feature. Here&rsquo;s how to use it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> lightrag <span style="color:#f92672">import</span> LightRAG, QueryParam
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize LightRAG</span>
</span></span><span style="display:flex;"><span>rag <span style="color:#f92672">=</span> LightRAG(working_dir<span style="color:#f92672">=</span>WORKING_DIR)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create conversation history</span>
</span></span><span style="display:flex;"><span>conversation_history <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;What is the main character&#39;s attitude towards Christmas?&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;assistant&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;At the beginning of the story, Ebenezer Scrooge has a very negative attitude towards Christmas...&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;How does his attitude change?&#34;</span>}
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create query parameters with conversation history</span>
</span></span><span style="display:flex;"><span>query_param <span style="color:#f92672">=</span> QueryParam(
</span></span><span style="display:flex;"><span>    mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;mix&#34;</span>,  <span style="color:#75715e"># or any other mode: &#34;local&#34;, &#34;global&#34;, &#34;hybrid&#34;</span>
</span></span><span style="display:flex;"><span>    conversation_history<span style="color:#f92672">=</span>conversation_history,  <span style="color:#75715e"># Add the conversation history</span>
</span></span><span style="display:flex;"><span>    history_turns<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>  <span style="color:#75715e"># Number of recent conversation turns to consider</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Make a query that takes into account the conversation history</span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> rag<span style="color:#f92672">.</span>query(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;What causes this change in his character?&#34;</span>,
</span></span><span style="display:flex;"><span>    param<span style="color:#f92672">=</span>query_param
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="custom-prompt-support">Custom Prompt Support</h3>
<p>LightRAG now supports custom prompts for fine-tuned control over the system&rsquo;s behavior. Here&rsquo;s how to use it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> lightrag <span style="color:#f92672">import</span> LightRAG, QueryParam
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize LightRAG</span>
</span></span><span style="display:flex;"><span>rag <span style="color:#f92672">=</span> LightRAG(working_dir<span style="color:#f92672">=</span>WORKING_DIR)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create query parameters</span>
</span></span><span style="display:flex;"><span>query_param <span style="color:#f92672">=</span> QueryParam(
</span></span><span style="display:flex;"><span>    mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;hybrid&#34;</span>,  <span style="color:#75715e"># or other mode: &#34;local&#34;, &#34;global&#34;, &#34;hybrid&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example 1: Using the default system prompt</span>
</span></span><span style="display:flex;"><span>response_default <span style="color:#f92672">=</span> rag<span style="color:#f92672">.</span>query(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;What are the primary benefits of renewable energy?&#34;</span>,
</span></span><span style="display:flex;"><span>    param<span style="color:#f92672">=</span>query_param
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>print(response_default)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example 2: Using a custom prompt</span>
</span></span><span style="display:flex;"><span>custom_prompt <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">You are an expert assistant in environmental science. Provide detailed and structured answers with examples.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>response_custom <span style="color:#f92672">=</span> rag<span style="color:#f92672">.</span>query(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;What are the primary benefits of renewable energy?&#34;</span>,
</span></span><span style="display:flex;"><span>    param<span style="color:#f92672">=</span>query_param,
</span></span><span style="display:flex;"><span>    prompt<span style="color:#f92672">=</span>custom_prompt  <span style="color:#75715e"># Pass the custom prompt</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>print(response_custom)
</span></span></code></pre></div><h4 id="using-open-ai-like-apis">Using Open AI-like APIs</h4>
<ul>
<li>LightRAG also supports Open AI-like chat/embeddings APIs:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">llm_model_func</span>(
</span></span><span style="display:flex;"><span>    prompt, system_prompt<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, history_messages<span style="color:#f92672">=</span>[], keyword_extraction<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, <span style="color:#f92672">**</span>kwargs
</span></span><span style="display:flex;"><span>) <span style="color:#f92672">-&gt;</span> str:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">await</span> openai_complete_if_cache(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;solar-mini&#34;</span>,
</span></span><span style="display:flex;"><span>        prompt,
</span></span><span style="display:flex;"><span>        system_prompt<span style="color:#f92672">=</span>system_prompt,
</span></span><span style="display:flex;"><span>        history_messages<span style="color:#f92672">=</span>history_messages,
</span></span><span style="display:flex;"><span>        api_key<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;UPSTAGE_API_KEY&#34;</span>),
</span></span><span style="display:flex;"><span>        base_url<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://api.upstage.ai/v1/solar&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">**</span>kwargs
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">embedding_func</span>(texts: list[str]) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">await</span> openai_embed(
</span></span><span style="display:flex;"><span>        texts,
</span></span><span style="display:flex;"><span>        model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;solar-embedding-1-large-query&#34;</span>,
</span></span><span style="display:flex;"><span>        api_key<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;UPSTAGE_API_KEY&#34;</span>),
</span></span><span style="display:flex;"><span>        base_url<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://api.upstage.ai/v1/solar&#34;</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>rag <span style="color:#f92672">=</span> LightRAG(
</span></span><span style="display:flex;"><span>    working_dir<span style="color:#f92672">=</span>WORKING_DIR,
</span></span><span style="display:flex;"><span>    llm_model_func<span style="color:#f92672">=</span>llm_model_func,
</span></span><span style="display:flex;"><span>    embedding_func<span style="color:#f92672">=</span>EmbeddingFunc(
</span></span><span style="display:flex;"><span>        embedding_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span>,
</span></span><span style="display:flex;"><span>        max_token_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8192</span>,
</span></span><span style="display:flex;"><span>        func<span style="color:#f92672">=</span>embedding_func
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h4 id="using-hugging-face-models">Using Hugging Face Models</h4>
<ul>
<li>If you want to use Hugging Face models, you only need to set LightRAG as follows:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> lightrag.llm <span style="color:#f92672">import</span> hf_model_complete, hf_embedding
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoModel, AutoTokenizer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> lightrag.utils <span style="color:#f92672">import</span> EmbeddingFunc
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize LightRAG with Hugging Face model</span>
</span></span><span style="display:flex;"><span>rag <span style="color:#f92672">=</span> LightRAG(
</span></span><span style="display:flex;"><span>    working_dir<span style="color:#f92672">=</span>WORKING_DIR,
</span></span><span style="display:flex;"><span>    llm_model_func<span style="color:#f92672">=</span>hf_model_complete,  <span style="color:#75715e"># Use Hugging Face model for text generation</span>
</span></span><span style="display:flex;"><span>    llm_model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;meta-llama/Llama-3.1-8B-Instruct&#39;</span>,  <span style="color:#75715e"># Model name from Hugging Face</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Use Hugging Face embedding function</span>
</span></span><span style="display:flex;"><span>    embedding_func<span style="color:#f92672">=</span>EmbeddingFunc(
</span></span><span style="display:flex;"><span>        embedding_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">384</span>,
</span></span><span style="display:flex;"><span>        max_token_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5000</span>,
</span></span><span style="display:flex;"><span>        func<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> texts: hf_embedding(
</span></span><span style="display:flex;"><span>            texts,
</span></span><span style="display:flex;"><span>            tokenizer<span style="color:#f92672">=</span>AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>),
</span></span><span style="display:flex;"><span>            embed_model<span style="color:#f92672">=</span>AutoModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    ),
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h4 id="using-ollama-models">Using Ollama Models</h4>
<h5 id="overview">Overview</h5>
<p>If you want to use Ollama models, you need to pull model you plan to use and embedding model, for example <code>nomic-embed-text</code>.</p>
<p>Then you only need to set LightRAG as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> lightrag.llm.ollama <span style="color:#f92672">import</span> ollama_model_complete, ollama_embed
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> lightrag.utils <span style="color:#f92672">import</span> EmbeddingFunc
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize LightRAG with Ollama model</span>
</span></span><span style="display:flex;"><span>rag <span style="color:#f92672">=</span> LightRAG(
</span></span><span style="display:flex;"><span>    working_dir<span style="color:#f92672">=</span>WORKING_DIR,
</span></span><span style="display:flex;"><span>    llm_model_func<span style="color:#f92672">=</span>ollama_model_complete,  <span style="color:#75715e"># Use Ollama model for text generation</span>
</span></span><span style="display:flex;"><span>    llm_model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;your_model_name&#39;</span>, <span style="color:#75715e"># Your model name</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Use Ollama embedding function</span>
</span></span><span style="display:flex;"><span>    embedding_func<span style="color:#f92672">=</span>EmbeddingFunc(
</span></span><span style="display:flex;"><span>        embedding_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>,
</span></span><span style="display:flex;"><span>        max_token_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8192</span>,
</span></span><span style="display:flex;"><span>        func<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> texts: ollama_embed(
</span></span><span style="display:flex;"><span>            texts,
</span></span><span style="display:flex;"><span>            embed_model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;nomic-embed-text&#34;</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    ),
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h5 id="increasing-context-size">Increasing context size</h5>
<p>In order for LightRAG to work context should be at least 32k tokens. By default Ollama models have context size of 8k. You can achieve this using one of two ways:</p>
<h5 id="increasing-the-num_ctx-parameter-in-modelfile">Increasing the <code>num_ctx</code> parameter in Modelfile.</h5>
<ol>
<li>Pull the model:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama pull qwen2
</span></span></code></pre></div><ol start="2">
<li>Display the model file:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama show --modelfile qwen2 &gt; Modelfile
</span></span></code></pre></div><ol start="3">
<li>Edit the Modelfile by adding the following line:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>PARAMETER num_ctx <span style="color:#ae81ff">32768</span>
</span></span></code></pre></div><ol start="4">
<li>Create the modified model:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama create -f Modelfile qwen2m
</span></span></code></pre></div><h5 id="setup-num_ctx-via-ollama-api">Setup <code>num_ctx</code> via Ollama API.</h5>
<p>Tiy can use <code>llm_model_kwargs</code> param to configure ollama:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rag <span style="color:#f92672">=</span> LightRAG(
</span></span><span style="display:flex;"><span>    working_dir<span style="color:#f92672">=</span>WORKING_DIR,
</span></span><span style="display:flex;"><span>    llm_model_func<span style="color:#f92672">=</span>ollama_model_complete,  <span style="color:#75715e"># Use Ollama model for text generation</span>
</span></span><span style="display:flex;"><span>    llm_model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;your_model_name&#39;</span>, <span style="color:#75715e"># Your model name</span>
</span></span><span style="display:flex;"><span>    llm_model_kwargs<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;options&#34;</span>: {<span style="color:#e6db74">&#34;num_ctx&#34;</span>: <span style="color:#ae81ff">32768</span>}},
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Use Ollama embedding function</span>
</span></span><span style="display:flex;"><span>    embedding_func<span style="color:#f92672">=</span>EmbeddingFunc(
</span></span><span style="display:flex;"><span>        embedding_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>,
</span></span><span style="display:flex;"><span>        max_token_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8192</span>,
</span></span><span style="display:flex;"><span>        func<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> texts: ollama_embedding(
</span></span><span style="display:flex;"><span>            texts,
</span></span><span style="display:flex;"><span>            embed_model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;nomic-embed-text&#34;</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    ),
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h5 id="fully-functional-example">Fully functional example</h5>
<p>There fully functional example <code>examples/lightrag_ollama_demo.py</code> that utilizes <code>gemma2:2b</code> model, runs only 4 requests in parallel and set context size to 32k.</p>
<h5 id="using-thinking-models-eg-deepseek">Using &ldquo;Thinking&rdquo; Models (e.g., DeepSeek)</h5>
<p>To return only the model&rsquo;s response, you can pass <code>reasoning_tag</code> in <code>llm_model_kwargs</code>.</p>
<p>For example, for DeepSeek models, <code>reasoning_tag</code> should be set to <code>think</code>.</p>
<h5 id="low-ram-gpus">Low RAM GPUs</h5>
<p>In order to run this experiment on low RAM GPU you should select small model and tune context window (increasing context increase memory consumption). For example, running this ollama example on repurposed mining GPU with 6Gb of RAM required to set context size to 26k while using <code>gemma2:2b</code>. It was able to find 197 entities and 19 relations on <code>book.txt</code>.</p>
<h4 id="query-param">Query Param</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QueryParam</span>:
</span></span><span style="display:flex;"><span>    mode: Literal[<span style="color:#e6db74">&#34;local&#34;</span>, <span style="color:#e6db74">&#34;global&#34;</span>, <span style="color:#e6db74">&#34;hybrid&#34;</span>, <span style="color:#e6db74">&#34;naive&#34;</span>, <span style="color:#e6db74">&#34;mix&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;global&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Specifies the retrieval mode:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    - &#34;local&#34;: Focuses on context-dependent information.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    - &#34;global&#34;: Utilizes global knowledge.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    - &#34;hybrid&#34;: Combines local and global retrieval methods.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    - &#34;naive&#34;: Performs a basic search without advanced techniques.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    - &#34;mix&#34;: Integrates knowledge graph and vector retrieval.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    only_need_context: bool <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;If True, only returns the retrieved context without generating a response.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    response_type: str <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Multiple Paragraphs&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Defines the response format. Examples: &#39;Multiple Paragraphs&#39;, &#39;Single Paragraph&#39;, &#39;Bullet Points&#39;.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    top_k: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">60</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Number of top items to retrieve. Represents entities in &#39;local&#39; mode and relationships in &#39;global&#39; mode.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    max_token_for_text_unit: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">4000</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Maximum number of tokens allowed for each retrieved text chunk.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    max_token_for_global_context: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">4000</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Maximum number of tokens allocated for relationship descriptions in global retrieval.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    max_token_for_local_context: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">4000</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Maximum number of tokens allocated for entity descriptions in local retrieval.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span></code></pre></div><blockquote>
<p>default value of Top_k can be change by environment  variables  TOP_K.</p></blockquote>
<h4 id="batch-insert">Batch Insert</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Basic Batch Insert: Insert multiple texts at once</span>
</span></span><span style="display:flex;"><span>rag<span style="color:#f92672">.</span>insert([<span style="color:#e6db74">&#34;TEXT1&#34;</span>, <span style="color:#e6db74">&#34;TEXT2&#34;</span>,<span style="color:#f92672">...</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Batch Insert with custom batch size configuration</span>
</span></span><span style="display:flex;"><span>rag <span style="color:#f92672">=</span> LightRAG(
</span></span><span style="display:flex;"><span>    working_dir<span style="color:#f92672">=</span>WORKING_DIR,
</span></span><span style="display:flex;"><span>    addon_params<span style="color:#f92672">=</span>{
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;insert_batch_size&#34;</span>: <span style="color:#ae81ff">20</span>  <span style="color:#75715e"># Process 20 documents per batch</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>rag<span style="color:#f92672">.</span>insert([<span style="color:#e6db74">&#34;TEXT1&#34;</span>, <span style="color:#e6db74">&#34;TEXT2&#34;</span>, <span style="color:#e6db74">&#34;TEXT3&#34;</span>, <span style="color:#f92672">...</span>])  <span style="color:#75715e"># Documents will be processed in batches of 20</span>
</span></span></code></pre></div><p>The <code>insert_batch_size</code> parameter in <code>addon_params</code> controls how many documents are processed in each batch during insertion. This is useful for:</p>
<ul>
<li>Managing memory usage with large document collections</li>
<li>Optimizing processing speed</li>
<li>Providing better progress tracking</li>
<li>Default value is 10 if not specified</li>
</ul>
<h4 id="incremental-insert">Incremental Insert</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Incremental Insert: Insert new documents into an existing LightRAG instance</span>
</span></span><span style="display:flex;"><span>rag <span style="color:#f92672">=</span> LightRAG(
</span></span><span style="display:flex;"><span>     working_dir<span style="color:#f92672">=</span>WORKING_DIR,
</span></span><span style="display:flex;"><span>     llm_model_func<span style="color:#f92672">=</span>llm_model_func,
</span></span><span style="display:flex;"><span>     embedding_func<span style="color:#f92672">=</span>EmbeddingFunc(
</span></span><span style="display:flex;"><span>          embedding_dim<span style="color:#f92672">=</span>embedding_dimension,
</span></span><span style="display:flex;"><span>          max_token_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8192</span>,
</span></span><span style="display:flex;"><span>          func<span style="color:#f92672">=</span>embedding_func,
</span></span><span style="display:flex;"><span>     ),
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;./newText.txt&#34;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    rag<span style="color:#f92672">.</span>insert(f<span style="color:#f92672">.</span>read())
</span></span></code></pre></div><h4 id="insert-using-pipeline">Insert using Pipeline</h4>
<p>The <code>apipeline_enqueue_documents</code> and <code>apipeline_process_enqueue_documents</code> functions allow you to perform incremental insertion of documents into the graph.</p>
<p>This is useful for scenarios where you want to process documents in the background while still allowing the main thread to continue executing.</p>
<p>And using a routine to process news documents.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rag <span style="color:#f92672">=</span> LightRAG(<span style="color:#f92672">..</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">await</span> rag<span style="color:#f92672">.</span>apipeline_enqueue_documents(string_or_strings)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Your routine in loop</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">await</span> rag<span style="color:#f92672">.</span>apipeline_process_enqueue_documents(string_or_strings)
</span></span></code></pre></div><h4 id="separate-keyword-extraction">Separate Keyword Extraction</h4>
<p>We&rsquo;ve introduced a new function <code>query_with_separate_keyword_extraction</code> to enhance the keyword extraction capabilities. This function separates the keyword extraction process from the user&rsquo;s prompt, focusing solely on the query to improve the relevance of extracted keywords.</p>
<h5 id="how-it-works">How It Works?</h5>
<p>The function operates by dividing the input into two parts:</p>
<ul>
<li><code>User Query</code></li>
<li><code>Prompt</code></li>
</ul>
<p>It then performs keyword extraction exclusively on the <code>user query</code>. This separation ensures that the extraction process is focused and relevant, unaffected by any additional language in the <code>prompt</code>. It also allows the <code>prompt</code> to serve purely for response formatting, maintaining the intent and clarity of the user&rsquo;s original question.</p>
<h5 id="usage-example">Usage Example</h5>
<p>This <code>example</code> shows how to tailor the function for educational content, focusing on detailed explanations for older students.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rag<span style="color:#f92672">.</span>query_with_separate_keyword_extraction(
</span></span><span style="display:flex;"><span>    query<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Explain the law of gravity&#34;</span>,
</span></span><span style="display:flex;"><span>    prompt<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Provide a detailed explanation suitable for high school students studying physics.&#34;</span>,
</span></span><span style="display:flex;"><span>    param<span style="color:#f92672">=</span>QueryParam(mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;hybrid&#34;</span>)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h4 id="using-neo4j-for-storage">Using Neo4J for Storage</h4>
<ul>
<li>For production level scenarios you will most likely want to leverage an enterprise solution</li>
<li>for KG storage. Running Neo4J in Docker is recommended for seamless local testing.</li>
<li>See: <a href="https://hub.docker.com/_/neo4j">https://hub.docker.com/_/neo4j</a></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>export NEO4J_URI<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;neo4j://localhost:7687&#34;</span>
</span></span><span style="display:flex;"><span>export NEO4J_USERNAME<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;neo4j&#34;</span>
</span></span><span style="display:flex;"><span>export NEO4J_PASSWORD<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;password&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># When you launch the project be sure to override the default KG: NetworkX</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># by specifying kg=&#34;Neo4JStorage&#34;.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Note: Default settings use NetworkX</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize LightRAG with Neo4J implementation.</span>
</span></span><span style="display:flex;"><span>WORKING_DIR <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;./local_neo4jWorkDir&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>rag <span style="color:#f92672">=</span> LightRAG(
</span></span><span style="display:flex;"><span>    working_dir<span style="color:#f92672">=</span>WORKING_DIR,
</span></span><span style="display:flex;"><span>    llm_model_func<span style="color:#f92672">=</span>gpt_4o_mini_complete,  <span style="color:#75715e"># Use gpt_4o_mini_complete LLM model</span>
</span></span><span style="display:flex;"><span>    graph_storage<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Neo4JStorage&#34;</span>, <span style="color:#75715e">#&lt;-----------override KG default</span>
</span></span><span style="display:flex;"><span>    log_level<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;DEBUG&#34;</span>  <span style="color:#75715e">#&lt;-----------override log_level default</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>see test_neo4j.py for a working example.</p>
<h4 id="using-postgresql-for-storage">Using PostgreSQL for Storage</h4>
<p>For production level scenarios you will most likely want to leverage an enterprise solution. PostgreSQL can provide a one-stop solution for you as KV store, VectorDB (pgvector) and GraphDB (apache AGE).</p>
<ul>
<li>PostgreSQL is lightweight,the whole binary distribution including all necessary plugins can be zipped to 40MB: Ref to <a href="https://github.com/ShanGor/apache-age-windows/releases/tag/PG17%2Fv1.5.0-rc0">Windows Release</a> as it is easy to install for Linux/Mac.</li>
<li>If you prefer docker, please start with this image if you are a beginner to avoid hiccups (DO read the overview): <a href="https://hub.docker.com/r/shangor/postgres-for-rag">https://hub.docker.com/r/shangor/postgres-for-rag</a></li>
<li>How to start? Ref to: <a href="https://github.com/HKUDS/LightRAG/blob/main/examples/lightrag_zhipu_postgres_demo.py">examples/lightrag_zhipu_postgres_demo.py</a></li>
<li>Create index for AGE example: (Change below <code>dickens</code> to your graph name if necessary)
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sql" data-lang="sql"><span style="display:flex;"><span><span style="color:#66d9ef">load</span> <span style="color:#e6db74">&#39;age&#39;</span>;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">SET</span> search_path <span style="color:#f92672">=</span> ag_catalog, <span style="color:#e6db74">&#34;$user&#34;</span>, <span style="color:#66d9ef">public</span>;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">INDEX</span> CONCURRENTLY entity_p_idx <span style="color:#66d9ef">ON</span> dickens.<span style="color:#e6db74">&#34;Entity&#34;</span> (id);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">INDEX</span> CONCURRENTLY vertex_p_idx <span style="color:#66d9ef">ON</span> dickens.<span style="color:#e6db74">&#34;_ag_label_vertex&#34;</span> (id);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">INDEX</span> CONCURRENTLY directed_p_idx <span style="color:#66d9ef">ON</span> dickens.<span style="color:#e6db74">&#34;DIRECTED&#34;</span> (id);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">INDEX</span> CONCURRENTLY directed_eid_idx <span style="color:#66d9ef">ON</span> dickens.<span style="color:#e6db74">&#34;DIRECTED&#34;</span> (end_id);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">INDEX</span> CONCURRENTLY directed_sid_idx <span style="color:#66d9ef">ON</span> dickens.<span style="color:#e6db74">&#34;DIRECTED&#34;</span> (start_id);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">INDEX</span> CONCURRENTLY directed_seid_idx <span style="color:#66d9ef">ON</span> dickens.<span style="color:#e6db74">&#34;DIRECTED&#34;</span> (start_id,end_id);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">INDEX</span> CONCURRENTLY edge_p_idx <span style="color:#66d9ef">ON</span> dickens.<span style="color:#e6db74">&#34;_ag_label_edge&#34;</span> (id);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">INDEX</span> CONCURRENTLY edge_sid_idx <span style="color:#66d9ef">ON</span> dickens.<span style="color:#e6db74">&#34;_ag_label_edge&#34;</span> (start_id);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">INDEX</span> CONCURRENTLY edge_eid_idx <span style="color:#66d9ef">ON</span> dickens.<span style="color:#e6db74">&#34;_ag_label_edge&#34;</span> (end_id);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">INDEX</span> CONCURRENTLY edge_seid_idx <span style="color:#66d9ef">ON</span> dickens.<span style="color:#e6db74">&#34;_ag_label_edge&#34;</span> (start_id,end_id);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">create</span> <span style="color:#66d9ef">INDEX</span> CONCURRENTLY vertex_idx_node_id <span style="color:#66d9ef">ON</span> dickens.<span style="color:#e6db74">&#34;_ag_label_vertex&#34;</span> (ag_catalog.agtype_access_operator(properties, <span style="color:#e6db74">&#39;&#34;node_id&#34;&#39;</span>::agtype));
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">create</span> <span style="color:#66d9ef">INDEX</span> CONCURRENTLY entity_idx_node_id <span style="color:#66d9ef">ON</span> dickens.<span style="color:#e6db74">&#34;Entity&#34;</span> (ag_catalog.agtype_access_operator(properties, <span style="color:#e6db74">&#39;&#34;node_id&#34;&#39;</span>::agtype));
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">INDEX</span> CONCURRENTLY entity_node_id_gin_idx <span style="color:#66d9ef">ON</span> dickens.<span style="color:#e6db74">&#34;Entity&#34;</span> <span style="color:#66d9ef">using</span> gin(properties);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">ALTER</span> <span style="color:#66d9ef">TABLE</span> dickens.<span style="color:#e6db74">&#34;DIRECTED&#34;</span> <span style="color:#66d9ef">CLUSTER</span> <span style="color:#66d9ef">ON</span> directed_sid_idx;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">-- drop if necessary
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">drop</span> <span style="color:#66d9ef">INDEX</span> entity_p_idx;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">drop</span> <span style="color:#66d9ef">INDEX</span> vertex_p_idx;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">drop</span> <span style="color:#66d9ef">INDEX</span> directed_p_idx;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">drop</span> <span style="color:#66d9ef">INDEX</span> directed_eid_idx;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">drop</span> <span style="color:#66d9ef">INDEX</span> directed_sid_idx;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">drop</span> <span style="color:#66d9ef">INDEX</span> directed_seid_idx;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">drop</span> <span style="color:#66d9ef">INDEX</span> edge_p_idx;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">drop</span> <span style="color:#66d9ef">INDEX</span> edge_sid_idx;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">drop</span> <span style="color:#66d9ef">INDEX</span> edge_eid_idx;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">drop</span> <span style="color:#66d9ef">INDEX</span> edge_seid_idx;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">drop</span> <span style="color:#66d9ef">INDEX</span> vertex_idx_node_id;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">drop</span> <span style="color:#66d9ef">INDEX</span> entity_idx_node_id;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">drop</span> <span style="color:#66d9ef">INDEX</span> entity_node_id_gin_idx;
</span></span></code></pre></div></li>
<li>Known issue of the Apache AGE: The released versions got below issue:
<blockquote>
<p>You might find that the properties of the nodes/edges are empty.
It is a known issue of the release version: <a href="https://github.com/apache/age/pull/1721">https://github.com/apache/age/pull/1721</a></p>
<p>You can Compile the AGE from source code and fix it.</p></blockquote>
</li>
</ul>
<h3 id="using-faiss-for-storage">Using Faiss for Storage</h3>
<ul>
<li>Install the required dependencies:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install faiss-cpu
</span></span></code></pre></div><p>You can also install <code>faiss-gpu</code> if you have GPU support.</p>
<ul>
<li>Here we are using <code>sentence-transformers</code> but you can also use <code>OpenAIEmbedding</code> model with <code>3072</code> dimensions.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">embedding_func</span>(texts: list[str]) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> SentenceTransformer(<span style="color:#e6db74">&#39;all-MiniLM-L6-v2&#39;</span>)
</span></span><span style="display:flex;"><span>    embeddings <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>encode(texts, convert_to_numpy<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> embeddings
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize LightRAG with the LLM model function and embedding function</span>
</span></span><span style="display:flex;"><span>    rag <span style="color:#f92672">=</span> LightRAG(
</span></span><span style="display:flex;"><span>        working_dir<span style="color:#f92672">=</span>WORKING_DIR,
</span></span><span style="display:flex;"><span>        llm_model_func<span style="color:#f92672">=</span>llm_model_func,
</span></span><span style="display:flex;"><span>        embedding_func<span style="color:#f92672">=</span>EmbeddingFunc(
</span></span><span style="display:flex;"><span>            embedding_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">384</span>,
</span></span><span style="display:flex;"><span>            max_token_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8192</span>,
</span></span><span style="display:flex;"><span>            func<span style="color:#f92672">=</span>embedding_func,
</span></span><span style="display:flex;"><span>        ),
</span></span><span style="display:flex;"><span>        vector_storage<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;FaissVectorDBStorage&#34;</span>,
</span></span><span style="display:flex;"><span>        vector_db_storage_cls_kwargs<span style="color:#f92672">=</span>{
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;cosine_better_than_threshold&#34;</span>: <span style="color:#ae81ff">0.3</span>  <span style="color:#75715e"># Your desired threshold</span>
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div><h4 id="insert-custom-kg">Insert Custom KG</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rag <span style="color:#f92672">=</span> LightRAG(
</span></span><span style="display:flex;"><span>     working_dir<span style="color:#f92672">=</span>WORKING_DIR,
</span></span><span style="display:flex;"><span>     llm_model_func<span style="color:#f92672">=</span>llm_model_func,
</span></span><span style="display:flex;"><span>     embedding_func<span style="color:#f92672">=</span>EmbeddingFunc(
</span></span><span style="display:flex;"><span>          embedding_dim<span style="color:#f92672">=</span>embedding_dimension,
</span></span><span style="display:flex;"><span>          max_token_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8192</span>,
</span></span><span style="display:flex;"><span>          func<span style="color:#f92672">=</span>embedding_func,
</span></span><span style="display:flex;"><span>     ),
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>custom_kg <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;entities&#34;</span>: [
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;entity_name&#34;</span>: <span style="color:#e6db74">&#34;CompanyA&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;entity_type&#34;</span>: <span style="color:#e6db74">&#34;Organization&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;description&#34;</span>: <span style="color:#e6db74">&#34;A major technology company&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;source_id&#34;</span>: <span style="color:#e6db74">&#34;Source1&#34;</span>
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;entity_name&#34;</span>: <span style="color:#e6db74">&#34;ProductX&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;entity_type&#34;</span>: <span style="color:#e6db74">&#34;Product&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;description&#34;</span>: <span style="color:#e6db74">&#34;A popular product developed by CompanyA&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;source_id&#34;</span>: <span style="color:#e6db74">&#34;Source1&#34;</span>
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;relationships&#34;</span>: [
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;src_id&#34;</span>: <span style="color:#e6db74">&#34;CompanyA&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;tgt_id&#34;</span>: <span style="color:#e6db74">&#34;ProductX&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;description&#34;</span>: <span style="color:#e6db74">&#34;CompanyA develops ProductX&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;keywords&#34;</span>: <span style="color:#e6db74">&#34;develop, produce&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;weight&#34;</span>: <span style="color:#ae81ff">1.0</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;source_id&#34;</span>: <span style="color:#e6db74">&#34;Source1&#34;</span>
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;chunks&#34;</span>: [
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;ProductX, developed by CompanyA, has revolutionized the market with its cutting-edge features.&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;source_id&#34;</span>: <span style="color:#e6db74">&#34;Source1&#34;</span>,
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;PersonA is a prominent researcher at UniversityB, focusing on artificial intelligence and machine learning.&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;source_id&#34;</span>: <span style="color:#e6db74">&#34;Source2&#34;</span>,
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;None&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;source_id&#34;</span>: <span style="color:#e6db74">&#34;UNKNOWN&#34;</span>,
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>rag<span style="color:#f92672">.</span>insert_custom_kg(custom_kg)
</span></span></code></pre></div><h4 id="delete">Delete</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>rag <span style="color:#f92672">=</span> LightRAG(
</span></span><span style="display:flex;"><span>     working_dir<span style="color:#f92672">=</span>WORKING_DIR,
</span></span><span style="display:flex;"><span>     llm_model_func<span style="color:#f92672">=</span>llm_model_func,
</span></span><span style="display:flex;"><span>     embedding_func<span style="color:#f92672">=</span>EmbeddingFunc(
</span></span><span style="display:flex;"><span>          embedding_dim<span style="color:#f92672">=</span>embedding_dimension,
</span></span><span style="display:flex;"><span>          max_token_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8192</span>,
</span></span><span style="display:flex;"><span>          func<span style="color:#f92672">=</span>embedding_func,
</span></span><span style="display:flex;"><span>     ),
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  Delete Entity: Deleting entities by their names</span>
</span></span><span style="display:flex;"><span>rag<span style="color:#f92672">.</span>delete_by_entity(<span style="color:#e6db74">&#34;Project Gutenberg&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  Delete Document: Deleting entities and relationships associated with the document by doc id</span>
</span></span><span style="display:flex;"><span>rag<span style="color:#f92672">.</span>delete_by_doc_id(<span style="color:#e6db74">&#34;doc_id&#34;</span>)
</span></span></code></pre></div><h4 id="multi-file-type-support">Multi-file Type Support</h4>
<p>The <code>textract</code> supports reading file types such as TXT, DOCX, PPTX, CSV, and PDF.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> textract
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>file_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;TEXT.pdf&#39;</span>
</span></span><span style="display:flex;"><span>text_content <span style="color:#f92672">=</span> textract<span style="color:#f92672">.</span>process(file_path)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>rag<span style="color:#f92672">.</span>insert(text_content<span style="color:#f92672">.</span>decode(<span style="color:#e6db74">&#39;utf-8&#39;</span>))
</span></span></code></pre></div><h4 id="graph-visualization">Graph Visualization</h4>
<h5 id="graph-visualization-with-html">Graph visualization with html</h5>
<ul>
<li>The following code can be found in <code>examples/graph_visual_with_html.py</code></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> networkx <span style="color:#66d9ef">as</span> nx
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyvis.network <span style="color:#f92672">import</span> Network
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load the GraphML file</span>
</span></span><span style="display:flex;"><span>G <span style="color:#f92672">=</span> nx<span style="color:#f92672">.</span>read_graphml(<span style="color:#e6db74">&#39;./dickens/graph_chunk_entity_relation.graphml&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create a Pyvis network</span>
</span></span><span style="display:flex;"><span>net <span style="color:#f92672">=</span> Network(notebook<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Convert NetworkX graph to Pyvis network</span>
</span></span><span style="display:flex;"><span>net<span style="color:#f92672">.</span>from_nx(G)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Save and display the network</span>
</span></span><span style="display:flex;"><span>net<span style="color:#f92672">.</span>show(<span style="color:#e6db74">&#39;knowledge_graph.html&#39;</span>)
</span></span></code></pre></div><h5 id="graph-visualization-with-neo4j">Graph visualization with Neo4j</h5>
<ul>
<li>The following code can be found in <code>examples/graph_visual_with_neo4j.py</code></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> json
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> lightrag.utils <span style="color:#f92672">import</span> xml_to_json
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> neo4j <span style="color:#f92672">import</span> GraphDatabase
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Constants</span>
</span></span><span style="display:flex;"><span>WORKING_DIR <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;./dickens&#34;</span>
</span></span><span style="display:flex;"><span>BATCH_SIZE_NODES <span style="color:#f92672">=</span> <span style="color:#ae81ff">500</span>
</span></span><span style="display:flex;"><span>BATCH_SIZE_EDGES <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Neo4j connection credentials</span>
</span></span><span style="display:flex;"><span>NEO4J_URI <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;bolt://localhost:7687&#34;</span>
</span></span><span style="display:flex;"><span>NEO4J_USERNAME <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;neo4j&#34;</span>
</span></span><span style="display:flex;"><span>NEO4J_PASSWORD <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;your_password&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">convert_xml_to_json</span>(xml_path, output_path):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Converts XML file to JSON and saves the output.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>exists(xml_path):
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Error: File not found - </span><span style="color:#e6db74">{</span>xml_path<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    json_data <span style="color:#f92672">=</span> xml_to_json(xml_path)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> json_data:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> open(output_path, <span style="color:#e6db74">&#39;w&#39;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;utf-8&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>            json<span style="color:#f92672">.</span>dump(json_data, f, ensure_ascii<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, indent<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;JSON file created: </span><span style="color:#e6db74">{</span>output_path<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> json_data
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;Failed to create JSON data&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">process_in_batches</span>(tx, query, data, batch_size):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Process data in batches and execute the given query.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>, len(data), batch_size):
</span></span><span style="display:flex;"><span>        batch <span style="color:#f92672">=</span> data[i:i <span style="color:#f92672">+</span> batch_size]
</span></span><span style="display:flex;"><span>        tx<span style="color:#f92672">.</span>run(query, {<span style="color:#e6db74">&#34;nodes&#34;</span>: batch} <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;nodes&#34;</span> <span style="color:#f92672">in</span> query <span style="color:#66d9ef">else</span> {<span style="color:#e6db74">&#34;edges&#34;</span>: batch})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Paths</span>
</span></span><span style="display:flex;"><span>    xml_file <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(WORKING_DIR, <span style="color:#e6db74">&#39;graph_chunk_entity_relation.graphml&#39;</span>)
</span></span><span style="display:flex;"><span>    json_file <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(WORKING_DIR, <span style="color:#e6db74">&#39;graph_data.json&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Convert XML to JSON</span>
</span></span><span style="display:flex;"><span>    json_data <span style="color:#f92672">=</span> convert_xml_to_json(xml_file, json_file)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> json_data <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Load nodes and edges</span>
</span></span><span style="display:flex;"><span>    nodes <span style="color:#f92672">=</span> json_data<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;nodes&#39;</span>, [])
</span></span><span style="display:flex;"><span>    edges <span style="color:#f92672">=</span> json_data<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;edges&#39;</span>, [])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Neo4j queries</span>
</span></span><span style="display:flex;"><span>    create_nodes_query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    UNWIND $nodes AS node
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    MERGE (e:Entity {id: node.id})
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    SET e.entity_type = node.entity_type,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        e.description = node.description,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        e.source_id = node.source_id,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        e.displayName = node.id
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    REMOVE e:Entity
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    WITH e, node
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    CALL apoc.create.addLabels(e, [node.entity_type]) YIELD node AS labeledNode
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    RETURN count(*)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    create_edges_query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    UNWIND $edges AS edge
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    MATCH (source {id: edge.source})
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    MATCH (target {id: edge.target})
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    WITH source, target, edge,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">         CASE
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            WHEN edge.keywords CONTAINS &#39;lead&#39; THEN &#39;lead&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            WHEN edge.keywords CONTAINS &#39;participate&#39; THEN &#39;participate&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            WHEN edge.keywords CONTAINS &#39;uses&#39; THEN &#39;uses&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            WHEN edge.keywords CONTAINS &#39;located&#39; THEN &#39;located&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            WHEN edge.keywords CONTAINS &#39;occurs&#39; THEN &#39;occurs&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">           ELSE REPLACE(SPLIT(edge.keywords, &#39;,&#39;)[0], &#39;</span><span style="color:#ae81ff">\&#34;</span><span style="color:#e6db74">&#39;, &#39;&#39;)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">         END AS relType
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    CALL apoc.create.relationship(source, relType, {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      weight: edge.weight,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      description: edge.description,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      keywords: edge.keywords,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      source_id: edge.source_id
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    }, target) YIELD rel
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    RETURN count(*)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    set_displayname_and_labels_query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    MATCH (n)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    SET n.displayName = n.id
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    WITH n
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    CALL apoc.create.setLabels(n, [n.entity_type]) YIELD node
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    RETURN count(*)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Create a Neo4j driver</span>
</span></span><span style="display:flex;"><span>    driver <span style="color:#f92672">=</span> GraphDatabase<span style="color:#f92672">.</span>driver(NEO4J_URI, auth<span style="color:#f92672">=</span>(NEO4J_USERNAME, NEO4J_PASSWORD))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Execute queries in batches</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> driver<span style="color:#f92672">.</span>session() <span style="color:#66d9ef">as</span> session:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Insert nodes in batches</span>
</span></span><span style="display:flex;"><span>            session<span style="color:#f92672">.</span>execute_write(process_in_batches, create_nodes_query, nodes, BATCH_SIZE_NODES)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Insert edges in batches</span>
</span></span><span style="display:flex;"><span>            session<span style="color:#f92672">.</span>execute_write(process_in_batches, create_edges_query, edges, BATCH_SIZE_EDGES)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Set displayName and labels</span>
</span></span><span style="display:flex;"><span>            session<span style="color:#f92672">.</span>run(set_displayname_and_labels_query)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">Exception</span> <span style="color:#66d9ef">as</span> e:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Error occurred: </span><span style="color:#e6db74">{</span>e<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">finally</span>:
</span></span><span style="display:flex;"><span>        driver<span style="color:#f92672">.</span>close()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    main()
</span></span></code></pre></div><h3 id="lightrag-init-parameters">LightRAG init parameters</h3>
<table>
  <thead>
      <tr>
          <th><strong>Parameter</strong></th>
          <th><strong>Type</strong></th>
          <th><strong>Explanation</strong></th>
          <th><strong>Default</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>working_dir</strong></td>
          <td><code>str</code></td>
          <td>Directory where the cache will be stored</td>
          <td><code>lightrag_cache+timestamp</code></td>
      </tr>
      <tr>
          <td><strong>kv_storage</strong></td>
          <td><code>str</code></td>
          <td>Storage type for documents and text chunks. Supported types: <code>JsonKVStorage</code>, <code>OracleKVStorage</code></td>
          <td><code>JsonKVStorage</code></td>
      </tr>
      <tr>
          <td><strong>vector_storage</strong></td>
          <td><code>str</code></td>
          <td>Storage type for embedding vectors. Supported types: <code>NanoVectorDBStorage</code>, <code>OracleVectorDBStorage</code></td>
          <td><code>NanoVectorDBStorage</code></td>
      </tr>
      <tr>
          <td><strong>graph_storage</strong></td>
          <td><code>str</code></td>
          <td>Storage type for graph edges and nodes. Supported types: <code>NetworkXStorage</code>, <code>Neo4JStorage</code>, <code>OracleGraphStorage</code></td>
          <td><code>NetworkXStorage</code></td>
      </tr>
      <tr>
          <td><strong>log_level</strong></td>
          <td></td>
          <td>Log level for application runtime</td>
          <td><code>logging.DEBUG</code></td>
      </tr>
      <tr>
          <td><strong>chunk_token_size</strong></td>
          <td><code>int</code></td>
          <td>Maximum token size per chunk when splitting documents</td>
          <td><code>1200</code></td>
      </tr>
      <tr>
          <td><strong>chunk_overlap_token_size</strong></td>
          <td><code>int</code></td>
          <td>Overlap token size between two chunks when splitting documents</td>
          <td><code>100</code></td>
      </tr>
      <tr>
          <td><strong>tiktoken_model_name</strong></td>
          <td><code>str</code></td>
          <td>Model name for the Tiktoken encoder used to calculate token numbers</td>
          <td><code>gpt-4o-mini</code></td>
      </tr>
      <tr>
          <td><strong>entity_extract_max_gleaning</strong></td>
          <td><code>int</code></td>
          <td>Number of loops in the entity extraction process, appending history messages</td>
          <td><code>1</code></td>
      </tr>
      <tr>
          <td><strong>entity_summary_to_max_tokens</strong></td>
          <td><code>int</code></td>
          <td>Maximum token size for each entity summary</td>
          <td><code>500</code></td>
      </tr>
      <tr>
          <td><strong>node_embedding_algorithm</strong></td>
          <td><code>str</code></td>
          <td>Algorithm for node embedding (currently not used)</td>
          <td><code>node2vec</code></td>
      </tr>
      <tr>
          <td><strong>node2vec_params</strong></td>
          <td><code>dict</code></td>
          <td>Parameters for node embedding</td>
          <td><code>{&quot;dimensions&quot;: 1536,&quot;num_walks&quot;: 10,&quot;walk_length&quot;: 40,&quot;window_size&quot;: 2,&quot;iterations&quot;: 3,&quot;random_seed&quot;: 3,}</code></td>
      </tr>
      <tr>
          <td><strong>embedding_func</strong></td>
          <td><code>EmbeddingFunc</code></td>
          <td>Function to generate embedding vectors from text</td>
          <td><code>openai_embed</code></td>
      </tr>
      <tr>
          <td><strong>embedding_batch_num</strong></td>
          <td><code>int</code></td>
          <td>Maximum batch size for embedding processes (multiple texts sent per batch)</td>
          <td><code>32</code></td>
      </tr>
      <tr>
          <td><strong>embedding_func_max_async</strong></td>
          <td><code>int</code></td>
          <td>Maximum number of concurrent asynchronous embedding processes</td>
          <td><code>16</code></td>
      </tr>
      <tr>
          <td><strong>llm_model_func</strong></td>
          <td><code>callable</code></td>
          <td>Function for LLM generation</td>
          <td><code>gpt_4o_mini_complete</code></td>
      </tr>
      <tr>
          <td><strong>llm_model_name</strong></td>
          <td><code>str</code></td>
          <td>LLM model name for generation</td>
          <td><code>meta-llama/Llama-3.2-1B-Instruct</code></td>
      </tr>
      <tr>
          <td><strong>llm_model_max_token_size</strong></td>
          <td><code>int</code></td>
          <td>Maximum token size for LLM generation (affects entity relation summaries)</td>
          <td><code>32768</code>（default value changed by  env var MAX_TOKENS)</td>
      </tr>
      <tr>
          <td><strong>llm_model_max_async</strong></td>
          <td><code>int</code></td>
          <td>Maximum number of concurrent asynchronous LLM processes</td>
          <td><code>16</code>（default value changed by  env var MAX_ASYNC)</td>
      </tr>
      <tr>
          <td><strong>llm_model_kwargs</strong></td>
          <td><code>dict</code></td>
          <td>Additional parameters for LLM generation</td>
          <td></td>
      </tr>
      <tr>
          <td><strong>vector_db_storage_cls_kwargs</strong></td>
          <td><code>dict</code></td>
          <td>Additional parameters for vector database, like setting the threshold for nodes and relations retrieval.</td>
          <td>cosine_better_than_threshold: 0.2（default value changed by  env var COSINE_THRESHOLD)</td>
      </tr>
      <tr>
          <td><strong>enable_llm_cache</strong></td>
          <td><code>bool</code></td>
          <td>If <code>TRUE</code>, stores LLM results in cache; repeated prompts return cached responses</td>
          <td><code>TRUE</code></td>
      </tr>
      <tr>
          <td><strong>enable_llm_cache_for_entity_extract</strong></td>
          <td><code>bool</code></td>
          <td>If <code>TRUE</code>, stores LLM results in cache for entity extraction; Good for beginners to debug your application</td>
          <td><code>TRUE</code></td>
      </tr>
      <tr>
          <td><strong>addon_params</strong></td>
          <td><code>dict</code></td>
          <td>Additional parameters, e.g., <code>{&quot;example_number&quot;: 1, &quot;language&quot;: &quot;Simplified Chinese&quot;, &quot;entity_types&quot;: [&quot;organization&quot;, &quot;person&quot;, &quot;geo&quot;, &quot;event&quot;], &quot;insert_batch_size&quot;: 10}</code>: sets example limit, output language, and batch size for document processing</td>
          <td><code>example_number: all examples, language: English, insert_batch_size: 10</code></td>
      </tr>
      <tr>
          <td><strong>convert_response_to_json_func</strong></td>
          <td><code>callable</code></td>
          <td>Not used</td>
          <td><code>convert_response_to_json</code></td>
      </tr>
      <tr>
          <td><strong>embedding_cache_config</strong></td>
          <td><code>dict</code></td>
          <td>Configuration for question-answer caching. Contains three parameters:<br>- <code>enabled</code>: Boolean value to enable/disable cache lookup functionality. When enabled, the system will check cached responses before generating new answers.<br>- <code>similarity_threshold</code>: Float value (0-1), similarity threshold. When a new question&rsquo;s similarity with a cached question exceeds this threshold, the cached answer will be returned directly without calling the LLM.<br>- <code>use_llm_check</code>: Boolean value to enable/disable LLM similarity verification. When enabled, LLM will be used as a secondary check to verify the similarity between questions before returning cached answers.</td>
          <td>Default: <code>{&quot;enabled&quot;: False, &quot;similarity_threshold&quot;: 0.95, &quot;use_llm_check&quot;: False}</code></td>
      </tr>
      <tr>
          <td><strong>log_dir</strong></td>
          <td><code>str</code></td>
          <td>Directory to store logs.</td>
          <td><code>./</code></td>
      </tr>
  </tbody>
</table>
<h4 id="error-handling">Error Handling</h4>
<p>Click to view error handling details</p>
<p>The API includes comprehensive error handling:</p>
<ul>
<li>File not found errors (404)</li>
<li>Processing errors (500)</li>
<li>Supports multiple file encodings (UTF-8 and GBK)</li>
</ul>
<h2 id="evaluation">Evaluation</h2>
<h3 id="dataset">Dataset</h3>
<p>The dataset used in LightRAG can be downloaded from <a href="https://huggingface.co/datasets/TommyChien/UltraDomain">TommyChien/UltraDomain</a>.</p>
<h3 id="generate-query">Generate Query</h3>
<p>LightRAG uses the following prompt to generate high-level queries, with the corresponding code in <code>example/generate_query.py</code>.</p>
<p>Prompt</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Given the following description of a dataset:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>{description}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Please identify <span style="color:#ae81ff">5</span> potential users who would engage <span style="color:#66d9ef">with</span> this dataset<span style="color:#f92672">.</span> For each user, list <span style="color:#ae81ff">5</span> tasks they would perform <span style="color:#66d9ef">with</span> this dataset<span style="color:#f92672">.</span> Then, <span style="color:#66d9ef">for</span> each (user, task) combination, generate <span style="color:#ae81ff">5</span> questions that require a high<span style="color:#f92672">-</span>level understanding of the entire dataset<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Output the results <span style="color:#f92672">in</span> the following structure:
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> User <span style="color:#ae81ff">1</span>: [user description]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">-</span> Task <span style="color:#ae81ff">1</span>: [task description]
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">-</span> Question <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">-</span> Question <span style="color:#ae81ff">2</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">-</span> Question <span style="color:#ae81ff">3</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">-</span> Question <span style="color:#ae81ff">4</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">-</span> Question <span style="color:#ae81ff">5</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">-</span> Task <span style="color:#ae81ff">2</span>: [task description]
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">-</span> Task <span style="color:#ae81ff">5</span>: [task description]
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> User <span style="color:#ae81ff">2</span>: [user description]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> User <span style="color:#ae81ff">5</span>: [user description]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span></code></pre></div><h3 id="batch-eval">Batch Eval</h3>
<p>To evaluate the performance of two RAG systems on high-level queries, LightRAG uses the following prompt, with the specific code available in <code>example/batch_eval.py</code>.</p>
<p>Prompt</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">---</span>Role<span style="color:#f92672">---</span>
</span></span><span style="display:flex;"><span>You are an expert tasked <span style="color:#66d9ef">with</span> evaluating two answers to the same question based on three criteria: <span style="color:#f92672">**</span>Comprehensiveness<span style="color:#f92672">**</span>, <span style="color:#f92672">**</span>Diversity<span style="color:#f92672">**</span>, <span style="color:#f92672">and</span> <span style="color:#f92672">**</span>Empowerment<span style="color:#f92672">**.</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">---</span>Goal<span style="color:#f92672">---</span>
</span></span><span style="display:flex;"><span>You will evaluate two answers to the same question based on three criteria: <span style="color:#f92672">**</span>Comprehensiveness<span style="color:#f92672">**</span>, <span style="color:#f92672">**</span>Diversity<span style="color:#f92672">**</span>, <span style="color:#f92672">and</span> <span style="color:#f92672">**</span>Empowerment<span style="color:#f92672">**.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#f92672">**</span>Comprehensiveness<span style="color:#f92672">**</span>: How much detail does the answer provide to cover all aspects <span style="color:#f92672">and</span> details of the question<span style="color:#960050;background-color:#1e0010">?</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#f92672">**</span>Diversity<span style="color:#f92672">**</span>: How varied <span style="color:#f92672">and</span> rich <span style="color:#f92672">is</span> the answer <span style="color:#f92672">in</span> providing different perspectives <span style="color:#f92672">and</span> insights on the question<span style="color:#960050;background-color:#1e0010">?</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#f92672">**</span>Empowerment<span style="color:#f92672">**</span>: How well does the answer help the reader understand <span style="color:#f92672">and</span> make informed judgments about the topic<span style="color:#960050;background-color:#1e0010">?</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>For each criterion, choose the better answer (either Answer <span style="color:#ae81ff">1</span> <span style="color:#f92672">or</span> Answer <span style="color:#ae81ff">2</span>) <span style="color:#f92672">and</span> explain why<span style="color:#f92672">.</span> Then, select an overall winner based on these three categories<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Here <span style="color:#f92672">is</span> the question:
</span></span><span style="display:flex;"><span>{query}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Here are the two answers:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">**</span>Answer <span style="color:#ae81ff">1</span>:<span style="color:#f92672">**</span>
</span></span><span style="display:flex;"><span>{answer1}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">**</span>Answer <span style="color:#ae81ff">2</span>:<span style="color:#f92672">**</span>
</span></span><span style="display:flex;"><span>{answer2}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Evaluate both answers using the three criteria listed above <span style="color:#f92672">and</span> provide detailed explanations <span style="color:#66d9ef">for</span> each criterion<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Output your evaluation <span style="color:#f92672">in</span> the following JSON format:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>{{
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Comprehensiveness&#34;</span>: {{
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Winner&#34;</span>: <span style="color:#e6db74">&#34;[Answer 1 or Answer 2]&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Explanation&#34;</span>: <span style="color:#e6db74">&#34;[Provide explanation here]&#34;</span>
</span></span><span style="display:flex;"><span>    }},
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Empowerment&#34;</span>: {{
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Winner&#34;</span>: <span style="color:#e6db74">&#34;[Answer 1 or Answer 2]&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Explanation&#34;</span>: <span style="color:#e6db74">&#34;[Provide explanation here]&#34;</span>
</span></span><span style="display:flex;"><span>    }},
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Overall Winner&#34;</span>: {{
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Winner&#34;</span>: <span style="color:#e6db74">&#34;[Answer 1 or Answer 2]&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Explanation&#34;</span>: <span style="color:#e6db74">&#34;[Summarize why this answer is the overall winner based on the three criteria]&#34;</span>
</span></span><span style="display:flex;"><span>    }}
</span></span><span style="display:flex;"><span>}}
</span></span></code></pre></div><h3 id="overall-performance-table">Overall Performance Table</h3>
<table>
  <thead>
      <tr>
          <th></th>
          <th><strong>Agriculture</strong></th>
          <th></th>
          <th><strong>CS</strong></th>
          <th></th>
          <th><strong>Legal</strong></th>
          <th></th>
          <th><strong>Mix</strong></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td>NaiveRAG</td>
          <td><strong>LightRAG</strong></td>
          <td>NaiveRAG</td>
          <td><strong>LightRAG</strong></td>
          <td>NaiveRAG</td>
          <td><strong>LightRAG</strong></td>
          <td>NaiveRAG</td>
          <td><strong>LightRAG</strong></td>
      </tr>
      <tr>
          <td><strong>Comprehensiveness</strong></td>
          <td>32.4%</td>
          <td><strong>67.6%</strong></td>
          <td>38.4%</td>
          <td><strong>61.6%</strong></td>
          <td>16.4%</td>
          <td><strong>83.6%</strong></td>
          <td>38.8%</td>
          <td><strong>61.2%</strong></td>
      </tr>
      <tr>
          <td><strong>Diversity</strong></td>
          <td>23.6%</td>
          <td><strong>76.4%</strong></td>
          <td>38.0%</td>
          <td><strong>62.0%</strong></td>
          <td>13.6%</td>
          <td><strong>86.4%</strong></td>
          <td>32.4%</td>
          <td><strong>67.6%</strong></td>
      </tr>
      <tr>
          <td><strong>Empowerment</strong></td>
          <td>32.4%</td>
          <td><strong>67.6%</strong></td>
          <td>38.8%</td>
          <td><strong>61.2%</strong></td>
          <td>16.4%</td>
          <td><strong>83.6%</strong></td>
          <td>42.8%</td>
          <td><strong>57.2%</strong></td>
      </tr>
      <tr>
          <td><strong>Overall</strong></td>
          <td>32.4%</td>
          <td><strong>67.6%</strong></td>
          <td>38.8%</td>
          <td><strong>61.2%</strong></td>
          <td>15.2%</td>
          <td><strong>84.8%</strong></td>
          <td>40.0%</td>
          <td><strong>60.0%</strong></td>
      </tr>
      <tr>
          <td></td>
          <td>RQ-RAG</td>
          <td><strong>LightRAG</strong></td>
          <td>RQ-RAG</td>
          <td><strong>LightRAG</strong></td>
          <td>RQ-RAG</td>
          <td><strong>LightRAG</strong></td>
          <td>RQ-RAG</td>
          <td><strong>LightRAG</strong></td>
      </tr>
      <tr>
          <td><strong>Comprehensiveness</strong></td>
          <td>31.6%</td>
          <td><strong>68.4%</strong></td>
          <td>38.8%</td>
          <td><strong>61.2%</strong></td>
          <td>15.2%</td>
          <td><strong>84.8%</strong></td>
          <td>39.2%</td>
          <td><strong>60.8%</strong></td>
      </tr>
      <tr>
          <td><strong>Diversity</strong></td>
          <td>29.2%</td>
          <td><strong>70.8%</strong></td>
          <td>39.2%</td>
          <td><strong>60.8%</strong></td>
          <td>11.6%</td>
          <td><strong>88.4%</strong></td>
          <td>30.8%</td>
          <td><strong>69.2%</strong></td>
      </tr>
      <tr>
          <td><strong>Empowerment</strong></td>
          <td>31.6%</td>
          <td><strong>68.4%</strong></td>
          <td>36.4%</td>
          <td><strong>63.6%</strong></td>
          <td>15.2%</td>
          <td><strong>84.8%</strong></td>
          <td>42.4%</td>
          <td><strong>57.6%</strong></td>
      </tr>
      <tr>
          <td><strong>Overall</strong></td>
          <td>32.4%</td>
          <td><strong>67.6%</strong></td>
          <td>38.0%</td>
          <td><strong>62.0%</strong></td>
          <td>14.4%</td>
          <td><strong>85.6%</strong></td>
          <td>40.0%</td>
          <td><strong>60.0%</strong></td>
      </tr>
      <tr>
          <td></td>
          <td>HyDE</td>
          <td><strong>LightRAG</strong></td>
          <td>HyDE</td>
          <td><strong>LightRAG</strong></td>
          <td>HyDE</td>
          <td><strong>LightRAG</strong></td>
          <td>HyDE</td>
          <td><strong>LightRAG</strong></td>
      </tr>
      <tr>
          <td><strong>Comprehensiveness</strong></td>
          <td>26.0%</td>
          <td><strong>74.0%</strong></td>
          <td>41.6%</td>
          <td><strong>58.4%</strong></td>
          <td>26.8%</td>
          <td><strong>73.2%</strong></td>
          <td>40.4%</td>
          <td><strong>59.6%</strong></td>
      </tr>
      <tr>
          <td><strong>Diversity</strong></td>
          <td>24.0%</td>
          <td><strong>76.0%</strong></td>
          <td>38.8%</td>
          <td><strong>61.2%</strong></td>
          <td>20.0%</td>
          <td><strong>80.0%</strong></td>
          <td>32.4%</td>
          <td><strong>67.6%</strong></td>
      </tr>
      <tr>
          <td><strong>Empowerment</strong></td>
          <td>25.2%</td>
          <td><strong>74.8%</strong></td>
          <td>40.8%</td>
          <td><strong>59.2%</strong></td>
          <td>26.0%</td>
          <td><strong>74.0%</strong></td>
          <td>46.0%</td>
          <td><strong>54.0%</strong></td>
      </tr>
      <tr>
          <td><strong>Overall</strong></td>
          <td>24.8%</td>
          <td><strong>75.2%</strong></td>
          <td>41.6%</td>
          <td><strong>58.4%</strong></td>
          <td>26.4%</td>
          <td><strong>73.6%</strong></td>
          <td>42.4%</td>
          <td><strong>57.6%</strong></td>
      </tr>
      <tr>
          <td></td>
          <td>GraphRAG</td>
          <td><strong>LightRAG</strong></td>
          <td>GraphRAG</td>
          <td><strong>LightRAG</strong></td>
          <td>GraphRAG</td>
          <td><strong>LightRAG</strong></td>
          <td>GraphRAG</td>
          <td><strong>LightRAG</strong></td>
      </tr>
      <tr>
          <td><strong>Comprehensiveness</strong></td>
          <td>45.6%</td>
          <td><strong>54.4%</strong></td>
          <td>48.4%</td>
          <td><strong>51.6%</strong></td>
          <td>48.4%</td>
          <td><strong>51.6%</strong></td>
          <td><strong>50.4%</strong></td>
          <td>49.6%</td>
      </tr>
      <tr>
          <td><strong>Diversity</strong></td>
          <td>22.8%</td>
          <td><strong>77.2%</strong></td>
          <td>40.8%</td>
          <td><strong>59.2%</strong></td>
          <td>26.4%</td>
          <td><strong>73.6%</strong></td>
          <td>36.0%</td>
          <td><strong>64.0%</strong></td>
      </tr>
      <tr>
          <td><strong>Empowerment</strong></td>
          <td>41.2%</td>
          <td><strong>58.8%</strong></td>
          <td>45.2%</td>
          <td><strong>54.8%</strong></td>
          <td>43.6%</td>
          <td><strong>56.4%</strong></td>
          <td><strong>50.8%</strong></td>
          <td>49.2%</td>
      </tr>
      <tr>
          <td><strong>Overall</strong></td>
          <td>45.2%</td>
          <td><strong>54.8%</strong></td>
          <td>48.0%</td>
          <td><strong>52.0%</strong></td>
          <td>47.2%</td>
          <td><strong>52.8%</strong></td>
          <td><strong>50.4%</strong></td>
          <td>49.6%</td>
      </tr>
  </tbody>
</table>
<h2 id="reproduce">Reproduce</h2>
<p>All the code can be found in the <code>./reproduce</code> directory.</p>
<h3 id="step-0-extract-unique-contexts">Step-0 Extract Unique Contexts</h3>
<p>First, we need to extract unique contexts in the datasets.</p>
<p>Code</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">extract_unique_contexts</span>(input_directory, output_directory):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    os<span style="color:#f92672">.</span>makedirs(output_directory, exist_ok<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    jsonl_files <span style="color:#f92672">=</span> glob<span style="color:#f92672">.</span>glob(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(input_directory, <span style="color:#e6db74">&#39;*.jsonl&#39;</span>))
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Found </span><span style="color:#e6db74">{</span>len(jsonl_files)<span style="color:#e6db74">}</span><span style="color:#e6db74"> JSONL files.&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> file_path <span style="color:#f92672">in</span> jsonl_files:
</span></span><span style="display:flex;"><span>        filename <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>basename(file_path)
</span></span><span style="display:flex;"><span>        name, ext <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>splitext(filename)
</span></span><span style="display:flex;"><span>        output_filename <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>name<span style="color:#e6db74">}</span><span style="color:#e6db74">_unique_contexts.json&#34;</span>
</span></span><span style="display:flex;"><span>        output_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(output_directory, output_filename)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        unique_contexts_dict <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Processing file: </span><span style="color:#e6db74">{</span>filename<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">with</span> open(file_path, <span style="color:#e6db74">&#39;r&#39;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;utf-8&#39;</span>) <span style="color:#66d9ef">as</span> infile:
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">for</span> line_number, line <span style="color:#f92672">in</span> enumerate(infile, start<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>                    line <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>strip()
</span></span><span style="display:flex;"><span>                    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> line:
</span></span><span style="display:flex;"><span>                        <span style="color:#66d9ef">continue</span>
</span></span><span style="display:flex;"><span>                    <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>                        json_obj <span style="color:#f92672">=</span> json<span style="color:#f92672">.</span>loads(line)
</span></span><span style="display:flex;"><span>                        context <span style="color:#f92672">=</span> json_obj<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;context&#39;</span>)
</span></span><span style="display:flex;"><span>                        <span style="color:#66d9ef">if</span> context <span style="color:#f92672">and</span> context <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> unique_contexts_dict:
</span></span><span style="display:flex;"><span>                            unique_contexts_dict[context] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>                    <span style="color:#66d9ef">except</span> json<span style="color:#f92672">.</span>JSONDecodeError <span style="color:#66d9ef">as</span> e:
</span></span><span style="display:flex;"><span>                        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;JSON decoding error in file </span><span style="color:#e6db74">{</span>filename<span style="color:#e6db74">}</span><span style="color:#e6db74"> at line </span><span style="color:#e6db74">{</span>line_number<span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>e<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">FileNotFoundError</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;File not found: </span><span style="color:#e6db74">{</span>filename<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">continue</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">Exception</span> <span style="color:#66d9ef">as</span> e:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;An error occurred while processing file </span><span style="color:#e6db74">{</span>filename<span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>e<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">continue</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        unique_contexts_list <span style="color:#f92672">=</span> list(unique_contexts_dict<span style="color:#f92672">.</span>keys())
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;There are </span><span style="color:#e6db74">{</span>len(unique_contexts_list)<span style="color:#e6db74">}</span><span style="color:#e6db74"> unique `context` entries in the file </span><span style="color:#e6db74">{</span>filename<span style="color:#e6db74">}</span><span style="color:#e6db74">.&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">with</span> open(output_path, <span style="color:#e6db74">&#39;w&#39;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;utf-8&#39;</span>) <span style="color:#66d9ef">as</span> outfile:
</span></span><span style="display:flex;"><span>                json<span style="color:#f92672">.</span>dump(unique_contexts_list, outfile, ensure_ascii<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, indent<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Unique `context` entries have been saved to: </span><span style="color:#e6db74">{</span>output_filename<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">Exception</span> <span style="color:#66d9ef">as</span> e:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;An error occurred while saving to the file </span><span style="color:#e6db74">{</span>output_filename<span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>e<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;All files have been processed.&#34;</span>)
</span></span></code></pre></div><h3 id="step-1-insert-contexts">Step-1 Insert Contexts</h3>
<p>For the extracted contexts, we insert them into the LightRAG system.</p>
<p>Code</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">insert_text</span>(rag, file_path):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> open(file_path, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>        unique_contexts <span style="color:#f92672">=</span> json<span style="color:#f92672">.</span>load(f)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    retries <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    max_retries <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> retries <span style="color:#f92672">&lt;</span> max_retries:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>            rag<span style="color:#f92672">.</span>insert(unique_contexts)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">Exception</span> <span style="color:#66d9ef">as</span> e:
</span></span><span style="display:flex;"><span>            retries <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Insertion failed, retrying (</span><span style="color:#e6db74">{</span>retries<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>max_retries<span style="color:#e6db74">}</span><span style="color:#e6db74">), error: </span><span style="color:#e6db74">{</span>e<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>            time<span style="color:#f92672">.</span>sleep(<span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> retries <span style="color:#f92672">==</span> max_retries:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;Insertion failed after exceeding the maximum number of retries&#34;</span>)
</span></span></code></pre></div><h3 id="step-2-generate-queries">Step-2 Generate Queries</h3>
<p>We extract tokens from the first and the second half of each context in the dataset, then combine them as dataset descriptions to generate queries.</p>
<p>Code</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> GPT2Tokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#39;gpt2&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_summary</span>(context, tot_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">2000</span>):
</span></span><span style="display:flex;"><span>    tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>tokenize(context)
</span></span><span style="display:flex;"><span>    half_tokens <span style="color:#f92672">=</span> tot_tokens <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    start_tokens <span style="color:#f92672">=</span> tokens[<span style="color:#ae81ff">1000</span>:<span style="color:#ae81ff">1000</span> <span style="color:#f92672">+</span> half_tokens]
</span></span><span style="display:flex;"><span>    end_tokens <span style="color:#f92672">=</span> tokens[<span style="color:#f92672">-</span>(<span style="color:#ae81ff">1000</span> <span style="color:#f92672">+</span> half_tokens):<span style="color:#ae81ff">1000</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    summary_tokens <span style="color:#f92672">=</span> start_tokens <span style="color:#f92672">+</span> end_tokens
</span></span><span style="display:flex;"><span>    summary <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>convert_tokens_to_string(summary_tokens)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> summary
</span></span></code></pre></div><h3 id="step-3-query">Step-3 Query</h3>
<p>For the queries generated in Step-2, we will extract them and query LightRAG.</p>
<p>Code</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">extract_queries</span>(file_path):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> open(file_path, <span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>        data <span style="color:#f92672">=</span> f<span style="color:#f92672">.</span>read()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    data <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#39;**&#39;</span>, <span style="color:#e6db74">&#39;&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    queries <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>findall(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;- Question \d+: (.+)&#39;</span>, data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> queries
</span></span></code></pre></div><h2 id="api">API</h2>
<p>LightRag can be installed with API support to serve a Fast api interface to perform data upload and indexing/Rag operations/Rescan of the input folder etc..</p>
<p>The documentation can be found <a href="lightrag/api/README.md">here</a></p>
<h2 id="graph-viewer">Graph viewer</h2>
<p>LightRag can be installed with Tools support to add extra tools like the graphml 3d visualizer.</p>
<p>The documentation can be found <a href="lightrag/tools/lightrag_visualizer/README.md">here</a>
``</p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
