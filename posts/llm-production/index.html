<!doctype html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <title>LLM Production // davidgao7 blog</title>
    <link rel="shortcut icon" href="/images/favicon.jpg" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.149.1">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="David Gao" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.5b1fcc8902588589c4767187402a3c29f8b8d7a6fdef6d9f8f77045bb0d14fee.css" />
    

    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="LLM Production">
  <meta name="twitter:description" content="Deploying LLMs in a single Machine In this post, I will share my experience deploying LLMs in production. I will cover the challenges I faced, the solutions I found, and the lessons I learned along the way.
first I managed to find a gpu from runpod to experience one complete conversation(multi-inferences). now I know that if you want to deploy this in production, you not only need multiple gpu like the one I’m using in the following writing, you also need a system to help you manage resources (i.e. Kubernetes). 2.1. so they have the solution called vllm production-stack Got to find a cluster with powerful GPUs, OR I could just deploy the stack, since now I’ve experienced model inference. But it will be hard to tell if I deploy it successfully or not… ~(- &lt;_ -)&gt; Reference repo">

    <meta property="og:url" content="http://localhost:1313/posts/llm-production/">
  <meta property="og:site_name" content="davidgao7 blog">
  <meta property="og:title" content="LLM Production">
  <meta property="og:description" content="Deploying LLMs in a single Machine In this post, I will share my experience deploying LLMs in production. I will cover the challenges I faced, the solutions I found, and the lessons I learned along the way.
first I managed to find a gpu from runpod to experience one complete conversation(multi-inferences). now I know that if you want to deploy this in production, you not only need multiple gpu like the one I’m using in the following writing, you also need a system to help you manage resources (i.e. Kubernetes). 2.1. so they have the solution called vllm production-stack Got to find a cluster with powerful GPUs, OR I could just deploy the stack, since now I’ve experienced model inference. But it will be hard to tell if I deploy it successfully or not… ~(- &lt;_ -)&gt; Reference repo">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-04-13T15:13:29-04:00">
    <meta property="article:modified_time" content="2025-04-13T15:13:29-04:00">
    <meta property="article:tag" content="VLLM">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="Production">
    <meta property="article:tag" content="Deployment">
    <meta property="article:tag" content="EGPU">
    <meta property="article:tag" content="Lambda">


    
      <script>
window.MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]'], ['$$', '$$']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
</script>


    

  </head>
  <body>
    <header class="app-header">
      <a href="/"><img class="app-header-avatar" src="/images/avatar.jpg" alt="David Gao" /></a>
      <span class="app-header-title">davidgao7 blog</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
      </nav>
      <p>Note new findings every day to let the magic happen!</p>
      <div class="app-header-social">
        
          <a href="https://github.com/davidgao7" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-brand-github" viewBox="0 0 24 24" fill="currentColor"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
          </a>
        
          <a href="https://x.com/AiiGen71976j" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-brand-x" viewBox="0 0 24 24" fill="currentColor"><title>X</title><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"/></svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">LLM Production</h1>
      <div class="post-meta">
        <div>
          <svg class="icon icon-calendar" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
          Apr 13, 2025
        </div>
        <div>
          <svg class="icon icon-clock" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>clock</title><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>
          51 min read
        </div>
        <div>
          <svg class="icon icon-tag" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>tag</title><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line></svg>
              <a class="tag" href="/tags/vllm/">VLLM</a>
              <a class="tag" href="/tags/llm/">LLM</a>
              <a class="tag" href="/tags/production/">Production</a>
              <a class="tag" href="/tags/deployment/">Deployment</a>
              <a class="tag" href="/tags/egpu/">EGPU</a>
              <a class="tag" href="/tags/lambda/">Lambda</a>
              <a class="tag" href="/tags/runpod/">Runpod</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <h1 id="deploying-llms-in-a-single-machine">Deploying LLMs in a single Machine</h1>
<blockquote>
<p>In this post, I will share my experience deploying LLMs in production. I will cover the challenges I faced, the solutions I found, and the lessons I learned along the way.</p></blockquote>
<ol>
<li>first I managed to find a gpu from runpod to experience one complete conversation(multi-inferences).</li>
<li>now I know that if you want to deploy this in production, you not only need multiple gpu like the one
I&rsquo;m using in the following writing, you also need a system to help you manage resources (i.e. Kubernetes).
2.1. so they have the solution called <code>vllm production-stack</code>
Got to find a cluster with powerful GPUs, OR I could just deploy the stack, since now I&rsquo;ve experienced model inference. But it will be hard to tell if I deploy it successfully or not&hellip;  ~(- &lt;_ -)&gt;</li>
</ol>
<ul>
<li>
<p><a href="https://github.com/vllm-project/production-stack/">Reference repo</a></p>
</li>
<li>
<p><a href="https://github.com/davidgao7/homelab">Code for people in hurry</a></p>
</li>
<li>
<p>resource for personal: <a href="https://www.runpod.io/console/signup">runpod.io</a></p>
</li>
<li>
<p>for similar service, you can use <a href="https://lambda.ai/">Lambda(not AWS)</a>, GCP, Azure, etc.</p>
</li>
</ul>
<p>If using AKS: Setup automated deployments to the AKS cluster ( optional )</p>
<ul>
<li><a href="https://learn.microsoft.com/en-us/azure/aks/automated-deployments">Reference</a></li>
</ul>
<h2 id="create-a-runpodio-account-to-get-cloud-gpu-access-api-access">Create a runpod.io account to get cloud GPU access (API access)</h2>
<ul>
<li>You can use API keys for project access</li>
<li>Passkey for secure passwordless authentication access</li>
<li>How to choose the GPU
<ul>
<li>It depends on your needs, you can just spin up a cheapest choice on runpod, which is enough for me to run the model</li>
</ul>
</li>
</ul>
<p><img src="/images/run-pod-api-passkey-tutorial.png" alt="run-pod-api-passkey-tutorial"></p>
<h2 id="create-ssh-keys-to-for-remote-access--virtual-machine-access">Create ssh keys to for remote access  (Virtual Machine access)</h2>
<ul>
<li>
<p>Runpod just a service to provide you powerful GPUs</p>
</li>
<li>
<p>create a ssh-key pair</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ssh-keygen -t rsa -b <span style="color:#ae81ff">4096</span> -C <span style="color:#e6db74">&#34;This is a comment&#34;</span>
</span></span></code></pre></div><ul>
<li>Paste the public key to the runpod.io&rsquo;s ssh public key at <code>Account &gt; Settings &gt; SSH Public Keys</code></li>
</ul>
<p><img src="/images/generate-ssh-public-private-keys.png" alt="ssh-key-recognize"></p>
<ul>
<li>Click <strong>Update Public Key</strong></li>
</ul>
<p>ssh into the server</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ssh <span style="color:#f92672">[</span>server-ip<span style="color:#f92672">]</span> -i ~/
</span></span></code></pre></div><ul>
<li>Go to <code>Pods</code> -&gt; <code>Deploy</code> -&gt; create a pod -&gt; wait till it creates -&gt; click <code>Connect</code></li>
</ul>
<p><img src="/images/connect-options.png" alt="connect options"></p>
<h2 id="yay-now-you-got-powerful-gpus">yay! now you got powerful GPUs</h2>
<ul>
<li>lets see what OS is it</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>root@f00742593164:/# uname -a
</span></span><span style="display:flex;"><span>Linux f00742593164 6.8.0-49-generic <span style="color:#75715e">#49~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Nov  6 17:42:15 UTC 2 x86_64 x86_64 x86_64 GNU/Linux</span>
</span></span></code></pre></div><ul>
<li>
<p>alright, Ubuntu 22.04, classic</p>
</li>
<li>
<p>alright &hellip; same xxx, different day</p>
</li>
<li>
<p>install python, lets use uv this time</p>
</li>
<li>
<p>make it more intresting, lets build the vllm from source, so that</p>
<ol>
<li>in production, are u really sure you don&rsquo;t want to customize it to fit your company&rsquo;s need?</li>
<li>are you sure vllm production-stack is mature enough?</li>
</ol>
</li>
<li>
<p>I only have one machine, I could build few virtual pods, then add k8s to manage them, then use production-stack to get the &ldquo;llm in production starter pack&rdquo;</p>
</li>
</ul>
<h2 id="install-dependencies">Install dependencies</h2>
<h3 id="install-uv-python-virtual-environment-manager">Install uv python virtual environment manager</h3>
<h3 id="install-python">Install python</h3>
<ul>
<li>
<p>recommend python version: <code>python 3.12</code></p>
</li>
<li>
<p>give a little old <code>apt update &amp;&amp; apt upgrade</code> first to fetch the latest packages</p>
<ul>
<li><font color=red>always do this first, makes sure your OS packages are clean and fresh</font></li>
</ul>
</li>
<li>
<p>for Ubuntu you have these package managers:</p>
<ul>
<li><code>apt</code> (Debian-based)</li>
<li><code>yum</code> (Red Hat-based)</li>
<li><code>dnf</code> (Fedora-based)</li>
<li><code>zypper</code> (openSUSE-based)</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>apt update <span style="color:#f92672">&amp;&amp;</span> apt upgrade
</span></span></code></pre></div><p>Lets check the GPU to make sure I&rsquo;m not get scammed</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>root@f00742593164:/# nvidia-smi
</span></span><span style="display:flex;"><span>Fri May  <span style="color:#ae81ff">9</span> 01:39:25 <span style="color:#ae81ff">2025</span>
</span></span><span style="display:flex;"><span>+-----------------------------------------------------------------------------------------+
</span></span><span style="display:flex;"><span>| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
</span></span><span style="display:flex;"><span>|-----------------------------------------+------------------------+----------------------+
</span></span><span style="display:flex;"><span>| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
</span></span><span style="display:flex;"><span>| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
</span></span><span style="display:flex;"><span>|                                         |                        |               MIG M. |
</span></span><span style="display:flex;"><span>|<span style="color:#f92672">=========================================</span>+<span style="color:#f92672">========================</span>+<span style="color:#f92672">======================</span>|
</span></span><span style="display:flex;"><span>|   <span style="color:#ae81ff">0</span>  NVIDIA H100 PCIe               On  |   00000000:61:00.0 Off |                    <span style="color:#ae81ff">0</span> |
</span></span><span style="display:flex;"><span>| N/A   35C    P0             48W /  310W |       1MiB /  81559MiB |      0%      Default |
</span></span><span style="display:flex;"><span>|                                         |                        |             Disabled |
</span></span><span style="display:flex;"><span>+-----------------------------------------+------------------------+----------------------+
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>+-----------------------------------------------------------------------------------------+
</span></span><span style="display:flex;"><span>| Processes:                                                                              |
</span></span><span style="display:flex;"><span>|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
</span></span><span style="display:flex;"><span>|        ID   ID                                                               Usage      |
</span></span><span style="display:flex;"><span>|<span style="color:#f92672">=========================================================================================</span>|
</span></span><span style="display:flex;"><span>|  No running processes found                                                             |
</span></span><span style="display:flex;"><span>+-----------------------------------------------------------------------------------------+
</span></span></code></pre></div><p>Good&hellip;</p>
<p>Lets see if system has python or not</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>root@f00742593164:/# which python
</span></span><span style="display:flex;"><span><span style="color:#75715e"># /usr/bin/python</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>root@f00742593164:/# whereis python
</span></span><span style="display:flex;"><span><span style="color:#75715e"># python: /usr/bin/python</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>root@f00742593164:/# /usr/bin/python
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] on linux</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Type &#34;help&#34;, &#34;copyright&#34;, &#34;credits&#34; or &#34;license&#34; for more information.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># &gt;&gt;&gt; exit()</span>
</span></span></code></pre></div><p>Impressive, but we need 3.12 -_-</p>
<p>Well don&rsquo;t worry, 🤓☝️ we can use <a href="https://docs.astral.sh/uv/">uv</a>, a python package and project manager, written in <strong>Rust</strong>🐐.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Install uv</span>
</span></span><span style="display:flex;"><span>curl -LsSf https://astral.sh/uv/install.sh | sh
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># downloading uv 0.7.3 x86_64-unknown-linux-gnu</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># no checksums to verify</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># installing to /root/.local/bin</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   uv</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   uvx</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># everything&#39;s installed!</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># To add $HOME/.local/bin to your PATH, either restart your shell or run:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     source $HOME/.local/bin/env (sh, bash, zsh)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     source $HOME/.local/bin/env.fish (fish)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># know what shell you&#39;re using</span>
</span></span><span style="display:flex;"><span>echo $SHELL
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add the env loader to your bash startup</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#echo &#39;source $HOME/.local/bin/env&#39; &gt;&gt; ~/.bashrc</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># use this so that it won&#39;t accedentally crash the shell and disconnected</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># but you have to do it everytime when login</span>
</span></span><span style="display:flex;"><span>export PATH<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span>$HOME<span style="color:#e6db74">/.local/bin:</span>$PATH<span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>root@f00742593164:/# echo $SHELL
</span></span><span style="display:flex;"><span>/bin/bash
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Reload</span>
</span></span><span style="display:flex;"><span>source ~/.bashrc
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># test uv</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># root@f00742593164:/# uv</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># An extremely fast Python package manager.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Usage: uv [OPTIONS] &lt;COMMAND&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Commands:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   run      Run a command or script</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   init     Create a new project</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   add      Add dependencies to the project</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   remove   Remove dependencies from the project</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   sync     Update the project&#39;s environment</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   lock     Update the project&#39;s lockfile</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   export   Export the project&#39;s lockfile to an alternate format</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   tree     Display the project&#39;s dependency tree</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   tool     Run and install commands provided by Python packages</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   python   Manage Python versions and installations</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   pip      Manage Python packages with a pip-compatible interface</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   venv     Create a virtual environment</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   build    Build Python packages into source distributions and wheels</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   publish  Upload distributions to an index</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   cache    Manage uv&#39;s cache</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   self     Manage the uv executable</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   version  Read or update the project&#39;s version</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   help     Display documentation for a command</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Cache options:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   -n, --no-cache               Avoid reading from or writing to the cache, instead using a temporary directory for</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#                                the duration of the operation [env: UV_NO_CACHE=]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       --cache-dir &lt;CACHE_DIR&gt;  Path to the cache directory [env: UV_CACHE_DIR=]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Python options:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   --managed-python       Require use of uv-managed Python versions [env: UV_MANAGED_PYTHON=]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   --no-managed-python    Disable use of uv-managed Python versions [env: UV_NO_MANAGED_PYTHON=]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   --no-python-downloads  Disable automatic downloads of Python. [env: &#34;UV_PYTHON_DOWNLOADS=never&#34;]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Global options:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   -q, --quiet...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#           Use quiet output</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   -v, --verbose...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#           Use verbose output</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       --color &lt;COLOR_CHOICE&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#           Control the use of color in output [possible values: auto, always, never]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       --native-tls</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#           Whether to load TLS certificates from the platform&#39;s native certificate store [env: UV_NATIVE_TLS=]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       --offline</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#           Disable network access [env: UV_OFFLINE=]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       --allow-insecure-host &lt;ALLOW_INSECURE_HOST&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#           Allow insecure connections to a host [env: UV_INSECURE_HOST=]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       --no-progress</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#           Hide all progress outputs [env: UV_NO_PROGRESS=]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       --directory &lt;DIRECTORY&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#           Change to the given directory prior to running the command</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       --project &lt;PROJECT&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#           Run the command within the given project directory [env: UV_PROJECT=]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       --config-file &lt;CONFIG_FILE&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#           The path to a `uv.toml` file to use for configuration [env: UV_CONFIG_FILE=]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       --no-config</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#           Avoid discovering configuration files (`pyproject.toml`, `uv.toml`) [env: UV_NO_CONFIG=]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   -h, --help</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#           Display the concise help for this command</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   -V, --version</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#           Display the uv version</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Use `uv help` for more details.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># create python virtual environment in one line</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># NOTE: create this venv above both production-stack and vllm , for consistent</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># uv [venv name] --python [python version]</span>
</span></span><span style="display:flex;"><span>uv venv --python 3.12 --seed
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --seed  # Install seed packages (one or more of: `pip`, `setuptools`, and</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># #`wheel`) into the virtual environment [env: UV_VENV_SEED=]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># root@f00742593164:/# uv venv --python 3.12 --seed</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Using CPython 3.12.10</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Creating virtual environment with seed packages at: .venv</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  + pip==25.1.1</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Activate with: source .venv/bin/activate</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>source .venv/bin/activate
</span></span></code></pre></div><ul>
<li><code>--seed</code> option will install <em>essential packages</em> into the virtual environment after it&rsquo;s created, not python itself
<ul>
<li>So it&rsquo;s like using apt-get install ppa-xxx, setup-tools &hellip; &ldquo;pre-packages&rdquo; like this before building python</li>
</ul>
</li>
</ul>
<h3 id="build-vllm-from-source">Build vllm from source</h3>
<ul>
<li>since it can build with cuda, lets check the cuda version</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>nvcc --version
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># nvcc: NVIDIA (R) Cuda compiler driver</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Copyright (c) 2005-2022 NVIDIA Corporation</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Built on Wed_Sep_21_10:33:58_PDT_2022</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Cuda compilation tools, release 11.8, V11.8.89</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Build cuda_11.8.r11.8/compiler.31833905_0</span>
</span></span></code></pre></div><ul>
<li>
<p>build the vllm from source , with correct cuda version</p>
</li>
<li>
<p>clone my fork(you can just clone the original repo)</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/home# git clone https://github.com/davidgao7/vllm.git
</span></span><span style="display:flex;"><span>Cloning into <span style="color:#e6db74">&#39;vllm&#39;</span>...
</span></span><span style="display:flex;"><span>remote: Enumerating objects: 70376, <span style="color:#66d9ef">done</span>.
</span></span><span style="display:flex;"><span>remote: Counting objects: 100% <span style="color:#f92672">(</span>189/189<span style="color:#f92672">)</span>, <span style="color:#66d9ef">done</span>.
</span></span><span style="display:flex;"><span>remote: Compressing objects: 100% <span style="color:#f92672">(</span>157/157<span style="color:#f92672">)</span>, <span style="color:#66d9ef">done</span>.
</span></span><span style="display:flex;"><span>remote: Total <span style="color:#ae81ff">70376</span> <span style="color:#f92672">(</span>delta 92<span style="color:#f92672">)</span>, reused <span style="color:#ae81ff">32</span> <span style="color:#f92672">(</span>delta 32<span style="color:#f92672">)</span>, pack-reused <span style="color:#ae81ff">70187</span> <span style="color:#f92672">(</span>from 3<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>Receiving objects: 100% <span style="color:#f92672">(</span>70376/70376<span style="color:#f92672">)</span>, 48.99 MiB | 15.81 MiB/s, <span style="color:#66d9ef">done</span>.
</span></span><span style="display:flex;"><span>Resolving deltas: 100% <span style="color:#f92672">(</span>54878/54878<span style="color:#f92672">)</span>, <span style="color:#66d9ef">done</span>.
</span></span></code></pre></div><ul>
<li>
<p>build from source (for nividia cuda)</p>
</li>
<li>
<p>In the docs: <code>If you only need to change Python code, you can build and install vLLM without compilation. Using pip’s --editable flag, changes you make to the code will be reflected when you run vLLM</code>, which is exactly what we want</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>cd vllm
</span></span><span style="display:flex;"><span>VLLM_USE_PRECOMPILED<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> uv pip install <span style="color:#f92672">--</span>editable <span style="color:#f92672">.</span>
</span></span></code></pre></div><p>This command will do the following:</p>
<ol>
<li>
<p>Look for the current branch in your vLLM clone(default branch: ).</p>
</li>
<li>
<p>Identify the corresponding base commit in the main branch.</p>
</li>
<li>
<p>Download the pre-built wheel of the base commit.</p>
</li>
<li>
<p>Use its compiled libraries in the installation.</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#f92672">(</span>vllm<span style="color:#f92672">)</span> root@49924402e807:/workspace/vllm# VLLM_USE_PRECOMPILED<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> uv pip install --editable .
</span></span><span style="display:flex;"><span>Resolved <span style="color:#ae81ff">146</span> packages in 24.08s
</span></span><span style="display:flex;"><span>      Built vllm @ file:///workspace/vllm
</span></span><span style="display:flex;"><span>Prepared <span style="color:#ae81ff">1</span> package in 56.50s
</span></span><span style="display:flex;"><span>░░░░░░░░░░░░░░░░░░░░ <span style="color:#f92672">[</span>0/146<span style="color:#f92672">]</span> Installing wheels...                                                                 warning: Failed to hardlink files; falling back to full copy. This may lead to degraded performance.
</span></span><span style="display:flex;"><span>         If the cache and target directories are on different filesystems, hardlinking may not be supported.
</span></span><span style="display:flex;"><span>         If this is intentional, set <span style="color:#e6db74">`</span>export UV_LINK_MODE<span style="color:#f92672">=</span>copy<span style="color:#e6db74">`</span> or use <span style="color:#e6db74">`</span>--link-mode<span style="color:#f92672">=</span>copy<span style="color:#e6db74">`</span> to suppress this warning.
</span></span><span style="display:flex;"><span>Installed <span style="color:#ae81ff">146</span> packages in 54.49s
</span></span><span style="display:flex;"><span> + aiohappyeyeballs<span style="color:#f92672">==</span>2.6.1
</span></span><span style="display:flex;"><span> + aiohttp<span style="color:#f92672">==</span>3.11.18
</span></span><span style="display:flex;"><span> + aiosignal<span style="color:#f92672">==</span>1.3.2
</span></span><span style="display:flex;"><span> + airportsdata<span style="color:#f92672">==</span><span style="color:#ae81ff">20250224</span>
</span></span><span style="display:flex;"><span> + annotated-types<span style="color:#f92672">==</span>0.7.0
</span></span><span style="display:flex;"><span> + anyio<span style="color:#f92672">==</span>4.9.0
</span></span><span style="display:flex;"><span> + astor<span style="color:#f92672">==</span>0.8.1
</span></span><span style="display:flex;"><span> + attrs<span style="color:#f92672">==</span>25.3.0
</span></span><span style="display:flex;"><span> + blake3<span style="color:#f92672">==</span>1.0.4
</span></span><span style="display:flex;"><span> + cachetools<span style="color:#f92672">==</span>5.5.2
</span></span><span style="display:flex;"><span> + certifi<span style="color:#f92672">==</span>2025.4.26
</span></span><span style="display:flex;"><span> + charset-normalizer<span style="color:#f92672">==</span>3.4.2
</span></span><span style="display:flex;"><span> + click<span style="color:#f92672">==</span>8.1.8
</span></span><span style="display:flex;"><span> + cloudpickle<span style="color:#f92672">==</span>3.1.1
</span></span><span style="display:flex;"><span> + compressed-tensors<span style="color:#f92672">==</span>0.9.4
</span></span><span style="display:flex;"><span> + cupy-cuda12x<span style="color:#f92672">==</span>13.4.1
</span></span><span style="display:flex;"><span> + deprecated<span style="color:#f92672">==</span>1.2.18
</span></span><span style="display:flex;"><span> + depyf<span style="color:#f92672">==</span>0.18.0
</span></span><span style="display:flex;"><span> + dill<span style="color:#f92672">==</span>0.4.0
</span></span><span style="display:flex;"><span> + diskcache<span style="color:#f92672">==</span>5.6.3
</span></span><span style="display:flex;"><span> + distro<span style="color:#f92672">==</span>1.9.0
</span></span><span style="display:flex;"><span> + dnspython<span style="color:#f92672">==</span>2.7.0
</span></span><span style="display:flex;"><span> + einops<span style="color:#f92672">==</span>0.8.1
</span></span><span style="display:flex;"><span> + email-validator<span style="color:#f92672">==</span>2.2.0
</span></span><span style="display:flex;"><span> + fastapi<span style="color:#f92672">==</span>0.115.12
</span></span><span style="display:flex;"><span> + fastapi-cli<span style="color:#f92672">==</span>0.0.7
</span></span><span style="display:flex;"><span> + fastrlock<span style="color:#f92672">==</span>0.8.3
</span></span><span style="display:flex;"><span> + filelock<span style="color:#f92672">==</span>3.18.0
</span></span><span style="display:flex;"><span> + frozenlist<span style="color:#f92672">==</span>1.6.0
</span></span><span style="display:flex;"><span> + fsspec<span style="color:#f92672">==</span>2025.3.2
</span></span><span style="display:flex;"><span> + gguf<span style="color:#f92672">==</span>0.16.3
</span></span><span style="display:flex;"><span> + googleapis-common-protos<span style="color:#f92672">==</span>1.70.0
</span></span><span style="display:flex;"><span> + grpcio<span style="color:#f92672">==</span>1.71.0
</span></span><span style="display:flex;"><span> + h11<span style="color:#f92672">==</span>0.16.0
</span></span><span style="display:flex;"><span> + hf-xet<span style="color:#f92672">==</span>1.1.0
</span></span><span style="display:flex;"><span> + httpcore<span style="color:#f92672">==</span>1.0.9
</span></span><span style="display:flex;"><span> + httptools<span style="color:#f92672">==</span>0.6.4
</span></span><span style="display:flex;"><span> + httpx<span style="color:#f92672">==</span>0.28.1
</span></span><span style="display:flex;"><span> + huggingface-hub<span style="color:#f92672">==</span>0.31.1
</span></span><span style="display:flex;"><span> + idna<span style="color:#f92672">==</span>3.10
</span></span><span style="display:flex;"><span> + importlib-metadata<span style="color:#f92672">==</span>8.6.1
</span></span><span style="display:flex;"><span> + interegular<span style="color:#f92672">==</span>0.3.3
</span></span><span style="display:flex;"><span> + jinja2<span style="color:#f92672">==</span>3.1.6
</span></span><span style="display:flex;"><span> + jiter<span style="color:#f92672">==</span>0.9.0
</span></span><span style="display:flex;"><span> + jsonschema<span style="color:#f92672">==</span>4.23.0
</span></span><span style="display:flex;"><span> + jsonschema-specifications<span style="color:#f92672">==</span>2025.4.1
</span></span><span style="display:flex;"><span> + lark<span style="color:#f92672">==</span>1.2.2
</span></span><span style="display:flex;"><span> + llguidance<span style="color:#f92672">==</span>0.7.19
</span></span><span style="display:flex;"><span> + llvmlite<span style="color:#f92672">==</span>0.44.0
</span></span><span style="display:flex;"><span> + lm-format-enforcer<span style="color:#f92672">==</span>0.10.11
</span></span><span style="display:flex;"><span> + markdown-it-py<span style="color:#f92672">==</span>3.0.0
</span></span><span style="display:flex;"><span> + markupsafe<span style="color:#f92672">==</span>3.0.2
</span></span><span style="display:flex;"><span> + mdurl<span style="color:#f92672">==</span>0.1.2
</span></span><span style="display:flex;"><span> + mistral-common<span style="color:#f92672">==</span>1.5.4
</span></span><span style="display:flex;"><span> + mpmath<span style="color:#f92672">==</span>1.3.0
</span></span><span style="display:flex;"><span> + msgpack<span style="color:#f92672">==</span>1.1.0
</span></span><span style="display:flex;"><span> + msgspec<span style="color:#f92672">==</span>0.19.0
</span></span><span style="display:flex;"><span> + multidict<span style="color:#f92672">==</span>6.4.3
</span></span><span style="display:flex;"><span> + nest-asyncio<span style="color:#f92672">==</span>1.6.0
</span></span><span style="display:flex;"><span> + networkx<span style="color:#f92672">==</span>3.4.2
</span></span><span style="display:flex;"><span> + ninja<span style="color:#f92672">==</span>1.11.1.4
</span></span><span style="display:flex;"><span> + numba<span style="color:#f92672">==</span>0.61.2
</span></span><span style="display:flex;"><span> + numpy<span style="color:#f92672">==</span>2.2.5
</span></span><span style="display:flex;"><span> + nvidia-cublas-cu12<span style="color:#f92672">==</span>12.6.4.1
</span></span><span style="display:flex;"><span> + nvidia-cuda-cupti-cu12<span style="color:#f92672">==</span>12.6.80
</span></span><span style="display:flex;"><span> + nvidia-cuda-nvrtc-cu12<span style="color:#f92672">==</span>12.6.77
</span></span><span style="display:flex;"><span> + nvidia-cuda-runtime-cu12<span style="color:#f92672">==</span>12.6.77
</span></span><span style="display:flex;"><span> + nvidia-cudnn-cu12<span style="color:#f92672">==</span>9.5.1.17
</span></span><span style="display:flex;"><span> + nvidia-cufft-cu12<span style="color:#f92672">==</span>11.3.0.4
</span></span><span style="display:flex;"><span> + nvidia-cufile-cu12<span style="color:#f92672">==</span>1.11.1.6
</span></span><span style="display:flex;"><span> + nvidia-curand-cu12<span style="color:#f92672">==</span>10.3.7.77
</span></span><span style="display:flex;"><span> + nvidia-cusolver-cu12<span style="color:#f92672">==</span>11.7.1.2
</span></span><span style="display:flex;"><span> + nvidia-cusparse-cu12<span style="color:#f92672">==</span>12.5.4.2
</span></span><span style="display:flex;"><span> + nvidia-cusparselt-cu12<span style="color:#f92672">==</span>0.6.3
</span></span><span style="display:flex;"><span> + nvidia-nccl-cu12<span style="color:#f92672">==</span>2.26.2
</span></span><span style="display:flex;"><span> + nvidia-nvjitlink-cu12<span style="color:#f92672">==</span>12.6.85
</span></span><span style="display:flex;"><span> + nvidia-nvtx-cu12<span style="color:#f92672">==</span>12.6.77
</span></span><span style="display:flex;"><span> + openai<span style="color:#f92672">==</span>1.78.0
</span></span><span style="display:flex;"><span> + opencv-python-headless<span style="color:#f92672">==</span>4.11.0.86
</span></span><span style="display:flex;"><span> + opentelemetry-api<span style="color:#f92672">==</span>1.32.1
</span></span><span style="display:flex;"><span> + opentelemetry-exporter-otlp<span style="color:#f92672">==</span>1.32.1
</span></span><span style="display:flex;"><span> + opentelemetry-exporter-otlp-proto-common<span style="color:#f92672">==</span>1.32.1
</span></span><span style="display:flex;"><span> + opentelemetry-exporter-otlp-proto-grpc<span style="color:#f92672">==</span>1.32.1
</span></span><span style="display:flex;"><span> + opentelemetry-exporter-otlp-proto-http<span style="color:#f92672">==</span>1.32.1
</span></span><span style="display:flex;"><span> + opentelemetry-proto<span style="color:#f92672">==</span>1.32.1
</span></span><span style="display:flex;"><span> + opentelemetry-sdk<span style="color:#f92672">==</span>1.32.1
</span></span><span style="display:flex;"><span> + opentelemetry-semantic-conventions<span style="color:#f92672">==</span>0.53b1
</span></span><span style="display:flex;"><span> + opentelemetry-semantic-conventions-ai<span style="color:#f92672">==</span>0.4.7
</span></span><span style="display:flex;"><span> + outlines<span style="color:#f92672">==</span>0.1.11
</span></span><span style="display:flex;"><span> + outlines-core<span style="color:#f92672">==</span>0.1.26
</span></span><span style="display:flex;"><span> + packaging<span style="color:#f92672">==</span>25.0
</span></span><span style="display:flex;"><span> + partial-json-parser<span style="color:#f92672">==</span>0.2.1.1.post5
</span></span><span style="display:flex;"><span> + pillow<span style="color:#f92672">==</span>11.2.1
</span></span><span style="display:flex;"><span> + prometheus-client<span style="color:#f92672">==</span>0.21.1
</span></span><span style="display:flex;"><span> + prometheus-fastapi-instrumentator<span style="color:#f92672">==</span>7.1.0
</span></span><span style="display:flex;"><span> + propcache<span style="color:#f92672">==</span>0.3.1
</span></span><span style="display:flex;"><span> + protobuf<span style="color:#f92672">==</span>5.29.4
</span></span><span style="display:flex;"><span> + psutil<span style="color:#f92672">==</span>7.0.0
</span></span><span style="display:flex;"><span> + py-cpuinfo<span style="color:#f92672">==</span>9.0.0
</span></span><span style="display:flex;"><span> + pycountry<span style="color:#f92672">==</span>24.6.1
</span></span><span style="display:flex;"><span> + pydantic<span style="color:#f92672">==</span>2.11.4
</span></span><span style="display:flex;"><span> + pydantic-core<span style="color:#f92672">==</span>2.33.2
</span></span><span style="display:flex;"><span> + pygments<span style="color:#f92672">==</span>2.19.1
</span></span><span style="display:flex;"><span> + python-dotenv<span style="color:#f92672">==</span>1.1.0
</span></span><span style="display:flex;"><span> + python-json-logger<span style="color:#f92672">==</span>3.3.0
</span></span><span style="display:flex;"><span> + python-multipart<span style="color:#f92672">==</span>0.0.20
</span></span><span style="display:flex;"><span> + pyyaml<span style="color:#f92672">==</span>6.0.2
</span></span><span style="display:flex;"><span> + pyzmq<span style="color:#f92672">==</span>26.4.0
</span></span><span style="display:flex;"><span> + ray<span style="color:#f92672">==</span>2.46.0
</span></span><span style="display:flex;"><span> + referencing<span style="color:#f92672">==</span>0.36.2
</span></span><span style="display:flex;"><span> + regex<span style="color:#f92672">==</span>2024.11.6
</span></span><span style="display:flex;"><span> + requests<span style="color:#f92672">==</span>2.32.3
</span></span><span style="display:flex;"><span> + rich<span style="color:#f92672">==</span>14.0.0
</span></span><span style="display:flex;"><span> + rich-toolkit<span style="color:#f92672">==</span>0.14.5
</span></span><span style="display:flex;"><span> + rpds-py<span style="color:#f92672">==</span>0.24.0
</span></span><span style="display:flex;"><span> + safetensors<span style="color:#f92672">==</span>0.5.3
</span></span><span style="display:flex;"><span> + scipy<span style="color:#f92672">==</span>1.15.3
</span></span><span style="display:flex;"><span> + sentencepiece<span style="color:#f92672">==</span>0.2.0
</span></span><span style="display:flex;"><span> + setuptools<span style="color:#f92672">==</span>79.0.1
</span></span><span style="display:flex;"><span> + shellingham<span style="color:#f92672">==</span>1.5.4
</span></span><span style="display:flex;"><span> + six<span style="color:#f92672">==</span>1.17.0
</span></span><span style="display:flex;"><span> + sniffio<span style="color:#f92672">==</span>1.3.1
</span></span><span style="display:flex;"><span> + starlette<span style="color:#f92672">==</span>0.46.2
</span></span><span style="display:flex;"><span> + sympy<span style="color:#f92672">==</span>1.14.0
</span></span><span style="display:flex;"><span> + tiktoken<span style="color:#f92672">==</span>0.9.0
</span></span><span style="display:flex;"><span> + tokenizers<span style="color:#f92672">==</span>0.21.1
</span></span><span style="display:flex;"><span> + torch<span style="color:#f92672">==</span>2.7.0
</span></span><span style="display:flex;"><span> + torchaudio<span style="color:#f92672">==</span>2.7.0
</span></span><span style="display:flex;"><span> + torchvision<span style="color:#f92672">==</span>0.22.0
</span></span><span style="display:flex;"><span> + tqdm<span style="color:#f92672">==</span>4.67.1
</span></span><span style="display:flex;"><span> + transformers<span style="color:#f92672">==</span>4.51.3
</span></span><span style="display:flex;"><span> + triton<span style="color:#f92672">==</span>3.3.0
</span></span><span style="display:flex;"><span> + typer<span style="color:#f92672">==</span>0.15.3
</span></span><span style="display:flex;"><span> + typing-extensions<span style="color:#f92672">==</span>4.13.2
</span></span><span style="display:flex;"><span> + typing-inspection<span style="color:#f92672">==</span>0.4.0
</span></span><span style="display:flex;"><span> + urllib3<span style="color:#f92672">==</span>2.4.0
</span></span><span style="display:flex;"><span> + uvicorn<span style="color:#f92672">==</span>0.34.2
</span></span><span style="display:flex;"><span> + uvloop<span style="color:#f92672">==</span>0.21.0
</span></span><span style="display:flex;"><span> + vllm<span style="color:#f92672">==</span>0.1.dev6367+g3d1e387.precompiled <span style="color:#f92672">(</span>from file:///workspace/vllm<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span> + watchfiles<span style="color:#f92672">==</span>1.0.5
</span></span><span style="display:flex;"><span> + websockets<span style="color:#f92672">==</span>15.0.1
</span></span><span style="display:flex;"><span> + wrapt<span style="color:#f92672">==</span>1.17.2
</span></span><span style="display:flex;"><span> + xformers<span style="color:#f92672">==</span>0.0.30
</span></span><span style="display:flex;"><span> + xgrammar<span style="color:#f92672">==</span>0.1.18
</span></span><span style="display:flex;"><span> + yarl<span style="color:#f92672">==</span>1.20.0
</span></span><span style="display:flex;"><span> + zipp<span style="color:#f92672">==</span>3.21.0
</span></span></code></pre></div><p>[!NOTE] 🤡 20GB is not engough to install the package</p>
<p>Usually <code>workspace/</code> has more storage.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/home/vllm# df -h
</span></span><span style="display:flex;"><span>Filesystem      Size  Used Avail Use% Mounted on
</span></span><span style="display:flex;"><span>overlay          20G   12G  8.5G  58% /
</span></span><span style="display:flex;"><span>tmpfs            64M     <span style="color:#ae81ff">0</span>   64M   0% /dev
</span></span><span style="display:flex;"><span>tmpfs           756G     <span style="color:#ae81ff">0</span>  756G   0% /sys/fs/cgroup
</span></span><span style="display:flex;"><span>shm              88G     <span style="color:#ae81ff">0</span>   88G   0% /dev/shm
</span></span><span style="display:flex;"><span>/dev/sda2       3.5T   22G  3.3T   1% /usr/bin/nvidia-smi
</span></span><span style="display:flex;"><span>/dev/sdb         20G     <span style="color:#ae81ff">0</span>   20G   0% /workspace
</span></span><span style="display:flex;"><span>tmpfs           756G   12K  756G   1% /proc/driver/nvidia
</span></span><span style="display:flex;"><span>tmpfs           756G  4.0K  756G   1% /etc/nvidia/nvidia-application-profiles-rc.d
</span></span><span style="display:flex;"><span>tmpfs           152G   52M  152G   1% /run/nvidia-persistenced/socket
</span></span><span style="display:flex;"><span>tmpfs           756G     <span style="color:#ae81ff">0</span>  756G   0% /proc/acpi
</span></span><span style="display:flex;"><span>tmpfs           756G     <span style="color:#ae81ff">0</span>  756G   0% /proc/scsi
</span></span><span style="display:flex;"><span>tmpfs           756G     <span style="color:#ae81ff">0</span>  756G   0% /sys/firmware
</span></span><span style="display:flex;"><span>tmpfs           756G     <span style="color:#ae81ff">0</span>  756G   0% /sys/devices/virtual/powercap
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/home/vllm# df -h --output<span style="color:#f92672">=</span>source,target,size,used,avail
</span></span><span style="display:flex;"><span>Filesystem     Mounted on                                    Size  Used Avail
</span></span><span style="display:flex;"><span>overlay        /                                              20G   12G  8.5G
</span></span><span style="display:flex;"><span>tmpfs          /dev                                           64M     <span style="color:#ae81ff">0</span>   64M
</span></span><span style="display:flex;"><span>tmpfs          /sys/fs/cgroup                                756G     <span style="color:#ae81ff">0</span>  756G
</span></span><span style="display:flex;"><span>shm            /dev/shm                                       88G     <span style="color:#ae81ff">0</span>   88G
</span></span><span style="display:flex;"><span>/dev/sda2      /usr/bin/nvidia-smi                           3.5T   22G  3.3T
</span></span><span style="display:flex;"><span>/dev/sdb       /workspace                                     20G     <span style="color:#ae81ff">0</span>   20G
</span></span><span style="display:flex;"><span>tmpfs          /proc/driver/nvidia                           756G   12K  756G
</span></span><span style="display:flex;"><span>tmpfs          /etc/nvidia/nvidia-application-profiles-rc.d  756G  4.0K  756G
</span></span><span style="display:flex;"><span>tmpfs          /run/nvidia-persistenced/socket               152G   52M  152G
</span></span><span style="display:flex;"><span>tmpfs          /proc/acpi                                    756G     <span style="color:#ae81ff">0</span>  756G
</span></span><span style="display:flex;"><span>tmpfs          /proc/scsi                                    756G     <span style="color:#ae81ff">0</span>  756G
</span></span><span style="display:flex;"><span>tmpfs          /sys/firmware                                 756G     <span style="color:#ae81ff">0</span>  756G
</span></span><span style="display:flex;"><span>tmpfs          /sys/devices/virtual/powercap                 756G     <span style="color:#ae81ff">0</span>  756G
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/home/vllm# cd ..
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/home# mv vllm/ ../workspace/
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/home# ls
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/home# cd ..
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/# cd workspace/
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/workspace# ls
</span></span><span style="display:flex;"><span>vllm
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/workspace# cd vllm
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/workspace/vllm# df -h --output<span style="color:#f92672">=</span>source,target,size,used,avail
</span></span><span style="display:flex;"><span>Filesystem     Mounted on                                    Size  Used Avail
</span></span><span style="display:flex;"><span>overlay        /                                              20G   11G  9.6G
</span></span><span style="display:flex;"><span>tmpfs          /dev                                           64M     <span style="color:#ae81ff">0</span>   64M
</span></span><span style="display:flex;"><span>tmpfs          /sys/fs/cgroup                                756G     <span style="color:#ae81ff">0</span>  756G
</span></span><span style="display:flex;"><span>shm            /dev/shm                                       88G     <span style="color:#ae81ff">0</span>   88G
</span></span><span style="display:flex;"><span>/dev/sda2      /usr/bin/nvidia-smi                           3.5T   22G  3.3T
</span></span><span style="display:flex;"><span>/dev/sdb       /workspace                                     20G  1.2G   19G
</span></span><span style="display:flex;"><span>tmpfs          /proc/driver/nvidia                           756G   12K  756G
</span></span><span style="display:flex;"><span>tmpfs          /etc/nvidia/nvidia-application-profiles-rc.d  756G  4.0K  756G
</span></span><span style="display:flex;"><span>tmpfs          /run/nvidia-persistenced/socket               152G   52M  152G
</span></span><span style="display:flex;"><span>tmpfs          /proc/acpi                                    756G     <span style="color:#ae81ff">0</span>  756G
</span></span><span style="display:flex;"><span>tmpfs          /proc/scsi                                    756G     <span style="color:#ae81ff">0</span>  756G
</span></span><span style="display:flex;"><span>tmpfs          /sys/firmware                                 756G     <span style="color:#ae81ff">0</span>  756G
</span></span><span style="display:flex;"><span>tmpfs          /sys/devices/virtual/powercap                 756G     <span style="color:#ae81ff">0</span>  756G
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/workspace/vllm# rm -rf ../../.venv/
</span></span></code></pre></div><p>Git structure should&rsquo;ve not changed, double check&hellip;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/workspace/vllm# git remote -v
</span></span><span style="display:flex;"><span>origin  https://github.com/davidgao7/vllm.git <span style="color:#f92672">(</span>fetch<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>origin  https://github.com/davidgao7/vllm.git <span style="color:#f92672">(</span>push<span style="color:#f92672">)</span>
</span></span></code></pre></div><p>Noice, 😅</p>
<p>Try that again&hellip;
Oh yea don&rsquo;t forget to create virtual environment again and activate it.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>VLLM_USE_PRECOMPILED<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> pip install --editable .
</span></span></code></pre></div><p>Install python env</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/workspace/vllm# uv venv --python 3.12 --seed
</span></span><span style="display:flex;"><span>Using CPython 3.12.10
</span></span><span style="display:flex;"><span>Creating virtual environment with seed packages at: .venv
</span></span><span style="display:flex;"><span>warning: Failed to hardlink files; falling back to full copy. This may lead to degraded performance.
</span></span><span style="display:flex;"><span>         If the cache and target directories are on different filesystems, hardlinking may not be supported.
</span></span><span style="display:flex;"><span>         If this is intentional, set <span style="color:#e6db74">`</span>export UV_LINK_MODE<span style="color:#f92672">=</span>copy<span style="color:#e6db74">`</span> or use <span style="color:#e6db74">`</span>--link-mode<span style="color:#f92672">=</span>copy<span style="color:#e6db74">`</span> to suppress this warning.
</span></span><span style="display:flex;"><span> + pip<span style="color:#f92672">==</span>25.1.1
</span></span><span style="display:flex;"><span>Activate with: source .venv/bin/activate
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>.venv<span style="color:#f92672">)</span> root@f00742593164:/workspace/vllm# source .venv/bin/activate
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>vllm<span style="color:#f92672">)</span> root@f00742593164:/workspace/vllm# python
</span></span><span style="display:flex;"><span>Python 3.12.10 <span style="color:#f92672">(</span>main, Apr  <span style="color:#ae81ff">9</span> 2025, 04:03:51<span style="color:#f92672">)</span> <span style="color:#f92672">[</span>Clang 20.1.0 <span style="color:#f92672">]</span> on linux
</span></span><span style="display:flex;"><span>Type <span style="color:#e6db74">&#34;help&#34;</span>, <span style="color:#e6db74">&#34;copyright&#34;</span>, <span style="color:#e6db74">&#34;credits&#34;</span> or <span style="color:#e6db74">&#34;license&#34;</span> <span style="color:#66d9ef">for</span> more information.
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; exit<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>vllm<span style="color:#f92672">)</span> root@f00742593164:/workspace/vllm#
</span></span></code></pre></div><p>Install dependencies</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>VLLM_USE_PRECOMPILED<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> pip install --editable .
</span></span></code></pre></div><p>Let&rsquo;s try if vllm really work, here is an <a href="https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/basic/basic.py">example script</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> vllm <span style="color:#f92672">import</span> LLM, SamplingParams
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Sample prompts.</span>
</span></span><span style="display:flex;"><span>prompts <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Hello, my name is&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;The president of the United States is&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;The capital of France is&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;The future of AI is&#34;</span>,
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create a sampling params object.</span>
</span></span><span style="display:flex;"><span>sampling_params <span style="color:#f92672">=</span> SamplingParams(temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>, top_p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.95</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Create an LLM.</span>
</span></span><span style="display:flex;"><span>    llm <span style="color:#f92672">=</span> LLM(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;facebook/opt-125m&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Generate texts from the prompts.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The output is a list of RequestOutput objects</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># that contain the prompt, generated text, and other information.</span>
</span></span><span style="display:flex;"><span>    outputs <span style="color:#f92672">=</span> llm<span style="color:#f92672">.</span>generate(prompts, sampling_params)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Print the outputs.</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Generated Outputs:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;-&#34;</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">60</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> output <span style="color:#f92672">in</span> outputs:
</span></span><span style="display:flex;"><span>        prompt <span style="color:#f92672">=</span> output<span style="color:#f92672">.</span>prompt
</span></span><span style="display:flex;"><span>        generated_text <span style="color:#f92672">=</span> output<span style="color:#f92672">.</span>outputs[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>text
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Prompt:    </span><span style="color:#e6db74">{</span>prompt<span style="color:#e6db74">!r}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Output:    </span><span style="color:#e6db74">{</span>generated_text<span style="color:#e6db74">!r}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;-&#34;</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">60</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    main()
</span></span></code></pre></div><p>In this example, we pull the opt-125m model from huggingface, and generate some text with it.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>uv run examples<span style="color:#f92672">/</span>offline_inference<span style="color:#f92672">/</span>basic<span style="color:#f92672">/</span>basic<span style="color:#f92672">.</span>py
</span></span></code></pre></div><p>Finally, if your machine is powerful enough(and you store at <em>correct large</em> disk), you could run one inference(multi-inferences when speaking about per token)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#f92672">(</span>vllm<span style="color:#f92672">)</span> root@49924402e807:/workspace/vllm# uv run examples/offline_inference/basic/basic.py
</span></span><span style="display:flex;"><span>INFO 05-09 06:46:59 <span style="color:#f92672">[</span>__init__.py:248<span style="color:#f92672">]</span> Automatically detected platform cuda.
</span></span><span style="display:flex;"><span>config.json: 100%|███████████████████████████████████████████████████████████████| 651/651 <span style="color:#f92672">[</span>00:00&lt;00:00, 3.33MB/s<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>INFO 05-09 06:47:58 <span style="color:#f92672">[</span>config.py:752<span style="color:#f92672">]</span> This model supports multiple tasks: <span style="color:#f92672">{</span><span style="color:#e6db74">&#39;classify&#39;</span>, <span style="color:#e6db74">&#39;score&#39;</span>, <span style="color:#e6db74">&#39;generate&#39;</span>, <span style="color:#e6db74">&#39;embed&#39;</span>, <span style="color:#e6db74">&#39;reward&#39;</span><span style="color:#f92672">}</span>. Defaulting to <span style="color:#e6db74">&#39;generate&#39;</span>.
</span></span><span style="display:flex;"><span>INFO 05-09 06:47:58 <span style="color:#f92672">[</span>config.py:2057<span style="color:#f92672">]</span> Chunked prefill is enabled with max_num_batched_tokens<span style="color:#f92672">=</span>8192.
</span></span><span style="display:flex;"><span>tokenizer_config.json: 100%|█████████████████████████████████████████████████████| 685/685 <span style="color:#f92672">[</span>00:00&lt;00:00, 4.21MB/s<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>vocab.json: 100%|██████████████████████████████████████████████████████████████| 899k/899k <span style="color:#f92672">[</span>00:00&lt;00:00, 3.76MB/s<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>merges.txt: 100%|██████████████████████████████████████████████████████████████| 456k/456k <span style="color:#f92672">[</span>00:00&lt;00:00, 2.91MB/s<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>special_tokens_map.json: 100%|███████████████████████████████████████████████████| 441/441 <span style="color:#f92672">[</span>00:00&lt;00:00, 3.28MB/s<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>generation_config.json: 100%|█████████████████████████████████████████████████████| 137/137 <span style="color:#f92672">[</span>00:00&lt;00:00, 809kB/s<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>INFO 05-09 06:48:05 <span style="color:#f92672">[</span>core.py:61<span style="color:#f92672">]</span> Initializing a V1 LLM engine <span style="color:#f92672">(</span>v0.1.dev6367+g3d1e387<span style="color:#f92672">)</span> with config: model<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;facebook/opt-125m&#39;</span>, speculative_config<span style="color:#f92672">=</span>None, tokenizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;facebook/opt-125m&#39;</span>, skip_tokenizer_init<span style="color:#f92672">=</span>False, tokenizer_mode<span style="color:#f92672">=</span>auto, revision<span style="color:#f92672">=</span>None, override_neuron_config<span style="color:#f92672">={}</span>, tokenizer_revision<span style="color:#f92672">=</span>None, trust_remote_code<span style="color:#f92672">=</span>False, dtype<span style="color:#f92672">=</span>torch.float16, max_seq_len<span style="color:#f92672">=</span>2048, download_dir<span style="color:#f92672">=</span>None, load_format<span style="color:#f92672">=</span>LoadFormat.AUTO, tensor_parallel_size<span style="color:#f92672">=</span>1, pipeline_parallel_size<span style="color:#f92672">=</span>1, disable_custom_all_reduce<span style="color:#f92672">=</span>False, quantization<span style="color:#f92672">=</span>None, enforce_eager<span style="color:#f92672">=</span>False, kv_cache_dtype<span style="color:#f92672">=</span>auto,  device_config<span style="color:#f92672">=</span>cuda, decoding_config<span style="color:#f92672">=</span>DecodingConfig<span style="color:#f92672">(</span>backend<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;auto&#39;</span>, disable_fallback<span style="color:#f92672">=</span>False, disable_any_whitespace<span style="color:#f92672">=</span>False, disable_additional_properties<span style="color:#f92672">=</span>False, reasoning_backend<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;</span><span style="color:#f92672">)</span>, observability_config<span style="color:#f92672">=</span>ObservabilityConfig<span style="color:#f92672">(</span>show_hidden_metrics_for_version<span style="color:#f92672">=</span>None, otlp_traces_endpoint<span style="color:#f92672">=</span>None, collect_detailed_traces<span style="color:#f92672">=</span>None<span style="color:#f92672">)</span>, seed<span style="color:#f92672">=</span>None, served_model_name<span style="color:#f92672">=</span>facebook/opt-125m, num_scheduler_steps<span style="color:#f92672">=</span>1, multi_step_stream_outputs<span style="color:#f92672">=</span>True, enable_prefix_caching<span style="color:#f92672">=</span>True, chunked_prefill_enabled<span style="color:#f92672">=</span>True, use_async_output_proc<span style="color:#f92672">=</span>True, pooler_config<span style="color:#f92672">=</span>None, compilation_config<span style="color:#f92672">={</span><span style="color:#e6db74">&#34;level&#34;</span>:3,<span style="color:#e6db74">&#34;custom_ops&#34;</span>:<span style="color:#f92672">[</span><span style="color:#e6db74">&#34;none&#34;</span><span style="color:#f92672">]</span>,<span style="color:#e6db74">&#34;splitting_ops&#34;</span>:<span style="color:#f92672">[</span><span style="color:#e6db74">&#34;vllm.unified_attention&#34;</span>,<span style="color:#e6db74">&#34;vllm.unified_attention_with_output&#34;</span><span style="color:#f92672">]</span>,<span style="color:#e6db74">&#34;use_inductor&#34;</span>:true,<span style="color:#e6db74">&#34;compile_sizes&#34;</span>:<span style="color:#f92672">[]</span>,<span style="color:#e6db74">&#34;use_cudagraph&#34;</span>:true,<span style="color:#e6db74">&#34;cudagraph_num_of_warmups&#34;</span>:1,<span style="color:#e6db74">&#34;cudagraph_capture_sizes&#34;</span>:<span style="color:#f92672">[</span>512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1<span style="color:#f92672">]</span>,<span style="color:#e6db74">&#34;max_capture_size&#34;</span>:512<span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>WARNING 05-09 06:48:09 <span style="color:#f92672">[</span>utils.py:2587<span style="color:#f92672">]</span> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in &lt;vllm.v1.worker.gpu_worker.Worker object at 0x7fb0ff70d8b0&gt;
</span></span><span style="display:flex;"><span>INFO 05-09 06:48:10 <span style="color:#f92672">[</span>parallel_state.py:1004<span style="color:#f92672">]</span> rank <span style="color:#ae81ff">0</span> in world size <span style="color:#ae81ff">1</span> is assigned as DP rank 0, PP rank 0, TP rank <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>INFO 05-09 06:48:10 <span style="color:#f92672">[</span>cuda.py:221<span style="color:#f92672">]</span> Using Flash Attention backend on V1 engine.
</span></span><span style="display:flex;"><span>WARNING 05-09 06:48:10 <span style="color:#f92672">[</span>topk_topp_sampler.py:69<span style="color:#f92672">]</span> FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p &amp; top-k sampling. For the best performance, please install FlashInfer.
</span></span><span style="display:flex;"><span>INFO 05-09 06:48:10 <span style="color:#f92672">[</span>gpu_model_runner.py:1393<span style="color:#f92672">]</span> Starting to load model facebook/opt-125m...
</span></span><span style="display:flex;"><span>INFO 05-09 06:48:11 <span style="color:#f92672">[</span>weight_utils.py:257<span style="color:#f92672">]</span> Using model weights format <span style="color:#f92672">[</span><span style="color:#e6db74">&#39;*.bin&#39;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>pytorch_model.bin: 100%|███████████████████████████████████████████████████████| 251M/251M <span style="color:#f92672">[</span>00:03&lt;00:00, 83.0MB/s<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>INFO 05-09 06:48:15 <span style="color:#f92672">[</span>weight_utils.py:273<span style="color:#f92672">]</span> Time spent downloading weights <span style="color:#66d9ef">for</span> facebook/opt-125m: 3.667131 seconds
</span></span><span style="display:flex;"><span>Loading pt checkpoint shards:   0% Completed | 0/1 <span style="color:#f92672">[</span>00:00&lt;?, ?it/s<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>Loading pt checkpoint shards: 100% Completed | 1/1 <span style="color:#f92672">[</span>00:00&lt;00:00,  3.20it/s<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>Loading pt checkpoint shards: 100% Completed | 1/1 <span style="color:#f92672">[</span>00:00&lt;00:00,  3.19it/s<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>INFO 05-09 06:48:15 <span style="color:#f92672">[</span>default_loader.py:278<span style="color:#f92672">]</span> Loading weights took 0.32 seconds
</span></span><span style="display:flex;"><span>INFO 05-09 06:48:15 <span style="color:#f92672">[</span>gpu_model_runner.py:1411<span style="color:#f92672">]</span> Model loading took 0.2389 GiB and 4.768521 seconds
</span></span><span style="display:flex;"><span>INFO 05-09 06:48:24 <span style="color:#f92672">[</span>backends.py:437<span style="color:#f92672">]</span> Using cache directory: /root/.cache/vllm/torch_compile_cache/f75f64201d/rank_0_0 <span style="color:#66d9ef">for</span> vLLM<span style="color:#e6db74">&#39;s torch.compile
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">INFO 05-09 06:48:24 [backends.py:447] Dynamo bytecode transform time: 8.85 s
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">INFO 05-09 06:48:27 [backends.py:138] Cache the graph of shape None for later use
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">INFO 05-09 06:48:34 [backends.py:150] Compiling a graph for general shape takes 9.91 s
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">INFO 05-09 06:48:37 [monitor.py:33] torch.compile takes 18.75 s in total
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">INFO 05-09 06:48:38 [kv_cache_utils.py:639] GPU KV cache size: 582,208 tokens
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">INFO 05-09 06:48:38 [kv_cache_utils.py:642] Maximum concurrency for 2,048 tokens per request: 284.28x
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">INFO 05-09 06:48:59 [gpu_model_runner.py:1774] Graph capturing finished in 21 secs, took 0.20 GiB
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">INFO 05-09 06:48:59 [core.py:163] init engine (profile, create kv cache, warmup model) took 43.73 seconds
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">INFO 05-09 06:48:59 [core_client.py:442] Core engine process 0 ready.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Adding requests: 100%|█████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00, 233.82it/s]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Processed prompts: 100%|█████| 4/4 [00:00&lt;00:00, 11.66it/s, est. speed input: 75.80 toks/s, output: 186.57 toks/s]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Generated Outputs:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">------------------------------------------------------------
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Prompt:    &#39;</span>Hello, my name is<span style="color:#e6db74">&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Output:    &#39;</span> Ken Li and I’m a computer science student. My primary interest is<span style="color:#e6db74">&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">------------------------------------------------------------
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Prompt:    &#39;</span>The president of the United States is<span style="color:#e6db74">&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Output:    &#39;</span> attempting to subvert a very important issue, one which many of us are of<span style="color:#e6db74">&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">------------------------------------------------------------
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Prompt:    &#39;</span>The capital of France is<span style="color:#e6db74">&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Output:    &#39;</span> filled with tourists, retirees and independent artists but, by far, the most popular<span style="color:#e6db74">&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">------------------------------------------------------------
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Prompt:    &#39;</span>The future of AI is<span style="color:#e6db74">&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Output:    &#39;</span> becoming more connected. This requires more research and development than ever before. With the<span style="color:#960050;background-color:#1e0010">&#39;</span>
</span></span><span style="display:flex;"><span>------------------------------------------------------------
</span></span></code></pre></div><p>Love the verbose logs.</p>
<hr>
<p><font color=red>Message from future me: you&rsquo;ll face a lot of unrelated problem when trying to deploy on a single pod. so start with cluster if you have!</font></p>
<p>Now for the real problem needs to solve, we need to add the <code>/v1/audio/transcriptions</code> support to the production stack.</p>
<h1 id="poc-add-v1audiotranscriptions-support-to-vllm-production-stack">POC: Add <code>/v1/audio/transcriptions</code> Support to vLLM Production-Stack</h1>
<h2 id="1-background">1. Background</h2>
<ul>
<li><strong>GitHub Issue</strong>: <a href="https://github.com/vllm-project/production-stack/issues/410">vllm-project/production-stack#410</a></li>
<li><strong>Problem</strong>:<br>
The vLLM backend already supports <code>/v1/audio/transcriptions</code> (Whisper-based ASR),<br>
but the <strong>production-stack router</strong> does not expose it, causing 404 errors.</li>
<li><strong>Goal</strong>:<br>
Extend the production-stack router to expose the <code>/v1/audio/transcriptions</code> endpoint and successfully forward requests to the backend.</li>
</ul>
<hr>
<h2 id="2-environment-setup">2. Environment Setup</h2>
<ul>
<li>
<p>spin up an instant cluster in runpod is MUCH easier</p>
</li>
<li>
<p><a href="/posts/wrong-vllm-production-stack-setup/">here is the issue I faced when I try to do in one machine</a></p>
</li>
<li>
<p>Through sweats and tears, trails and errors, I finally find the correct way: use the <a href="https://docs.runpod.io/instant-clusters">Instant clusters</a></p>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th>Pain in single pod</th>
          <th>Why it breaks</th>
          <th>How Instant Clusters fix it</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>OverlayFS / bind‑mount denied</strong></td>
          <td>Container security on a pod</td>
          <td>Nodes are full VMs → kernel features allowed (<a href="https://docs.runpod.io/instant-clusters" title="Overview | RunPod Documentation">RunPod Docs</a>)</td>
      </tr>
      <tr>
          <td><strong><code>/dev/kmsg</code> &amp; other device files missing</strong></td>
          <td>Device files masked</td>
          <td>Full device namespace, kubelet starts cleanly</td>
      </tr>
      <tr>
          <td><strong>Static intra‑cluster networking needed</strong></td>
          <td>Pod IPs are ephemeral</td>
          <td>Each node gets a static <code>NODE_ADDR</code>; primary node gets <code>PRIMARY_ADDR</code> (<a href="https://docs.runpod.io/instant-clusters" title="Overview | RunPod Documentation">RunPod Docs</a>)</td>
      </tr>
      <tr>
          <td><strong>GPU scheduling</strong></td>
          <td>No privileged runtime</td>
          <td>Install NVIDIA device‑plugin once; GPUs appear in <code>kubectl describe node</code></td>
      </tr>
      <tr>
          <td><strong>Scalability</strong></td>
          <td>Single host only</td>
          <td>Up to 8 nodes × 8 GPUs per node (64 GPU max) with high‑speed mesh (<a href="https://docs.runpod.io/instant-clusters" title="Overview | RunPod Documentation">RunPod Docs</a>)</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="so-what-machine-configuration-should-i-choose">So what machine configuration should I choose?</h2>
<table>
  <thead>
      <tr>
          <th>Option</th>
          <th>Meets K8s‑production need?</th>
          <th>Gives GPU node?</th>
          <th>Avoids earlier sandbox errors?</th>
          <th>Practical notes</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>RunPod Single Pod</strong></td>
          <td>✘ (kubelet fails)</td>
          <td>✔ RTX 3090</td>
          <td>✘ sandbox limits</td>
          <td>Already ruled out</td>
      </tr>
      <tr>
          <td><strong>RunPod Instant Cluster</strong></td>
          <td>✘ (K8s not supported)</td>
          <td>✔ multi‑GPU</td>
          <td>✔ root VM, but no kube‑API</td>
          <td>Great for PyTorch/Slurm, not Helm</td>
      </tr>
      <tr>
          <td><strong>RunPod Bare Metal</strong></td>
          <td>✔ install K3s easily</td>
          <td>✔ pick 1–8 GPUs</td>
          <td>✔ full server</td>
          <td>Same RunPod UI; hourly billing</td>
      </tr>
      <tr>
          <td><strong>AWS / Azure / GCP managed K8s (EKS / AKS / GKE)</strong></td>
          <td>✔ fully managed control‑plane</td>
          <td>✔ add GPU node‑pool</td>
          <td>✔ real VMs</td>
          <td>Slightly higher cost; IAM setup</td>
      </tr>
      <tr>
          <td><strong>Self‑host K3s on any GPU VM (Lambda, Paperspace, Hetzner, OCI)</strong></td>
          <td>✔ single‑node or multi‑node</td>
          <td>✔ if you rent GPU VM</td>
          <td>✔ full root</td>
          <td>You manage everything, but simple for dev work</td>
      </tr>
  </tbody>
</table>
<ul>
<li>uhh, I still have 10 bucks remain on runpod&hellip;
<ul>
<li>Bare metal cluster cheapest one cost hundreds of bucks :(</li>
<li>I do need a k8s environment</li>
</ul>
</li>
</ul>
<h2 id="i-found-gke-is-the-cheapest-option">I found GKE is the cheapest option</h2>
<p>Why GKE?</p>
<ul>
<li><em>Focus on your app, not infrastructure</em>: GKE handles the heavy lifting of managing Kubernetes, letting you concentrate on building and innovating.</li>
<li><em>GPU Power Made Easy</em>: GKE makes it straightforward to use GPUs, which is crucial for your AI/ML workloads and resolving your specific bug.</li>
<li><em>Scale Effortlessly</em>: GKE grows with you, easily handling increased demands on your application without you having to worry about the details.</li>
<li><em>Production-Ready Reproduction</em>: GKE gives you the environment you need to pinpoint and fix the router pod, vLLM + Whisper worker, and GPU interaction bug.</li>
<li><em>Plays Well with Everything</em>: GKE integrates smoothly with other Google Cloud services you might need.
Cost-Effective: GKE has features like Autopilot mode and cost optimization tools to help you manage your spending.</li>
<li><em>Secure</em>: GKE offers built-in security features to protect your applications.
Future-Proof: Kubernetes is the industry standard, and GKE is a leading managed Kubernetes service.</li>
</ul>
<ol>
<li>standard cluster</li>
<li>us-east-1</li>
<li>fleet registration: A Google Cloud fleet is a top‑level “super‑cluster” object that lets you manage multiple Kubernetes clusters (GKE, on‑prem, or other clouds) as a single unit—enabling shared policies, multi‑cluster services, and centralized observability with no extra cost.</li>
<li>CIDR only allow my ip addr</li>
</ol>
<h2 id="connect-to-cluster">Connect to cluster</h2>
<p>For google cloud we can install <code>gcloud</code> cli tool, and use <code>gcloud container clusters get-credentials</code> to connect to the cluster</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gcloud container clusters get-credentials autopilot-cluster-1 --region us-east1
</span></span></code></pre></div><ul>
<li>If you don&rsquo;t have <code>gcloud</code>, <a href="https://cloud.google.com/sdk/docs/install">here is a tutorial how to install it</a>:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-darwin-arm.tar.gz
</span></span><span style="display:flex;"><span>tar -xf google-cloud-cli-darwin-arm.tar.gz
</span></span><span style="display:flex;"><span>cd google-cloud-sdk
</span></span><span style="display:flex;"><span>chmod +x install.sh  <span style="color:#75715e"># just in case...</span>
</span></span><span style="display:flex;"><span>sh install.sh
</span></span></code></pre></div><p>And it&rsquo;s writing on your rc file&hellip; Check the execution order, you can modify yourself.</p>
<p>Then login your gcloud cli, and setup corresponding project id</p>
<ol>
<li>login</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gcloud auth login
</span></span></code></pre></div><ol start="2">
<li>setup corresponding project id</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gcloud config set project gen-lang-client-0475913374
</span></span></code></pre></div><ol start="3">
<li>connect the local machine <code>kubectl</code> to cluster</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gcloud container clusters get-credentials autopilot-cluster-1 --region us-east1
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Fetching cluster endpoint and auth data.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># CRITICAL: ACTION REQUIRED: gke-gcloud-auth-plugin, which is needed for continued use of kubectl, was not found or is not executable. Install gke-gcloud-auth-plugin for use with kubectl by following https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl#install_plugin</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># kubeconfig entry generated for autopilot-cluster-1.</span>
</span></span></code></pre></div><p>Ok more installations,</p>
<ol start="4">
<li>Install required plugins</li>
</ol>
<p><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl#install_plugin">reference</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gcloud components install gke-gcloud-auth-plugin
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Your current Google Cloud CLI version is: 521.0.0</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Installing components from version: 521.0.0</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ┌────────────────────────────────────────────────────────────────┐</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># │              These components will be installed.               │</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ├────────────────────────────────────────────┬─────────┬─────────┤</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># │                    Name                    │ Version │   Size  │</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ├────────────────────────────────────────────┼─────────┼─────────┤</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># │ gke-gcloud-auth-plugin (Platform Specific) │  0.5.10 │ 3.3 MiB │</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># └────────────────────────────────────────────┴─────────┴─────────┘</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># For the latest full release notes, please visit:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   https://cloud.google.com/sdk/release_notes</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Once started, canceling this operation may leave your SDK installation in an inconsistent state.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Do you want to continue (Y/n)?  Y</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Performing in place update...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ╔════════════════════════════════════════════════════════════╗</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ╠═ Downloading: gke-gcloud-auth-plugin                      ═╣</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ╠════════════════════════════════════════════════════════════╣</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ╠═ Downloading: gke-gcloud-auth-plugin (Platform Specific)  ═╣</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ╠════════════════════════════════════════════════════════════╣</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ╠═ Installing: gke-gcloud-auth-plugin                       ═╣</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ╠════════════════════════════════════════════════════════════╣</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ╠═ Installing: gke-gcloud-auth-plugin (Platform Specific)   ═╣</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ╚════════════════════════════════════════════════════════════╝</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Performing post processing steps...done.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Update done!</span>
</span></span></code></pre></div><ol start="5">
<li>Try to initialize the cluster again</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gcloud container clusters get-credentials autopilot-cluster-1 --region us-east1 <span style="color:#75715e"># or other region</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Fetching cluster endpoint and auth data.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># kubeconfig entry generated for autopilot-cluster-1.</span>
</span></span></code></pre></div><p>[!NOTE]
Performance Monitoring: To monitor and verify the latency and performance between your location and the GKE cluster, you can utilize tools like <a href="https://www.gcping.com/">GCPing</a>, which provides latency measurements to various Google Cloud regions.</p>
<ol start="6">
<li>Connect to the cluster 🔥</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>&gt; kubectl get nodes
</span></span><span style="display:flex;"><span>No resources found
</span></span></code></pre></div><p>Nice, it&rsquo;s working</p>
<h2 id="install-nvidia-gpu-device-plugin-need">Install NVIDIA GPU Device Plugin (Need?)</h2>
<ul>
<li>Currently no, GKE Autopilot automatically provisions GPU nodes and installs the necessary NVIDIA drivers and device plugins when you deploy a workload that requests GPU resources.</li>
</ul>
<p>Psst Here is the Plugin repo</p>
<ul>
<li>Github: <a href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA/k8s-device-plugin</a></li>
<li>NGC Catalog: <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/k8s-device-plugin">NIVIDIA Device Plugin</a></li>
</ul>
<h2 id="reproducing-the-issue">Reproducing the issue</h2>
<h3 id="deploy-the-current-production-stack-on-gcp">Deploy the current production-stack on GCP</h3>
<p>For GCP, there are 2 tutorials:</p>
<ul>
<li><code>/tutorials/cloud_deployments/02-GCP-GKE-deployment.md</code></li>
<li><code>/deployment_on_cloud/gcp/README.md</code></li>
<li><code>/deployment_on_cloud/gcp/OPT125_CPU/README.md</code></li>
</ul>
<p>I already have GKE cluster, I just need to manually depoly the vLLM production stack using helm, no need to use the scripts to create cluster.</p>
<p>Add official Helm repo to my cluster.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>helm repo add production-stack https://vllm-project.github.io/production-stack-helm-charts/
</span></span><span style="display:flex;"><span>helm repo update
</span></span></code></pre></div><p>[!NOTE] I can access autopilot k8s cluster locally, think of autopilot as a serverless kubernetes experience. If I was to gain SSH access to an Autopilot node, it would be considered a <strong>security vulnerability</strong> because I could potentially bypass the Kubernetes API and run commands directly on the node.</p>
<p><a href="https://vllm-project.github.io/production-stack">Here is the official docs for vllm production deployment</a>
For code, we will use the <a href="https://vllm-project.github.io/production-stack-helm-charts/"><strong>Helm repo</strong></a></p>
<ul>
<li>The official documentation is using lmcache repo, which might be outdated.</li>
</ul>
<p>So , the correct way to deploy production-stack is</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># adds the URL to local list of helm repositories</span>
</span></span><span style="display:flex;"><span>helm repo add vllm https://vllm-project.github.io/production-stack
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># &gt; helm repo add vllm https://vllm-project.github.io/production-stack</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># zsh: command not found: helm</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># downloads and refreshes the latest list of charts from the repo added</span>
</span></span><span style="display:flex;"><span>helm repo update
</span></span></code></pre></div><p>Instead of <code>lmcache.github.io/helm/</code></p>
<p>😅need to install helm, yay one more install</p>
<p>To manage Kubernetes deployments on GKE Autopilot cluster using Helm, need to install Helm CLI on local machine.</p>
<p><a href="https://helm.sh/docs/intro/install/">Here is the official docs on how to install Helm</a>
Here is the one-liner</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># &gt; curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#                                  Dload  Upload   Total   Spent    Left  Speed</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 100 11913  100 11913    0     0  61922      0 --:--:-- --:--:-- --:--:-- 62046</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Downloading https://get.helm.sh/helm-v3.17.3-darwin-arm64.tar.gz</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Verifying checksum... Done.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Preparing to install helm into /usr/local/bin</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Password:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># helm installed into /usr/local/bin/helm</span>
</span></span></code></pre></div><p>Rerun the repo add and update command</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># adds the URL to local list of helm repositories</span>
</span></span><span style="display:flex;"><span>helm repo add vllm https://vllm-project.github.io/production-stack
</span></span><span style="display:flex;"><span><span style="color:#75715e"># &#34;vllm&#34; has been added to your repositories</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># downloads and refreshes the latest list of charts from the repo added</span>
</span></span><span style="display:flex;"><span>helm repo update
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Hang tight while we grab the latest from your chart repositories...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...Successfully got an update from the &#34;vllm&#34; chart repository</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Update Complete. ⎈Happy Helming!⎈</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>helm search repo vllm
</span></span><span style="display:flex;"><span><span style="color:#75715e"># NAME            CHART VERSION   APP VERSION     DESCRIPTION</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># vllm/vllm-stack 0.1.2                           The stack deployment of vLLM</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># quick sanity check</span>
</span></span><span style="display:flex;"><span>helm show values vllm/vllm-stack
</span></span><span style="display:flex;"><span><span style="color:#75715e"># # -- Default values for llmstack helm chart</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># # -- Declare variables to be passed into your templates.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># # -- Serving engine configuratoon</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># servingEngineSpec:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   enableEngine: true</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Customized labels for the serving engine deployment</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   labels:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     environment: &#34;test&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     release: &#34;test&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # vllmApiKey: (optional) api key for securing the vLLM models. Can be either:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - A string containing the token directly (will be stored in a generated secret)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - An object referencing an existing secret:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     secretName: &#34;my-existing-secret&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     secretKey: &#34;vllm-api-key&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # modelSpec - configuring multiple serving engines deployments that runs different models</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # Each entry in the modelSpec array should contain the following fields:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - name: (string) The name of the model, e.g., &#34;example-model&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - repository: (string) The repository of the model, e.g., &#34;vllm/vllm-openai&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - tag: (string) The tag of the model, e.g., &#34;latest&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - imagePullSecret: (Optional, string) Name of secret with credentials to private container repository, e.g. &#34;secret&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - modelURL: (string) The URL of the model, e.g., &#34;facebook/opt-125m&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - chatTemplate: (Optional, string) Chat template (Jinga2) specifying tokenizer configuration, e.g. &#34;{% for message in messages %}\n{% if message[&#39;role&#39;] == &#39;user&#39; %}\n{{ &#39;Question:\n&#39; + message[&#39;content&#39;] + &#39;\n\n&#39; }}{% elif message[&#39;role&#39;] == &#39;system&#39; %}\n{{ &#39;System:\n&#39; + message[&#39;content&#39;] + &#39;\n\n&#39; }}{% elif message[&#39;role&#39;] == &#39;assistant&#39; %}{{ &#39;Answer:\n&#39;  + message[&#39;content&#39;] + &#39;\n\n&#39; }}{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ &#39;Answer:\n&#39; }}{% endif %}{% endfor %}&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - replicaCount: (int) The number of replicas for the model, e.g. 1</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - requestCPU: (int) The number of CPUs requested for the model, e.g. 6</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - requestMemory: (string) The amount of memory requested for the model, e.g., &#34;16Gi&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - requestGPU: (int) The number of GPUs requested for the model, e.g., 1</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - requestGPUType: (Optional, string) The type of GPU requested, e.g., &#34;nvidia.com/mig-4g.71gb&#34;. If not specified, defaults to &#34;nvidia.com/gpu&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - limitCPU: (Optional, string) The CPU limit for the model, e.g., &#34;8&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - limitMemory: (Optional, string) The memory limit for the model, e.g., &#34;32Gi&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # Note: If limitCPU and limitMemory are not specified, only GPU resources will have limits set equal to their requests.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - pvcStorage: (Optional, string) The amount of storage requested for the model, e.g., &#34;50Gi&#34;.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - pvcAccessMode: (Optional, list) The access mode policy for the mounted volume, e.g., [&#34;ReadWriteOnce&#34;]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - storageClass: (Optional, String) The storage class of the PVC e.g., &#34;&#34;, default is &#34;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - pvcMatchLabels: (Optional, map) The labels to match the PVC, e.g., {model: &#34;opt125m&#34;}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - extraVolumes: (Optional, list) Additional volumes to add to the pod, in Kubernetes volume format. https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#volume-v1-core</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   Example for an emptyDir volume:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   extraVolumes:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - name: tmp-volume</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     emptyDir:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #       medium: &#34;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #       sizeLimit: 5Gi</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - extraVolumeMounts: (Optional, list) Additional volume mounts to add to the container, in Kubernetes volumeMount format. https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#volumemount-v1-core</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   Example for mounting the tmp-volume to /tmp:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   extraVolumeMounts:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - name: tmp-volume</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     mountPath: /tmp</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - initContainer: (optional, list of objects) The configuration for the init container to be run before the main container.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - name: (string) The name of the init container, e.g., &#34;init&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - image: (string) The Docker image for the init container, e.g., &#34;busybox:latest&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - command: (optional, list) The command to run in the init container, e.g., [&#34;sh&#34;, &#34;-c&#34;]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - args: (optional, list) Additional arguments to pass to the command, e.g., [&#34;ls&#34;]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - env: (optional, list) List of environment variables to set in the container, each being a map with:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - resources: (optional, map) The resource requests and limits for the container:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - mountPvcStorage: (optional, bool) Whether to mount the model&#39;s volume.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - vllmConfig: (optional, map) The configuration for the VLLM model, supported options are:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - enablePrefixCaching: (optional, bool) Enable prefix caching, e.g., false</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - enableChunkedPrefill: (optional, bool) Enable chunked prefill, e.g., false</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - maxModelLen: (optional, int) The maximum model length, e.g., 16384</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - dtype: (optional, string) The data type, e.g., &#34;bfloat16&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - tensorParallelSize: (optional, int) The degree of tensor parallelism, e.g., 2</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - maxNumSeqs: (optional, int) Maximum number of sequences to be processed in a single iteration., e.g., 32</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - maxLoras: (optional, int) The maximum number of LoRA models to be loaded in a single batch, e.g., 4</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - gpuMemoryUtilization: (optional, float) The fraction of GPU memory to be used for the model executor, which can range from 0 to 1. e.g., 0.95</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - extraArgs: (optional, list) Extra command line arguments to pass to vLLM, e.g., [&#34;--disable-log-requests&#34;]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - lmcacheConfig: (optional, map) The configuration of the LMCache for KV offloading, supported options are:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - enabled: (optional, bool) Enable LMCache, e.g., true</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - cpuOffloadingBufferSize: (optional, string) The CPU offloading buffer size, e.g., &#34;30&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - hf_token: (optional) Hugging Face token configuration. Can be either:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - A string containing the token directly (will be stored in a generated secret)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - An object referencing an existing secret:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     secretName: &#34;my-existing-secret&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     secretKey: &#34;hf-token-key&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - env: (optional, list) The environment variables to set in the container, e.g., your HF_TOKEN</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - nodeSelectorTerms: (optional, list) The node selector terms to match the nodes</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - shmSize: (optional, string) The size of the shared memory, e.g., &#34;20Gi&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # Example:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # vllmApiKey: &#34;vllm_xxxxxxxxxxxxx&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # modelSpec:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # - name: &#34;mistral&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   repository: &#34;lmcache/vllm-openai&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   tag: &#34;latest&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   modelURL: &#34;mistralai/Mistral-7B-Instruct-v0.2&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   replicaCount: 1</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   requestCPU: 10</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   requestMemory: &#34;64Gi&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   requestGPU: 1</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   pvcStorage: &#34;50Gi&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   pvcAccessMode:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     - ReadWriteOnce</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   pvcMatchLabels:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     model: &#34;mistral&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   initContainer:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     name: my-container</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     image: busybox</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     command: [&#34;sh&#34;]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     env: {}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     args: []</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     resources: {}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     mountPvcStorage: true</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   vllmConfig:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     enableChunkedPrefill: false</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     enablePrefixCaching: false</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     maxModelLen: 16384</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     dtype: &#34;bfloat16&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     maxNumSeqs: 32</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     gpuMemoryUtilization: 0.95</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     maxLoras: 4</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     extraArgs: [&#34;--disable-log-requests&#34;, &#34;--trust-remote-code&#34;]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   lmcacheConfig:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     enabled: true</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     cpuOffloadingBufferSize: &#34;30&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   hf_token: &#34;hf_xxxxxxxxxxxxx&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   nodeSelectorTerms:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     - matchExpressions:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #       - key: nvidia.com/gpu.product</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #         operator: &#34;In&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #         values:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #         - &#34;NVIDIA-RTX-A6000&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   modelSpec: []</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Container port</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   containerPort: 8000</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Service port</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   servicePort: 80</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Set other environment variables from config map</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   configs: {}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- deployment strategy</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   strategy: {}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Readiness probe configuration</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   startupProbe:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Number of seconds after the container has started before startup probe is initiated</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     initialDelaySeconds: 15</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- How often (in seconds) to perform the startup probe</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     periodSeconds: 10</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Number of times after which if a probe fails in a row, Kubernetes considers that the overall check has failed: the container is not ready</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     failureThreshold:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       60</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       # -- Configuration of the Kubelet http request on the server</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     httpGet:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       # -- Path to access on the HTTP server</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       path: /health</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       # -- Name or number of the port to access on the container, on which the server is listening</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       port: 8000</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Liveness probe configuration</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   livenessProbe:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Number of seconds after the container has started before liveness probe is initiated</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     initialDelaySeconds: 15</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Number of times after which if a probe fails in a row, Kubernetes considers that the overall check has failed: the container is not alive</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     failureThreshold: 3</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- How often (in seconds) to perform the liveness probe</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     periodSeconds: 10</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Configuration of the Kubelet http request on the server</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     httpGet:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       # -- Path to access on the HTTP server</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       path: /health</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       # -- Name or number of the port to access on the container, on which the server is listening</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       port: 8000</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Disruption Budget Configuration</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   maxUnavailablePodDisruptionBudget: &#34;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Tolerations configuration (when there are taints on nodes)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # Example:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # tolerations:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   - key: &#34;node-role.kubernetes.io/control-plane&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     operator: &#34;Exists&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     effect: &#34;NoSchedule&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   tolerations: []</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- RuntimeClassName configuration, set to &#34;nvidia&#34; if the model requires GPU</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   runtimeClassName: &#34;nvidia&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- SchedulerName configuration</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   schedulerName: &#34;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Pod-level security context configuration. https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#podsecuritycontext-v1-core</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   securityContext: {}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Run as a non-root user ID</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # runAsUser: 1000</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Run with a non-root group ID</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # runAsGroup: 1000</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Run as non-root</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # runAsNonRoot: true</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Set the seccomp profile</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # seccompProfile:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     #   type: RuntimeDefault</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Drop all capabilities</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # capabilities:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     #   drop:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     #   - ALL</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Set the file system group ID for all containers</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # fsGroup: 1000</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Container-level security context configuration. https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#securitycontext-v1-core</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   containerSecurityContext:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Run as non-root</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     runAsNonRoot: false</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Don&#39;t allow privilege escalation</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # allowPrivilegeEscalation: false</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Drop all capabilities</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # capabilities:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     #   drop:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     #   - ALL</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Read-only root filesystem</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # readOnlyRootFilesystem: true</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># routerSpec:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- The docker image of the router. The following values are defaults:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   repository: &#34;lmcache/lmstack-router&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   tag: &#34;latest&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   imagePullPolicy: &#34;Always&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Whether to enable the router service</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   enableRouter: true</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Number of replicas</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   replicaCount: 1</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Container port</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   containerPort: 8000</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Service type</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   serviceType: ClusterIP</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Service port</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   servicePort: 80</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Service discovery mode, supports &#34;k8s&#34; or &#34;static&#34;. Defaults to &#34;k8s&#34; if not set.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   serviceDiscovery: &#34;k8s&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- If serviceDiscovery is set to &#34;static&#34;, the comma-separated values below are required. There needs to be the same number of backends and models</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   staticBackends: &#34;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   staticModels: &#34;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- routing logic, could be &#34;roundrobin&#34; or &#34;session&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   routingLogic: &#34;roundrobin&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- session key if using &#34;session&#34; routing logic</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   sessionKey: &#34;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- extra router commandline arguments</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   extraArgs: []</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Interval in seconds to scrape the serving engine metrics</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   engineScrapeInterval: 15</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Window size in seconds to calculate the request statistics</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   requestStatsWindow: 60</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- deployment strategy</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   strategy: {}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # vllmApiKey: (optional) api key for securing the vLLM models. Must be an object referencing an existing secret</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   secretName: &#34;my-existing-secret&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   secretKey: &#34;vllm-api-key&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- router resource requests and limits</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   resources:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     requests:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       cpu: &#34;4&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       memory: &#34;16G&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     limits:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       cpu: &#34;8&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       memory: &#34;32G&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Customized labels for the router deployment</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   labels:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     environment: &#34;router&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     release: &#34;router&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   ingress:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Enable ingress controller resource</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     enabled: false</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- IngressClass that will be used to implement the Ingress</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     className: &#34;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # -- Additional annotations for the Ingress resource</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     annotations:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       {}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       # kubernetes.io/ingress.class: alb</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       # kubernetes.io/ingress.class: nginx</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       # kubernetes.io/tls-acme: &#34;true&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # The list of hostnames to be covered with this ingress record.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     hosts:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#       - host: vllm-router.local</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         paths:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#           - path: /</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#             pathType: Prefix</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     # --  The tls configuration for hostnames to be covered with this ingress record.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     tls: []</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     #  - secretName: vllm-router-tls</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     #    hosts:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     #      - vllm-router.local</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # The node selector terms to match the nodes</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # Example:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   nodeSelectorTerms:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #     - matchExpressions:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #       - key: nvidia.com/gpu.product</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #         operator: &#34;In&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #         values:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #         - &#34;NVIDIA-RTX-A6000&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   nodeSelectorTerms: []</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- TODO: Readiness probe configuration</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #startupProbe:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #  # -- Number of seconds after the container has started before startup probe is initiated</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #  initialDelaySeconds: 5</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #  # -- How often (in seconds) to perform the startup probe</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #  periodSeconds: 5</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #  # -- Number of times after which if a probe fails in a row, Kubernetes considers that the overall check has failed: the container is not ready</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #  failureThreshold: 100</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #   # -- Configuration of the Kubelet http request on the server</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #  httpGet:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #    # -- Path to access on the HTTP server</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   #</span>
</span></span></code></pre></div><p>Let&rsquo;s parse the yaml file</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#ae81ff">&gt; helm show values vllm/vllm-stack | yq</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -- Default values for llmstack helm chart</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -- Declare variables to be passed into your templates.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -- Serving engine configuratoon</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">servingEngineSpec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">enableEngine</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Customized labels for the serving engine deployment</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">environment</span>: <span style="color:#e6db74">&#34;test&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">release</span>: <span style="color:#e6db74">&#34;test&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># vllmApiKey: (optional) api key for securing the vLLM models. Can be either:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - A string containing the token directly (will be stored in a generated secret)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - An object referencing an existing secret:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     secretName: &#34;my-existing-secret&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     secretKey: &#34;vllm-api-key&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># modelSpec - configuring multiple serving engines deployments that runs different models</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Each entry in the modelSpec array should contain the following fields:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - name: (string) The name of the model, e.g., &#34;example-model&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - repository: (string) The repository of the model, e.g., &#34;vllm/vllm-openai&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - tag: (string) The tag of the model, e.g., &#34;latest&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - imagePullSecret: (Optional, string) Name of secret with credentials to private container repository, e.g. &#34;secret&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - modelURL: (string) The URL of the model, e.g., &#34;facebook/opt-125m&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - chatTemplate: (Optional, string) Chat template (Jinga2) specifying tokenizer configuration, e.g. &#34;{% for message in messages %}\n{% if message[&#39;role&#39;] == &#39;user&#39; %}\n{{ &#39;Question:\n&#39; + message[&#39;content&#39;] + &#39;\n\n&#39; }}{% elif message[&#39;role&#39;] == &#39;system&#39; %}\n{{ &#39;System:\n&#39; + message[&#39;content&#39;] + &#39;\n\n&#39; }}{% elif message[&#39;role&#39;] == &#39;assistant&#39; %}{{ &#39;Answer:\n&#39;  + message[&#39;content&#39;] + &#39;\n\n&#39; }}{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ &#39;Answer:\n&#39; }}{% endif %}{% endfor %}&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - replicaCount: (int) The number of replicas for the model, e.g. 1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - requestCPU: (int) The number of CPUs requested for the model, e.g. 6</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - requestMemory: (string) The amount of memory requested for the model, e.g., &#34;16Gi&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - requestGPU: (int) The number of GPUs requested for the model, e.g., 1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - requestGPUType: (Optional, string) The type of GPU requested, e.g., &#34;nvidia.com/mig-4g.71gb&#34;. If not specified, defaults to &#34;nvidia.com/gpu&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - limitCPU: (Optional, string) The CPU limit for the model, e.g., &#34;8&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - limitMemory: (Optional, string) The memory limit for the model, e.g., &#34;32Gi&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Note: If limitCPU and limitMemory are not specified, only GPU resources will have limits set equal to their requests.</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - pvcStorage: (Optional, string) The amount of storage requested for the model, e.g., &#34;50Gi&#34;.</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - pvcAccessMode: (Optional, list) The access mode policy for the mounted volume, e.g., [&#34;ReadWriteOnce&#34;]</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - storageClass: (Optional, String) The storage class of the PVC e.g., &#34;&#34;, default is &#34;&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - pvcMatchLabels: (Optional, map) The labels to match the PVC, e.g., {model: &#34;opt125m&#34;}</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - extraVolumes: (Optional, list) Additional volumes to add to the pod, in Kubernetes volume format. https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#volume-v1-core</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   Example for an emptyDir volume:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   extraVolumes:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - name: tmp-volume</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     emptyDir:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#       medium: &#34;&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#       sizeLimit: 5Gi</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - extraVolumeMounts: (Optional, list) Additional volume mounts to add to the container, in Kubernetes volumeMount format. https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#volumemount-v1-core</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   Example for mounting the tmp-volume to /tmp:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   extraVolumeMounts:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - name: tmp-volume</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     mountPath: /tmp</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - initContainer: (optional, list of objects) The configuration for the init container to be run before the main container.</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - name: (string) The name of the init container, e.g., &#34;init&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - image: (string) The Docker image for the init container, e.g., &#34;busybox:latest&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - command: (optional, list) The command to run in the init container, e.g., [&#34;sh&#34;, &#34;-c&#34;]</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - args: (optional, list) Additional arguments to pass to the command, e.g., [&#34;ls&#34;]</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - env: (optional, list) List of environment variables to set in the container, each being a map with:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - resources: (optional, map) The resource requests and limits for the container:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - mountPvcStorage: (optional, bool) Whether to mount the model&#39;s volume.</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - vllmConfig: (optional, map) The configuration for the VLLM model, supported options are:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - enablePrefixCaching: (optional, bool) Enable prefix caching, e.g., false</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - enableChunkedPrefill: (optional, bool) Enable chunked prefill, e.g., false</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - maxModelLen: (optional, int) The maximum model length, e.g., 16384</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - dtype: (optional, string) The data type, e.g., &#34;bfloat16&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - tensorParallelSize: (optional, int) The degree of tensor parallelism, e.g., 2</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - maxNumSeqs: (optional, int) Maximum number of sequences to be processed in a single iteration., e.g., 32</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - maxLoras: (optional, int) The maximum number of LoRA models to be loaded in a single batch, e.g., 4</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - gpuMemoryUtilization: (optional, float) The fraction of GPU memory to be used for the model executor, which can range from 0 to 1. e.g., 0.95</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - extraArgs: (optional, list) Extra command line arguments to pass to vLLM, e.g., [&#34;--disable-log-requests&#34;]</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - lmcacheConfig: (optional, map) The configuration of the LMCache for KV offloading, supported options are:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - enabled: (optional, bool) Enable LMCache, e.g., true</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - cpuOffloadingBufferSize: (optional, string) The CPU offloading buffer size, e.g., &#34;30&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - hf_token: (optional) Hugging Face token configuration. Can be either:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - A string containing the token directly (will be stored in a generated secret)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - An object referencing an existing secret:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     secretName: &#34;my-existing-secret&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     secretKey: &#34;hf-token-key&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - env: (optional, list) The environment variables to set in the container, e.g., your HF_TOKEN</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - nodeSelectorTerms: (optional, list) The node selector terms to match the nodes</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - shmSize: (optional, string) The size of the shared memory, e.g., &#34;20Gi&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Example:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># vllmApiKey: &#34;vllm_xxxxxxxxxxxxx&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># modelSpec:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># - name: &#34;mistral&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   repository: &#34;lmcache/vllm-openai&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   tag: &#34;latest&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   modelURL: &#34;mistralai/Mistral-7B-Instruct-v0.2&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   replicaCount: 1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   requestCPU: 10</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   requestMemory: &#34;64Gi&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   requestGPU: 1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   pvcStorage: &#34;50Gi&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   pvcAccessMode:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     - ReadWriteOnce</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   pvcMatchLabels:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     model: &#34;mistral&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   initContainer:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     name: my-container</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     image: busybox</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     command: [&#34;sh&#34;]</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     env: {}</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     args: []</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     resources: {}</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     mountPvcStorage: true</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   vllmConfig:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     enableChunkedPrefill: false</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     enablePrefixCaching: false</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     maxModelLen: 16384</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     dtype: &#34;bfloat16&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     maxNumSeqs: 32</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     gpuMemoryUtilization: 0.95</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     maxLoras: 4</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     extraArgs: [&#34;--disable-log-requests&#34;, &#34;--trust-remote-code&#34;]</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   lmcacheConfig:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     enabled: true</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     cpuOffloadingBufferSize: &#34;30&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   hf_token: &#34;hf_xxxxxxxxxxxxx&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   nodeSelectorTerms:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     - matchExpressions:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#       - key: nvidia.com/gpu.product</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#         operator: &#34;In&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#         values:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#         - &#34;NVIDIA-RTX-A6000&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">modelSpec</span>: []
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Container port</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">8000</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Service port</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">servicePort</span>: <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Set other environment variables from config map</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">configs</span>: {}
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- deployment strategy</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">strategy</span>: {}
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Readiness probe configuration</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">startupProbe</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- Number of seconds after the container has started before startup probe is initiated</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">initialDelaySeconds</span>: <span style="color:#ae81ff">15</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- How often (in seconds) to perform the startup probe</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">periodSeconds</span>: <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- Number of times after which if a probe fails in a row, Kubernetes considers that the overall check has failed: the container is not ready</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">failureThreshold</span>: <span style="color:#ae81ff">60</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- Configuration of the Kubelet http request on the server</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">httpGet</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># -- Path to access on the HTTP server</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/health</span>
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># -- Name or number of the port to access on the container, on which the server is listening</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">port</span>: <span style="color:#ae81ff">8000</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Liveness probe configuration</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">livenessProbe</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- Number of seconds after the container has started before liveness probe is initiated</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">initialDelaySeconds</span>: <span style="color:#ae81ff">15</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- Number of times after which if a probe fails in a row, Kubernetes considers that the overall check has failed: the container is not alive</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">failureThreshold</span>: <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- How often (in seconds) to perform the liveness probe</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">periodSeconds</span>: <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- Configuration of the Kubelet http request on the server</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">httpGet</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># -- Path to access on the HTTP server</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/health</span>
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># -- Name or number of the port to access on the container, on which the server is listening</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">port</span>: <span style="color:#ae81ff">8000</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Disruption Budget Configuration</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">maxUnavailablePodDisruptionBudget</span>: <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Tolerations configuration (when there are taints on nodes)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Example:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># tolerations:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - key: &#34;node-role.kubernetes.io/control-plane&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     operator: &#34;Exists&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     effect: &#34;NoSchedule&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">tolerations</span>: []
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- RuntimeClassName configuration, set to &#34;nvidia&#34; if the model requires GPU</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">runtimeClassName</span>: <span style="color:#e6db74">&#34;nvidia&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- SchedulerName configuration</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">schedulerName</span>: <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Pod-level security context configuration. https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#podsecuritycontext-v1-core</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">securityContext</span>: {}
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Run as a non-root user ID</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># runAsUser: 1000</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Run with a non-root group ID</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># runAsGroup: 1000</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Run as non-root</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># runAsNonRoot: true</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Set the seccomp profile</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># seccompProfile:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   type: RuntimeDefault</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Drop all capabilities</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># capabilities:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   drop:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   - ALL</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Set the file system group ID for all containers</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># fsGroup: 1000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Container-level security context configuration. https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#securitycontext-v1-core</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">containerSecurityContext</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- Run as non-root</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">runAsNonRoot</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- Don&#39;t allow privilege escalation</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># allowPrivilegeEscalation: false</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- Drop all capabilities</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># capabilities:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#   drop:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#   - ALL</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- Read-only root filesystem</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># readOnlyRootFilesystem: true</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">routerSpec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- The docker image of the router. The following values are defaults:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">repository</span>: <span style="color:#e6db74">&#34;lmcache/lmstack-router&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">tag</span>: <span style="color:#e6db74">&#34;latest&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">imagePullPolicy</span>: <span style="color:#e6db74">&#34;Always&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Whether to enable the router service</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">enableRouter</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Number of replicas</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">replicaCount</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Container port</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">8000</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Service type</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">serviceType</span>: <span style="color:#ae81ff">ClusterIP</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Service port</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">servicePort</span>: <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Service discovery mode, supports &#34;k8s&#34; or &#34;static&#34;. Defaults to &#34;k8s&#34; if not set.</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">serviceDiscovery</span>: <span style="color:#e6db74">&#34;k8s&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- If serviceDiscovery is set to &#34;static&#34;, the comma-separated values below are required. There needs to be the same number of backends and models</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">staticBackends</span>: <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">staticModels</span>: <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- routing logic, could be &#34;roundrobin&#34; or &#34;session&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">routingLogic</span>: <span style="color:#e6db74">&#34;roundrobin&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- session key if using &#34;session&#34; routing logic</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">sessionKey</span>: <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- extra router commandline arguments</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">extraArgs</span>: []
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Interval in seconds to scrape the serving engine metrics</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">engineScrapeInterval</span>: <span style="color:#ae81ff">15</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Window size in seconds to calculate the request statistics</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">requestStatsWindow</span>: <span style="color:#ae81ff">60</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- deployment strategy</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">strategy</span>: {}
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># vllmApiKey: (optional) api key for securing the vLLM models. Must be an object referencing an existing secret</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   secretName: &#34;my-existing-secret&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   secretKey: &#34;vllm-api-key&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- router resource requests and limits</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">requests</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;4&#34;</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;16G&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">limits</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;8&#34;</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;32G&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># -- Customized labels for the router deployment</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">environment</span>: <span style="color:#e6db74">&#34;router&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">release</span>: <span style="color:#e6db74">&#34;router&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ingress</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- Enable ingress controller resource</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- IngressClass that will be used to implement the Ingress</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">className</span>: <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># -- Additional annotations for the Ingress resource</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">annotations</span>: {}
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># kubernetes.io/ingress.class: alb</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># kubernetes.io/ingress.class: nginx</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># kubernetes.io/tls-acme: &#34;true&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The list of hostnames to be covered with this ingress record.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">hosts</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">vllm-router.local</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">paths</span>:
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">pathType</span>: <span style="color:#ae81ff">Prefix</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># --  The tls configuration for hostnames to be covered with this ingress record.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">tls</span>: []
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#  - secretName: vllm-router-tls</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#    hosts:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#      - vllm-router.local</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># The node selector terms to match the nodes</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Example:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#   nodeSelectorTerms:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#     - matchExpressions:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#       - key: nvidia.com/gpu.product</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#         operator: &#34;In&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#         values:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#         - &#34;NVIDIA-RTX-A6000&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">nodeSelectorTerms</span>: []
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -- TODO: Readiness probe configuration</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#startupProbe:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  # -- Number of seconds after the container has started before startup probe is initiated</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  initialDelaySeconds: 5</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  # -- How often (in seconds) to perform the startup probe</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  periodSeconds: 5</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  # -- Number of times after which if a probe fails in a row, Kubernetes considers that the overall check has failed: the container is not ready</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  failureThreshold: 100</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   # -- Configuration of the Kubelet http request on the server</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  httpGet:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#    # -- Path to access on the HTTP server</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span></code></pre></div><p>Here is what this yaml settings has:</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Section</th>
          <th style="text-align: left">What It Means</th>
          <th style="text-align: left">Impact</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><code>servingEngineSpec.enableEngine: true</code></td>
          <td style="text-align: left">The vLLM serving engine <strong>will deploy</strong></td>
          <td style="text-align: left">✅ A model server (FastAPI/vLLM backend) will run</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>servingEngineSpec.modelSpec: []</code></td>
          <td style="text-align: left"><strong>No models</strong> configured!</td>
          <td style="text-align: left">❌ vLLM backend will <strong>start EMPTY</strong>, no model loaded, not usable</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>servingEngineSpec.runtimeClassName: &quot;nvidia&quot;</code></td>
          <td style="text-align: left">Requests GPU scheduling (runtimeClass = NVIDIA)</td>
          <td style="text-align: left">⚠️ Will require GPU-capable nodes if enforced (Autopilot will handle automatically, but might error if none available)</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>servingEngineSpec.containerPort: 8000</code>, <code>servicePort: 80</code></td>
          <td style="text-align: left">Internal pod listens on 8000; service listens on 80</td>
          <td style="text-align: left">Standard setup for HTTP access inside cluster</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>routerSpec.enableRouter: true</code></td>
          <td style="text-align: left">The API router (proxy) <strong>will deploy</strong></td>
          <td style="text-align: left">✅ Will be able to route user traffic</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>routerSpec.repository: lmcache/lmstack-router</code>, <code>tag: latest</code></td>
          <td style="text-align: left">Uses default router Docker image</td>
          <td style="text-align: left">Fine, but might pull latest, unstable sometimes</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>routerSpec.serviceType: ClusterIP</code></td>
          <td style="text-align: left">Router is only <strong>internally</strong> reachable</td>
          <td style="text-align: left">❗ No public IP, need port-forward or LoadBalancer later if needed</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>routerSpec.routingLogic: roundrobin</code></td>
          <td style="text-align: left">Router sends requests evenly across backends</td>
          <td style="text-align: left">Good for simple load balancing</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>routerSpec.resources.requests: {cpu: &quot;4&quot;, memory: &quot;16G&quot;}</code></td>
          <td style="text-align: left">Router will <strong>reserve 4 CPU cores and 16Gi RAM</strong> minimum</td>
          <td style="text-align: left">🏋️ Heavy for tiny clusters; fine for prod-scale</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="i-found-that-i-could-test-it-using-only-1-pod-with-gpu">I found that I could test it using only 1 pod with gpu</h2>
<h3 id="open-api-port-on-runpod">Open api port on runpod</h3>
<ul>
<li><code>8001</code> is used by nginx, lets use 8002</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Serve on port 8002 instead:</span>
</span></span><span style="display:flex;"><span>vllm serve <span style="color:#f92672">--</span>task transcription openai<span style="color:#f92672">/</span>whisper<span style="color:#f92672">-</span>small \
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">--</span>host <span style="color:#ae81ff">0.0.0.0</span> <span style="color:#f92672">--</span>port <span style="color:#ae81ff">8002</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># if success , you will get this log</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># (workspace) root@33e3bb091952:/workspace/production-stack# vllm serve   --task transcription   openai/whisper-small   --host 0.0.0.0   --port 8002</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:52:43 [__init__.py:243] Automatically detected platform cuda.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:52:59 [__init__.py:31] Available plugins for group vllm.general_plugins:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:52:59 [__init__.py:33] - lora_filesystem_resolver -&gt; vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:52:59 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:53:01 [api_server.py:1289] vLLM API server version 0.9.0</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:53:02 [cli_args.py:300] non-default args: {&#39;host&#39;: &#39;0.0.0.0&#39;, &#39;port&#39;: 8002, &#39;task&#39;: &#39;transcription&#39;}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># config.json: 100%|███████████████████████████████████████████████████████████| 1.97k/1.97k [00:00&lt;00:00, 11.5MB/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># preprocessor_config.json: 100%|████████████████████████████████████████████████| 185k/185k [00:00&lt;00:00, 4.46MB/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:53:03 [config.py:3131] Downcasting torch.float32 to torch.float16.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># WARNING 05-29 21:53:30 [arg_utils.py:1583] --task transcription is not supported by the V1 Engine. Falling back to V0.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:53:46 [__init__.py:243] Automatically detected platform cuda.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:53:54 [api_server.py:257] Started engine process with PID 9035</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:53:54 [__init__.py:31] Available plugins for group vllm.general_plugins:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:53:54 [__init__.py:33] - lora_filesystem_resolver -&gt; vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:53:54 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:53:54 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.0) with config: model=&#39;openai/whisper-small&#39;, speculative_config=None, tokenizer=&#39;openai/whisper-small&#39;, skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=448, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend=&#39;auto&#39;, disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=&#39;&#39;), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=openai/whisper-small, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={&#34;compile_sizes&#34;: [], &#34;inductor_compile_config&#34;: {&#34;enable_auto_functionalized_v2&#34;: false}, &#34;cudagraph_capture_sizes&#34;: [256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], &#34;max_capture_size&#34;: 256}, use_cached_outputs=True,</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tokenizer_config.json: 100%|███████████████████████████████████████████████████| 283k/283k [00:00&lt;00:00, 4.41MB/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># vocab.json: 100%|██████████████████████████████████████████████████████████████| 836k/836k [00:00&lt;00:00, 33.2MB/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># tokenizer.json: 100%|████████████████████████████████████████████████████████| 2.48M/2.48M [00:00&lt;00:00, 31.7MB/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># merges.txt: 100%|██████████████████████████████████████████████████████████████| 494k/494k [00:00&lt;00:00, 14.5MB/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># normalizer.json: 100%|████████████████████████████████████████████████████████| 52.7k/52.7k [00:00&lt;00:00, 242MB/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># added_tokens.json: 100%|██████████████████████████████████████████████████████| 34.6k/34.6k [00:00&lt;00:00, 203MB/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># special_tokens_map.json: 100%|███████████████████████████████████████████████| 2.19k/2.19k [00:00&lt;00:00, 34.0MB/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># generation_config.json: 100%|████████████████████████████████████████████████| 3.87k/3.87k [00:00&lt;00:00, 48.9MB/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:53:55 [cuda.py:292] Using Flash Attention backend.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:53:56 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:53:56 [model_runner.py:1170] Starting to load model openai/whisper-small...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:53:57 [weight_utils.py:291] Using model weights format [&#39;*.safetensors&#39;]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># model.safetensors: 100%|████████████████████████████████████████████████████████| 967M/967M [00:03&lt;00:00, 294MB/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:01 [weight_utils.py:307] Time spent downloading weights for openai/whisper-small: 3.485868 seconds</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:01 [weight_utils.py:344] No model.safetensors.index.json found in remote.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00&lt;?, ?it/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  4.22it/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  4.22it/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:01 [default_loader.py:280] Loading weights took 0.28 seconds</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:01 [model_runner.py:1202] Model loading took 0.4668 GiB and 5.083266 seconds</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:03 [enc_dec_model_runner.py:315] Starting profile run for multi-modal models.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:07 [worker.py:291] Memory profiling takes 5.44 seconds</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:07 [worker.py:291] the current vLLM instance can use total_gpu_memory (23.54GiB) x gpu_memory_utilization (0.90) = 21.19GiB</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:07 [worker.py:291] model weights take 0.47GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 6.29GiB; the rest of the memory reserved for KV Cache is 14.35GiB.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:07 [executor_base.py:112] # cuda blocks: 26125, # CPU blocks: 7281</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:07 [executor_base.py:117] Maximum concurrency for 448 tokens per request: 933.04x</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:12 [model_runner.py:1512] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set &#39;enforce_eager=True&#39; or use &#39;--enforce-eager&#39; in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Capturing CUDA graph shapes: 100%|████████████████████████████████████████████████| 35/35 [00:15&lt;00:00,  2.22it/s]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:27 [model_runner.py:1670] Graph capturing finished in 16 secs, took 0.15 GiB</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:27 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 25.88 seconds</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [api_server.py:1336] Starting vLLM API server on http://0.0.0.0:8002</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:28] Available routes are:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /docs, Methods: HEAD, GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /redoc, Methods: HEAD, GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /health, Methods: GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /load, Methods: GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /ping, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /ping, Methods: GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /tokenize, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /detokenize, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /v1/models, Methods: GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /version, Methods: GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /v1/chat/completions, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /v1/completions, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /v1/embeddings, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /pooling, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /classify, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /score, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /v1/score, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /rerank, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /v1/rerank, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /v2/rerank, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /invocations, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 21:54:29 [launcher.py:36] Route: /metrics, Methods: GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO:     Started server process [8831]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO:     Waiting for application startup.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO:     Application startup complete.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># And update your router to point there:</span>
</span></span><span style="display:flex;"><span>export VLLM_BACKEND_URL<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;http://localhost:8002&#34;</span>
</span></span><span style="display:flex;"><span>uvicorn main_router:app <span style="color:#f92672">--</span>reload <span style="color:#f92672">--</span>port <span style="color:#ae81ff">8000</span>
</span></span></code></pre></div><h3 id="also-run-the-production-stacks">also run the production stacks</h3>
<p>Good thing we&rsquo;ve the consistent python env, we can set the <code>PYTHONPATH</code></p>
<p>also install the dependencies for production-stack in editable mode</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>source /workspace/.venv/bin/activate
</span></span><span style="display:flex;"><span>pip install -e /workspace/production-stack<span style="color:#f92672">[</span>dev<span style="color:#f92672">]</span>
</span></span></code></pre></div><p>[!NOTE] production-stack need python 3.12, let&rsquo;s use a python 3.12 venv</p>
<p>If you faced no space while installing, move cache to workspace</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>export PIP_CACHE_DIR<span style="color:#f92672">=</span>/workspace/.pip_cache
</span></span><span style="display:flex;"><span>mkdir -p /workspace/.pip_cache
</span></span><span style="display:flex;"><span>pip cache purge            <span style="color:#75715e"># clear the old cache on overlay</span>
</span></span><span style="display:flex;"><span>pip install vllm<span style="color:#f92672">[</span>audio<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>pip install -e production-stack<span style="color:#f92672">[</span>dev<span style="color:#f92672">]</span>
</span></span></code></pre></div><p>If there&rsquo;s no pip locally, install the pip</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#f92672">(</span>workspace<span style="color:#f92672">)</span> root@33e3bb091952:/workspace# which pip
</span></span><span style="display:flex;"><span>/usr/local/bin/pip
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>workspace<span style="color:#f92672">)</span> root@33e3bb091952:/workspace# python -m pip install --upgrade pip setuptools
</span></span><span style="display:flex;"><span>/workspace/.venv/bin/python: No module named pip
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>workspace<span style="color:#f92672">)</span> root@33e3bb091952:/workspace# ls .venv/
</span></span><span style="display:flex;"><span>CACHEDIR.TAG  bin  lib  lib64  pyvenv.cfg
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>workspace<span style="color:#f92672">)</span> root@33e3bb091952:/workspace# ls .venv/bin
</span></span><span style="display:flex;"><span>activate      activate.csh   activate.nu   activate_this.py  pydoc.bat  python3
</span></span><span style="display:flex;"><span>activate.bat  activate.fish  activate.ps1  deactivate.bat    python     python3.12
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>workspace<span style="color:#f92672">)</span> root@33e3bb091952:/workspace# python -m ensurepip --upgrade
</span></span><span style="display:flex;"><span>Looking in links: /tmp/tmp8w8h4iwe
</span></span><span style="display:flex;"><span>Processing /tmp/tmp8w8h4iwe/pip-23.2.1-py3-none-any.whl
</span></span><span style="display:flex;"><span>Installing collected packages: pip
</span></span><span style="display:flex;"><span>Successfully installed pip-23.2.1
</span></span></code></pre></div><p>Install vLLM (with audio) without filling the small overlay cache</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>export PIP_NO_CACHE_DIR<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>pip install vllm<span style="color:#f92672">[</span>audio<span style="color:#f92672">]</span>
</span></span></code></pre></div><p>Install router in editable mode</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install -e production-stack<span style="color:#f92672">[</span>dev<span style="color:#f92672">]</span>
</span></span></code></pre></div><p>verify both are installed under <code>python3.12</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip list | grep -E <span style="color:#e6db74">&#34;vllm|vllm-router&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># (workspace) root@33e3bb091952:/workspace# pip list | grep -E &#34;vllm|vllm-router&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># vllm                                     0.9.0</span>
</span></span></code></pre></div><h3 id="testing-script">Testing script</h3>
<ol>
<li>start the whisper backend</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>vllm serve --task transcription openai/whisper-small <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --host 0.0.0.0 --port <span style="color:#ae81ff">8002</span>
</span></span></code></pre></div><ol start="2">
<li>Started the production-stack router (your main_router on port 8000), for example with:</li>
</ol>
<ul>
<li>add this to <code>src/vllm_router/run-router.sh</code> and comment other router services for testing</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Use this command when testing with whisper transcription</span>
</span></span><span style="display:flex;"><span>ROUTER_PORT<span style="color:#f92672">=</span>$1
</span></span><span style="display:flex;"><span>BACKEND_URL<span style="color:#f92672">=</span>$2
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>python3 -m vllm_router.app <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --host 0.0.0.0 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --port <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">${</span>ROUTER_PORT<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --service-discovery static <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --static-backends <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">${</span>BACKEND_URL<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --static-models <span style="color:#e6db74">&#34;openai/whisper-small&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --static-model-types <span style="color:#e6db74">&#34;transcription&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --routing-logic roundrobin <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --log-stats <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --engine-stats-interval <span style="color:#ae81ff">10</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --request-stats-window <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># To disable this warning, you can either:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         - Avoid using `tokenizers` before the fork if possible</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [api_server.py:1336] Starting vLLM API server on http://0.0.0.0:8002</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:28] Available routes are:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /docs, Methods: HEAD, GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /redoc, Methods: HEAD, GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /health, Methods: GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /load, Methods: GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /ping, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /ping, Methods: GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /tokenize, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /detokenize, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /v1/models, Methods: GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /version, Methods: GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /v1/chat/completions, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /v1/completions, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /v1/embeddings, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /pooling, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /classify, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /score, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /v1/score, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /rerank, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /v1/rerank, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /v2/rerank, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /invocations, Methods: POST</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO 05-29 23:29:15 [launcher.py:36] Route: /metrics, Methods: GET</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO:     Started server process [9747]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO:     Waiting for application startup.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO:     Application startup complete.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO:     127.0.0.1:46250 - &#34;GET /metrics HTTP/1.1&#34; 200 OK</span>
</span></span></code></pre></div><ul>
<li>make sure make it executable</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#f92672">(</span>workspace<span style="color:#f92672">)</span> root@33e3bb091952:/workspace/production-stack# cd src/vllm_router
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>workspace<span style="color:#f92672">)</span> root@33e3bb091952:/workspace/production-stack# chmod +x run-router.sh
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>workspace<span style="color:#f92672">)</span> root@33e3bb091952:/workspace/production-stack# ./run-router.sh <span style="color:#ae81ff">8000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">(</span>workspace<span style="color:#f92672">)</span> root@33e3bb091952:/workspace/production-stack/src/vllm_router# ./run-router.sh <span style="color:#ae81ff">8000</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># [2025-05-29 23:30:44,964] INFO: Scraping metrics from 1 serving engine(s) (engine_stats.py:136:vllm_router.stats.engine_stats)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># [2025-05-29 23:30:44,964] INFO: Initializing round-robin routing logic (routing_logic.py:392:vllm_router.routers.routing_logic)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO:     Started server process [10309]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO:     Waiting for application startup.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># [2025-05-29 23:30:45,089] INFO: httpx AsyncClient instantiated. Id 140322398720160 (httpx_client.py:31:vllm_router.httpx_client)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO:     Application startup complete.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># [2025-05-29 23:30:54,965] INFO:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ==================================================</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Server: http://localhost:8002</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Models:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   - openai/whisper-small</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  Engine Stats: Running Requests: 0.0, Queued Requests: 0.0, GPU Cache Hit Rate: 0.00</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  Request Stats: No stats available</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --------------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ==================================================</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  (log_stats.py:104:vllm_router.stats.log_stats)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># [2025-05-29 23:30:54,984] INFO: Scraping metrics from 1 serving engine(s) (engine_stats.py:136:vllm_router.stats.engine_stats)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># [2025-05-29 23:31:04,966] INFO:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ==================================================</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Server: http://localhost:8002</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Models:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   - openai/whisper-small</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  Engine Stats: Running Requests: 0.0, Queued Requests: 0.0, GPU Cache Hit Rate: 0.00</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  Request Stats: No stats available</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --------------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ==================================================</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  (log_stats.py:104:vllm_router.stats.log_stats)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># [2025-05-29 23:31:04,991] INFO: Scraping metrics from 1 serving engine(s) (engine_stats.py:136:vllm_router.stats.engine_stats)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># [2025-05-29 23:31:14,966] INFO:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ==================================================</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Server: http://localhost:8002</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Models:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   - openai/whisper-small</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  Engine Stats: Running Requests: 0.0, Queued Requests: 0.0, GPU Cache Hit Rate: 0.00</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  Request Stats: No stats available</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --------------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ==================================================</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  (log_stats.py:104:vllm_router.stats.log_stats)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># [2025-05-29 23:31:14,997] INFO: Scraping metrics from 1 serving engine(s) (engine_stats.py:136:vllm_router.stats.engine_stats)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># [2025-05-29 23:31:24,967] INFO:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ==================================================</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Server: http://localhost:8002</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Models:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   - openai/whisper-small</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  Engine Stats: Running Requests: 0.0, Queued Requests: 0.0, GPU Cache Hit Rate: 0.00</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  Request Stats: No stats available</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --------------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ==================================================</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  (log_stats.py:104:vllm_router.stats.log_stats)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># [2025-05-29 23:31:25,004] INFO: Scraping metrics from 1 serving engine(s) (engine_stats.py:136:vllm_router.stats.engine_stats)</span>
</span></span></code></pre></div><ol start="3">
<li>smoke test the transcription endpoint</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -v http://localhost:8000/v1/audio/transcriptions <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -F <span style="color:#e6db74">&#39;file=@/path/to/audio.wav;type=audio/wav&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -F <span style="color:#e6db74">&#39;model=whisper-small&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -F <span style="color:#e6db74">&#39;language=en&#39;</span>
</span></span></code></pre></div><p>[!NOTE] Record an input audio before testing</p>
<h2 id="implementation-details">Implementation Details</h2>
<h3 id="enabling-the-transcription-model-type">Enabling the <code>transcription</code> model type</h3>
<p>Add a new enum member in <code>utils.py</code> so the parser recognizes transcription as a valid model type:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ModelType</span>(enum<span style="color:#f92672">.</span>Enum):
</span></span><span style="display:flex;"><span>    <span style="color:#960050;background-color:#1e0010">…</span>  
</span></span><span style="display:flex;"><span>    transcription <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/v1/audio/transcriptions&#34;</span>
</span></span></code></pre></div><p>Then include it in your get_all_fields() so validate_static_model_types() accepts it .</p>
<h3 id="parser-validation">Parser validation</h3>
<p>Update <code>validate_static_model_types</code> in <code>parsers/parser.py</code> to allow the new <code>transcription</code> string:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">validate_static_model_types</span>(model_types: str <span style="color:#f92672">|</span> <span style="color:#66d9ef">None</span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#960050;background-color:#1e0010">…</span>  
</span></span><span style="display:flex;"><span>    all_models <span style="color:#f92672">=</span> utils<span style="color:#f92672">.</span>ModelType<span style="color:#f92672">.</span>get_all_fields()
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># now includes &#39;transcription&#39;</span>
</span></span></code></pre></div><p>This ensures <code>--static-model-types</code> transcription passes validation .</p>
<h3 id="main-router-proxy-logic">Main router proxy logic</h3>
<p>In <code>routers/main_router.py</code>’s <code>audio_transcriptions</code> handler, use the same service‐discovery + routing logic you already have for chat/completions, but:</p>
<ol>
<li>Filter by <code>model_label==&quot;transcription&quot;</code> (or skip it in dev) and <code>model in ep.model_names</code>.</li>
<li>Increase HTTPX timeout, since Whisper can take a while:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">with</span> httpx<span style="color:#f92672">.</span>AsyncClient(
</span></span><span style="display:flex;"><span>    base_url<span style="color:#f92672">=</span>chosen_url,
</span></span><span style="display:flex;"><span>    timeout<span style="color:#f92672">=</span>httpx<span style="color:#f92672">.</span>Timeout(connect<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, read<span style="color:#f92672">=</span><span style="color:#ae81ff">120</span>, write<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>, pool<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)
</span></span><span style="display:flex;"><span>) <span style="color:#66d9ef">as</span> client:
</span></span><span style="display:flex;"><span>    <span style="color:#960050;background-color:#1e0010">…</span>
</span></span></code></pre></div><ol start="3">
<li><strong>Proxy</strong> the multipart form exactly to /v1/audio/transcriptions on the backend .</li>
</ol>
<h3 id="smoke-testing">Smoke-Testing</h3>
<ol>
<li>Start the Whisper backend on port 8002:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>vllm serve --task transcription openai/whisper-small <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --host 0.0.0.0 --port <span style="color:#ae81ff">8002</span>
</span></span></code></pre></div><ol start="2">
<li>run the modified router on port 8000:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>./run-router.sh <span style="color:#ae81ff">8000</span> http://localhost:8002
</span></span></code></pre></div><ol start="3">
<li>hit the endpoint with <code>curl</code>, noting you must specify a valid <code>response_format</code> (e.g. <code>json</code>):</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -v http://localhost:8000/v1/audio/transcriptions <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -F <span style="color:#e6db74">&#39;file=@/path/to/audio.wav;type=audio/wav&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -F <span style="color:#e6db74">&#39;model=openai/whisper-small&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -F <span style="color:#e6db74">&#39;response_format=json&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -F <span style="color:#e6db74">&#39;language=en&#39;</span>
</span></span></code></pre></div><ol start="4">
<li>Finally, you should get back something like:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#f92672">{</span><span style="color:#e6db74">&#34;text&#34;</span>:<span style="color:#e6db74">&#34;…your transcription…&#34;</span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>which confirms end-to-end routing.</p>
<h2 id="troubleshooting--tips">Troubleshooting &amp; Tips</h2>
<ul>
<li>
<p><em>“No transcription backend” 503</em>
Means your router didn’t detect any backend with <code>model_label==&quot;transcription&quot;</code>. Double-check <code>--static-model-labels transcription</code> or skip the label filter in dev.</p>
</li>
<li>
<p><em>Timeouts</em>
If you see <code>ReadTimeout</code>, bump up <code>read</code> in <code>httpx.Timeout(...)</code>.</p>
</li>
<li>
<p><em>Response format errors</em>
Whisper only accepts one of <code>json</code>, <code>text</code>, <code>srt</code>, <code>verbose_json</code>, <code>vtt</code> for <code>response_format</code>.</p>
</li>
</ul>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
