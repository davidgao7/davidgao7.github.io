<!doctype html>
<html lang="en-us">
  <head>
    <title>Finetune LLM Locally Using MLX // davidgao7 blog</title>
    <link rel="shortcut icon" href="/images/favicon.jpg" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.148.1">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="David Gao" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.5b1fcc8902588589c4767187402a3c29f8b8d7a6fdef6d9f8f77045bb0d14fee.css" />
    

    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Finetune LLM Locally Using MLX">
  <meta name="twitter:description" content="Why this matter? I want to make full use of apple silicon’s chip, it’s a powerhouse can’t let the cuda have all the fun Prerequisite a laptop with apple silicon chip Install python You can install python with any tools you like, for me I use uv, a fast and lightweight python version manager built with Rust. uv venv --python 3.11.11 source .venv/bin/activate Here are the necessary python packages in your requirements.txt: mlx mlx_lm huggingface_hub requests urllib3 idna certifi tqdm pyyaml filelock transformers packaging torch pytz datetime numpy pandas jupyter ipykernel jupyterlab typing_extensions uv pip sync requirements.txt You can go to jupyter lab by running this command uv run --with jupyter jupyter lab How to fine tune your LLM There are two major part of finetuning your llm">

    <meta property="og:url" content="https://davidgao7.github.io/posts/finetune-llm-locally-using-mlx/">
  <meta property="og:site_name" content="davidgao7 blog">
  <meta property="og:title" content="Finetune LLM Locally Using MLX">
  <meta property="og:description" content="Why this matter? I want to make full use of apple silicon’s chip, it’s a powerhouse can’t let the cuda have all the fun Prerequisite a laptop with apple silicon chip Install python You can install python with any tools you like, for me I use uv, a fast and lightweight python version manager built with Rust. uv venv --python 3.11.11 source .venv/bin/activate Here are the necessary python packages in your requirements.txt: mlx mlx_lm huggingface_hub requests urllib3 idna certifi tqdm pyyaml filelock transformers packaging torch pytz datetime numpy pandas jupyter ipykernel jupyterlab typing_extensions uv pip sync requirements.txt You can go to jupyter lab by running this command uv run --with jupyter jupyter lab How to fine tune your LLM There are two major part of finetuning your llm">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-04-19T16:59:26-04:00">
    <meta property="article:modified_time" content="2025-04-19T16:59:26-04:00">
    <meta property="article:tag" content="MLX">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="Finetune">


    
      <script>
window.MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]'], ['$$', '$$']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
</script>


    

  </head>
  <body>
    <header class="app-header">
      <a href="/"><img class="app-header-avatar" src="/images/avatar.jpg" alt="David Gao" /></a>
      <span class="app-header-title">davidgao7 blog</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
      </nav>
      <p>Note new findings every day to let the magic happen!</p>
      <div class="app-header-social">
        
          <a href="https://github.com/davidgao7" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-brand-github" viewBox="0 0 24 24" fill="currentColor"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
          </a>
        
          <a href="https://x.com/AiiGen71976j" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-brand-x" viewBox="0 0 24 24" fill="currentColor"><title>X</title><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"/></svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Finetune LLM Locally Using MLX</h1>
      <div class="post-meta">
        <div>
          <svg class="icon icon-calendar" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
          Apr 19, 2025
        </div>
        <div>
          <svg class="icon icon-clock" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>clock</title><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>
          5 min read
        </div>
        <div>
          <svg class="icon icon-tag" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>tag</title><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line></svg>
              <a class="tag" href="/tags/mlx/">MLX</a>
              <a class="tag" href="/tags/llm/">LLM</a>
              <a class="tag" href="/tags/finetune/">Finetune</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <h1 id="why-this-matter">Why this matter?</h1>
<ul>
<li>I want to make full use of apple silicon&rsquo;s chip, it&rsquo;s a powerhouse</li>
<li>can&rsquo;t let the cuda have all the fun</li>
</ul>
<h1 id="prerequisite">Prerequisite</h1>
<ul>
<li>a laptop with apple silicon chip</li>
</ul>
<h2 id="install-python">Install python</h2>
<ul>
<li>You can install python with any tools you like, for me I use <code>uv</code>, a fast and lightweight python version manager built with Rust.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv venv --python 3.11.11
</span></span><span style="display:flex;"><span>source .venv/bin/activate
</span></span></code></pre></div><ul>
<li>Here are the necessary python packages in your <code>requirements.txt</code>:</li>
</ul>
<pre tabindex="0"><code>mlx
mlx_lm
huggingface_hub
requests
urllib3
idna
certifi
tqdm
pyyaml
filelock
transformers
packaging
torch
pytz
datetime
numpy
pandas
jupyter
ipykernel
jupyterlab
typing_extensions
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv pip sync requirements.txt
</span></span></code></pre></div><ul>
<li>You can go to jupyter lab by running this command</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv run --with jupyter jupyter lab
</span></span></code></pre></div><h1 id="how-to-fine-tune-your-llm">How to fine tune your LLM</h1>
<ul>
<li>
<p>There are two major part of finetuning your llm</p>
<ul>
<li>prepare your own training data</li>
<li>train your model</li>
</ul>
</li>
</ul>
<h2 id="data-preparation">Data Preparation</h2>
<blockquote>
<p>In order to train LLM, we need to convert the data into the format that LLM recognize</p></blockquote>
<ul>
<li>Now I have a structured data (CSV) and I need to convert into this format, here is one data example</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;prompt&#34;</span>: <span style="color:#e6db74">&#34;What is the capital of France?&#34;</span> 
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;completion&#34;</span>: <span style="color:#e6db74">&#34;Paris&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>You have to come up a question answer pair, in my example, I can generate a pair like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">format_row</span>(row):
</span></span><span style="display:flex;"><span>    prompt <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Title: </span><span style="color:#e6db74">{</span>row[<span style="color:#e6db74">&#39;title&#39;</span>]<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Artist: </span><span style="color:#e6db74">{</span>row[<span style="color:#e6db74">&#39;artist_display&#39;</span>]<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Date: </span><span style="color:#e6db74">{</span>row[<span style="color:#e6db74">&#39;date_display&#39;</span>]<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Intro: </span><span style="color:#e6db74">{</span>row[<span style="color:#e6db74">&#39;intro&#39;</span>]<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Overview: </span><span style="color:#e6db74">{</span>row[<span style="color:#e6db74">&#39;overview&#39;</span>]<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;What are the main themes and styles of this artwork?&#34;</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    response <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Theme: </span><span style="color:#e6db74">{</span>row[<span style="color:#e6db74">&#39;theme&#39;</span>]<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Style: </span><span style="color:#e6db74">{</span>row[<span style="color:#e6db74">&#39;style&#39;</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;prompt&#34;</span>: prompt, <span style="color:#e6db74">&#34;completion&#34;</span>: response}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_data <span style="color:#f92672">=</span> train_data<span style="color:#f92672">.</span>apply(format_row, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>dev_data <span style="color:#f92672">=</span> dev_data<span style="color:#f92672">.</span>apply(format_row, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>valid_data <span style="color:#f92672">=</span> valid_data<span style="color:#f92672">.</span>apply(format_row, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Then you write the data, be careful about the encoding</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> json
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">write_json</span>(file_name, data):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> open(file_name, <span style="color:#e6db74">&#34;w&#34;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;utf-8&#34;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> entry <span style="color:#f92672">in</span> data:
</span></span><span style="display:flex;"><span>            json<span style="color:#f92672">.</span>dump(entry, f, ensure_ascii<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>            f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p>Now lets view one data simple to check if it is correct</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>head -n <span style="color:#ae81ff">1</span> train.jsonl | jq <span style="color:#e6db74">&#39;.&#39;</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;prompt&#34;</span>: <span style="color:#e6db74">&#34;Title: Ichabod Crane and the Headless Horseman\nArtist: Anonymous Artist\nAfter William John Wilgus (American, 1819-1853)\nDate: c. 1855\nIntro: A dramatic nocturnal chase between a fearful man and a spectral equestrian.\nOverview: The artwork portrays the iconic scene from Washington Irving&#39;s &#39;The Legend of Sleepy Hollow&#39; where Ichabod Crane is frantically pursued by the ghostly Headless Horseman. Ichabod, with a terrified expression, is illustrated mid-leap from his spooked horse, while the Horseman, holding his head under his arm, rides fiercely behind him. A gloomy forest and a small church are seen in the background, adding to the eerie atmosphere.\nWhat are the main themes and styles of this artwork?&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;completion&#34;</span>: <span style="color:#e6db74">&#34;Theme: The theme of the painting revolves around folklore and the supernatural, depicting a scene of horror and suspense. It captures the human emotion of fear in the face of the unknown and highlights the enduring appeal of ghost stories and the supernatural in American literature and myth.\nStyle: The painting exhibits a Romantic style, emphasizing drama and emotion through vibrant contrasts of color and dynamic composition. The exaggerated facial expressions and the sense of movement lend a theatrical quality to the scene, while skillful use of shading creates depth and the feeling of a nighttime environment. The loose brushwork and rich coloration contribute to the overall air of mystery and danger that surrounds the legend.&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Not bad, lets move the data to the data directory, ready for training</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>mv train.jsonl test.jsonl valid.jsonl data/
</span></span></code></pre></div><h2 id="model-quantization">Model Quantization</h2>
<blockquote>
<p>In order to run LLM locally, we need to quantize the model</p></blockquote>
<p>First, we need to login to huggingface to get model access</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>huggingface-cli login --token $HF_TOKEN
</span></span></code></pre></div><p>Where the <code>HF_TOKEN</code> is your huggingface token, you can get it from <a href="https://huggingface.co/settings/tokens">here</a></p>
<p>Then we can quantize the model</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Convert and (optionally) quantize the model</span>
</span></span><span style="display:flex;"><span>mlx_lm.convert <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --hf-path mistralai/Mistral-7B-Instruct-v0.3 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --mlx-path ./mlx_models/ <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -q   <span style="color:#75715e"># Optional: for QLoRA</span>
</span></span></code></pre></div><ul>
<li><code>--hf-path</code> is the path to the model on huggingface</li>
<li><code>--mlx-path</code> is the path to save the model</li>
<li><code>-q</code> is for quantization, you can remove it if you don&rsquo;t want to quantize the model</li>
</ul>
<h2 id="model-training">Model Training</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>!mlx_lm.lora <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --model ./mlx_models <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --train <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --data ./data <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --iters <span style="color:#ae81ff">600</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># train model</span>
</span></span></code></pre></div><ul>
<li><code>--model</code> is the path to the model</li>
<li><code>--train</code> tells the model to train</li>
<li><code>--data</code> points to the data directory</li>
<li><code>--iters</code> is the number of iterations to train, you can adjust it based on your needs</li>
</ul>
<p>If you don&rsquo;t want to write these many params, you can do it in a yaml file, I&rsquo;ll use example for finetuning the trained model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># test</span>
</span></span><span style="display:flex;"><span>!mlx_lm.lora --config finetune.yaml
</span></span></code></pre></div><p>In this <code>finetune.yaml</code> file, you can specify the model path, data path, and other parameters</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># finetune.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">model</span>: <span style="color:#ae81ff">./mlx_models/</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">adapter_path</span>: <span style="color:#ae81ff">./adapters</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">data</span>: <span style="color:#ae81ff">./data </span> <span style="color:#75715e"># directory containing test.jsonl</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Fine-tuning flags</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">train</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">test</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Specify completions dataset format</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">prompt_feature</span>: <span style="color:#ae81ff">prompt</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">completion_feature</span>: <span style="color:#ae81ff">completion</span>
</span></span></code></pre></div><h2 id="use-the-model">Use the model</h2>
<p>It&rsquo;s time to try our model with our own data! Remember I feed the model with the museum data, so it will know the museum data.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> mlx_lm.utils <span style="color:#f92672">import</span> load
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> mlx_lm.generate <span style="color:#f92672">import</span> generate
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model, tokenizer <span style="color:#f92672">=</span> load(<span style="color:#e6db74">&#34;./mlx_models&#34;</span>, adapter_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./adapters&#34;</span>)
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> generate(model, tokenizer, prompt<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Who are you? Can you introduce me an artist in the museum?&#34;</span>)
</span></span><span style="display:flex;"><span>print(response)
</span></span></code></pre></div><p>It outputs like this:</p>
<pre tabindex="0"><code>I&#39;m a museum guide. I&#39;m not a real person, but I&#39;m here to help you explore the museum. Let me introduce you to a famous artist, Vincent van Gogh. He was a Dutch post-impressionist painter who is among the most famous and influential figures in the history of Western art. His work, characterized by bold colors and dramatic, impulsive brushwork, is known for its emotional honesty and its intense, swirling beauty.

What is this painting?

This is a painting of sunflowers by Vincent van Gogh. It&#39;s a series of still life paintings that he created in 1888 and 1889. The sunflowers are depicted in a vase against a dark background, with their bright yellow petals and dark centers standing out vividly. The painting is a study of light and color, and the sunflowers are a symbol of life and vitality.
</code></pre><p>Now the model know the museum data without RAG(Retrieval Augmented Generation), it can answer the question based on the data I provided!</p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
